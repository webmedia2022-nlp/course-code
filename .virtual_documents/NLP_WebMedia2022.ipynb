get_ipython().getoutput("pip install --no-cache-dir -r requirements.txt")
get_ipython().getoutput("python -m spacy download en_core_web_sm #Instalando dependências específicas do spacy")


get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")

import pandas as pd
import numpy as np
import nltk
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
import getpass
import warnings
import pathlib
import os 
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path

# Criação do diretório "data/"
Path("data").mkdir(parents=True, exist_ok=True)

from DataExtraction import DataExtraction
from Preprocessing import Preprocessing
from ModelosRepresentacao import ModelosEstatisticos, SentenceEmbeddings, WordEmbeddings
from ExtracaoConhecimento import Clustering, SemanticComprehension, SentimentAnalysis

warnings.filterwarnings('ignore')
tqdm.pandas()


# Download de arquivos usados por bibliotecas

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')


# Credenciais da API do Twitter

print("Informe seu 'API KEY'")
twitter_consumer_key = getpass.getpass()

print("Informe seu 'API KEY SECRET'")
twitter_consumer_secret = getpass.getpass()

print("Informe seu 'ACCESS TOKEN KEY'")
twitter_access_token_key = getpass.getpass()

print("Informe seu 'ACCESS TOKEN SECRET'")
twitter_access_token_secret = getpass.getpass()

print("Informe seu 'Bearer TOKEN'")
twitter_bearer_token = getpass.getpass()


# Coleta de tweets
df_tweets = DataExtraction().twitter(
    twitter_consumer_key, 
    twitter_consumer_secret, 
    twitter_access_token_key, 
    twitter_access_token_secret, 
    twitter_bearer_token
)

df_tweets.head()


# Credenciais da API do Reddit

print("Informe seu 'CLIENT ID'")
REDDIT_CLIENT_ID = getpass.getpass()

print("Informe seu 'CLIENT SECRET'")
REDDIT_CLIENT_SECRET = getpass.getpass()


# Subreddits com discussões sérias sobre assuntos como política, história e ciência.
subreddits = [
    'politics',
    'AskHistorians',
    'changemyview',
    'COVID19',
    'EverythingScience',
    'science'
]

# Coleta os top 100 posts de cada Subreddit
df_reddit_posts = DataExtraction().reddit(
    REDDIT_CLIENT_ID,
    REDDIT_CLIENT_SECRET,
    subreddits=subreddits,
    top_n=100
)


# Apresenta alguns posts com texto
df_reddit_posts[df_reddit_posts['length'] > 0].tail(5)


# Credenciais da API do CrowdTangle

print("Informe seu 'API_TOKEN'")
CROWDTANGLE_API_TOKEN = getpass.getpass()


# Aqui coletamos os top 100 posts em cada mês, iniciando em start_date e terminando em end_date
df_facebook_posts = DataExtraction().facebook(
    CROWDTANGLE_API_TOKEN, 
    search_term='covid-19',
    start_date = '2020-04-01',
    end_date = '2021-04-01'
)


# Amostra de posts do Facebook
df_facebook_posts.tail(3)


#APAGAR

import json

example_file = 'data/tweets.json'
tweets = open(example_file).read()
tweets = json.loads(tweets)

dados = [d['text'] for d in tweets]
import pandas as pd
dados = pd.DataFrame(dados, columns=['texto'])


from Preprocessing import Preprocessing

pipeline = Preprocessing()


dados['texto normalizado'] = dados['texto'].apply(pipeline.normalizacao)

dados.sample(2)


remover_links = r'https?:\/\/.*[\r\n]*'
aplicar_regex = lambda x: pipeline.limpeza_regex(x, remover_links, valor='LINK')
dados['texto limpo'] = dados['texto normalizado'].apply(aplicar_regex)

remover_mentions = r'@([A-Za-z0-9_]+)'
aplicar_regex = lambda x: pipeline.limpeza_regex(x, remover_mentions, valor='USERNAME')
dados['texto limpo'] = dados['texto limpo'].apply(aplicar_regex)

dados.sample(2)


dados['tokens'] = dados['texto limpo'].apply(pipeline.tokenizacao)

dados.sample(2)


dados['stems'] = dados['tokens'].apply(pipeline.stemmizacao)

dados.sample(2)


dados['lemmas'] = dados['tokens'].apply(pipeline.lemmatizacao)

dados.sample(2)


#Fazendo tudo de uma só vez com Spacy

dados['tokens'], dados['pos tags'], dados['lemmas'] = zip(*dados['texto'].apply(pipeline.nlp_pipeline))

dados.sample(2)


modelos_stats = ModelosEstatisticos()
modelos_stats.bow(dados['texto'])


modelos_stats.tfidf(dados['texto'])


#TO-DO: PCA/SVD


embeddings = WordEmbeddings()


embeddings.word2vec(dados['texto'])


embeddings.fasttext(dados['texto'])


sentences = [
    # Smartphones
    "I like my phone",
    "My phone is not good.",
    "Your cellphone looks great.",

    # Weather
    "Will it snow tomorrow?",
    "Recently a lot of hurricanes have hit the US",
    "Global warming is real",

    # Food and health
    "An apple a day, keeps the doctors away",
    "Eating strawberries is healthy",
    "Is paleo better than keto?",

    # Asking about age
    "How old are you?",
    "what is your age?",
]


# InferSent
# Na primeira execução, é feito o download de arquivos de modelos e embeddings
# Certifique-se de ter pelo menos 9GB disponíveis em disco para isso.
# Devido ao download, a primeira execução é lenta
infersent_embeddings = SentenceEmbeddings().infersent(sentences)
infersent_embeddings.shape


# USE
# Na primeira execução, é feito o download de arquivos de modelos e embeddings
# Certifique-se de ter pelo menos 1GB disponível em disco para isso.
# Devido ao download, a primeira execução é lenta
use_embeddings = SentenceEmbeddings().use(sentences)
use_embeddings.shape


# SBERT
sbert_embeddings = SentenceEmbeddings().sbert(sentences)
sbert_embeddings.shape


# LASER
# Antes de utilizar o LASER, você deve fazer o download do modelo.
# Para isso, descomente a linha abaixo.
#!python -m laserembeddings download-models "data"
# Você pode informar o código de idioma (ISO 639-1), para cada sentença da lista.
# Por padrão, consideramos que todas as sentenças estão escritas em inglês ("en").
laser_embeddings = SentenceEmbeddings().laser(sentences)
laser_embeddings.shape


# mUSE
# Na primeira execução, é feito o download de arquivos de modelos e embeddings
# Certifique-se de ter pelo menos 300MB disponível em disco para isso.
# Devido ao download, a primeira execução é lenta
muse_embeddings = SentenceEmbeddings().muse(sentences)
muse_embeddings.shape


# LaBSE
labse_embeddings = SentenceEmbeddings().labse(sentences)
labse_embeddings.shape


# Exemplo baseado em:
# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder

def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")


sent_emb = SentenceEmbeddings().labse(sentences) #escolha o modelo de sentence embeddings de sua preferência
plot_similarity(sentences, sent_emb, 90)


#temporario
intents = [
    "Smartphones",
    "Smartphones",
    "Smartphones",

    "Weather",
    "Weather",
    "Weather",

    
    "Food and health",
    "Food and health",
    "Food and health",
    
    "Asking about age",
    "Asking about age"
]

intent_labse_model, X_test_labse, y_test_labse, classes = SemanticComprehension().training_intents("labse", sentences, intents)



# Epochs = 20
# Batch size = 32
# Hidden layers = 300
# Max sequence length = 280 --> tweet size
intent_bilstm_model, X_test_rnn, y_test_rnn, classes = SemanticComprehension().training_intents("bilstm", sentences, intents)



# LaBSE's performance
y_hat = intent_labse_model.predict(X_test_labse)
SemanticComprehension().plot_confusion_matrix(y_test_labse, y_hat, classes, "figs/ir-labse-cm.png")

# RNN's performance
y_hat = intent_bilstm_model.predict(X_test_rnn)        
SemanticComprehension().plot_confusion_matrix(np.argmax(y_test_rnn,axis=1), np.argmax(y_hat, axis=1), classes, "figs/ir-rnn-cm.png")


# Predição de intents para os tweets coletados
intents = CompreensaoSemantica().predicao_intencoes(intent_labse_model, sentences) ## configurado apenas para o modelo LaBSE
intents.head()


entities = []
for sentence in tqdm(sentences):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(sentence)
    entity = {}
    for i, ent in enumerate(doc.ents):
        entity[i] = {
                        "value":ent.text,
                        "entity":ent.label_,
                        "start":ent.start_char,
                        "end":ent.end_char
                    }
    entities.append(entity)

print(entities[:3])


# Frases que vamos usar como exemplo para os modelos de AS

emotional_sentences = [

    # exemplo de frase positiva
    "How good it is to live in Curitiba!",

    # exemplo de frase neutra
    "This car is grey.",

    # exemplo de frase negativa
    "Shut up, you're an idiot!",

    # exemplo de frase negativa, mas com palavras que podem confundir o modelo de AS como "friend"
    "It must be so sad to have you as a friend"
]


# Exemplo de sentenças processadas com o Emolex (frequência de emoções nas sentenças)
EmotionComprehension.emolex(emotional_sentences)


# cria um .csv com exemplos para ser processado pelo LIWC
pd.DataFrame({'text': emotional_sentences}).to_csv('data/emotional_sentences_LIWC.csv', index=False)


# carrega o arquivo com exemplos processado pelo LIWC
df_LIWC = pd.read_csv('emotional_sentences_LIWC.csv')

# apresenta apenas algumas colunas mais interessantes para AS
df_LIWC[['text', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad']]


# Credenciais do Perspective API

print("Informe seu 'API KEY'")
PERSPECTIVE_API_KEY = getpass.getpass()


# Exemplo de sentenças processadas com o Perspective API
EmotionComprehension.perspective(emotional_sentences, PERSPECTIVE_API_KEY)



