{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <center>Processamento de Linguagem Natural em Textos de Mídias Sociais: Fundamentos, Ferramentas e Aplicações</center>\n",
    "\n",
    "### <center>XXVIII Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia 2022)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Frances A. Santos (UNICAMP), Jordan Kobellarz (UTFPR), Fábio R. de Souza (USP), Leandro A. Villas (UNICAMP), Thiago H. Silva (UTFPR)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Curitiba, PR</center>\n",
    "<center>07 de Novembro de 2022</center>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/webmedia2022-nlp/course-code/blob/main/NLP_WebMedia2022.ipynb\" target=\"_parent\"><img style=\"float: right;\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir no Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.21.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.3.4)\n",
      "Requirement already satisfied: tweepy==4.10.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.10.1)\n",
      "Requirement already satisfied: praw==7.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (7.6.0)\n",
      "Requirement already satisfied: matplotlib==3.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: seaborn==0.12.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: protobuf==3.20.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: PyCrowdTangle==0.5.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: keras==2.3.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (2.3.1)\n",
      "Requirement already satisfied: sentence-transformers==2.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: laserembeddings==1.1.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: iso639-lang==2.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (1.0.2)\n",
      "Requirement already satisfied: nltk==3.4.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (3.4.5)\n",
      "Requirement already satisfied: spacy==3.4.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (3.4.1)\n",
      "Requirement already satisfied: torch==1.12.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (1.12.1)\n",
      "Requirement already satisfied: tensorflow==2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow_text==2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow_hub==0.12.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (0.12.0)\n",
      "Requirement already satisfied: gensim==4.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: bert==2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (2.2.0)\n",
      "Requirement already satisfied: bert-for-tf2==0.14.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.14.4)\n",
      "Collecting NRCLex==3.0.0\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-python-client==2.65.0\n",
      "  Downloading google_api_python_client-2.65.0-py2.py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (4.37.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: h5py in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: sentencepiece in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.1.97)\n",
      "Requirement already satisfied: tqdm in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.10.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (4.23.1)\n",
      "Requirement already satisfied: torchvision in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.13.1)\n",
      "Requirement already satisfied: transliterate==1.10.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 11)) (1.10.2)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 11)) (0.0.35)\n",
      "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 11)) (0.3.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (8.1.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (1.9.2)\n",
      "Requirement already satisfied: setuptools in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (63.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (0.6.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from torch==1.12.1->-r requirements.txt (line 16)) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.14.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.50.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (2.2.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (0.37.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from gensim==4.2.0->-r requirements.txt (line 20)) (5.2.1)\n",
      "Requirement already satisfied: erlastic in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from bert==2.2.0->-r requirements.txt (line 21)) (2.0.0)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from bert-for-tf2==0.14.4->-r requirements.txt (line 22)) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from bert-for-tf2==0.14.4->-r requirements.txt (line 22)) (0.10.2)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-api-python-client==2.65.0->-r requirements.txt (line 24)) (1.35.0)\n",
      "Requirement already satisfied: click in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings==1.1.2->-r requirements.txt (line 11)) (8.1.3)\n",
      "  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (4.2.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: mock in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from subword-nmt<0.4.0,>=0.3.6->laserembeddings==1.1.2->-r requirements.txt (line 11)) (4.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->-r requirements.txt (line 15)) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->-r requirements.txt (line 15)) (0.7.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 10)) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.13.1)\n",
      "Requirement already satisfied: filelock in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 10)) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from jinja2->spacy==3.4.1->-r requirements.txt (line 15)) (2.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (3.9.0)\n",
      "Building wheels for collected packages: NRCLex\n",
      "  Building wheel for NRCLex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43310 sha256=64dd5dff345cf78329538d214e770a8d651de1b29074ba2c9d8f2dcf30d6a54e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-myv880p2/wheels/83/95/c0/42b43fb15eb48e4f5a67cba8915540cb2783591c59c037a9e5\n",
      "Successfully built NRCLex\n",
      "Installing collected packages: uritemplate, httplib2, googleapis-common-protos, textblob, NRCLex, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed NRCLex-3.0.0 google-api-core-2.8.2 google-api-python-client-2.65.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.4 httplib2-0.21.0 textblob-0.17.1 uritemplate-4.1.1\n",
      "2022-11-01 15:46:47.203291: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-01 15:46:47.203312: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-01 15:46:47.203339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NOTE-665): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.4.0\n",
      "    Uninstalling en-core-web-sm-3.4.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.4.0\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm #Instalando dependências específicas do spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 17:26:12.640103: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-01 17:26:12.640121: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-01 17:26:12.640157: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NOTE-665): /proc/driver/nvidia/version does not exist\n",
      "Using TensorFlow backend.\n",
      "/home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import getpass\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação do diretório \"data/\"\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from DataExtraction import DataExtraction\n",
    "from Preprocessing import Preprocessing\n",
    "from ModelosRepresentacao import ModelosEstatisticos, SentenceEmbeddings, WordEmbeddings\n",
    "from ExtracaoConhecimento import Clustering, SemanticComprehension, SentimentAnalysis\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download de arquivos usados por bibliotecas\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Agenda\n",
    "\n",
    "<img src=\"figs/agenda.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>Introdução</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Por que mídias sociais?\n",
    "\n",
    "Mídias sociais são acessadas por aproximadamente 4,7 bilhões de usuários em todo o planeta (i.e., 59% da população) [Kemp 2022]\n",
    "\n",
    "<img src=\"figs/social-media.jpeg\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo (Twitter):\n",
    "\n",
    "* 200 Bilhões de postagens por ano\n",
    "* equivalente a 6 mil postagens por segundo\n",
    "\n",
    "Fonte: [Twitter Usage Statistics](https://www.internetlivestats.com/twitter-statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Possibilidades para acessar dados públicos em larga escala\n",
    "\n",
    "* **Twitter** - API (stream e histórico)\n",
    "* **Reddit** - API (stream e histórico)\n",
    "* **Meta (Instagram e Facebook)** - Plataforma CrowdTangle\n",
    "* **Swarm (Forsquare)** - API\n",
    "* **Flickr** - API\n",
    "* **Google Places** - API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Valiosa fonte de dados para diversas aplicações\n",
    "\n",
    "* **Na Academia**\n",
    "    * Análise de fenômenos sociais\n",
    "    * Sensoriamento social\n",
    "    * Detecção de notícias falsas\n",
    "    * Discurso de ódio\n",
    "    * Polarização política\n",
    "\n",
    "* **Na Indústria**\n",
    "    * Benchmarking (comparação com concorrentes)\n",
    "    * Forecasting (análise de tendências)\n",
    "    * Sistemas de recomendação\n",
    "    * Personalização / customização em larga escala\n",
    "    * Análise de risco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Por que textos?\n",
    "\n",
    "Textos escritos em linguagem natural, <br>estão presentes na maioria dos dados disponíveis de mídias sociais\n",
    "\n",
    "<img src=\"figs/social-media-content.png\" style=\"float: right; zoom:80%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>1. Textos de mídias sociais <br>Suas principais características e como coletá-los</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Onde encontrar dados textuais?\n",
    "\n",
    "* Postagens\n",
    "* Artigos\n",
    "* Mensagens / comentários\n",
    "* Metainformações de páginas, imagens, videos, perfis, postagens, mensagens, etc. \n",
    "* Extração de texto em imagem, áudio e video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mídias sociais consideradas\n",
    "\n",
    "\n",
    "| <img src=\"figs/twitter.png\" style=\"float: center; zoom:20%;\" /> | <img src=\"figs/reddit.png\" style=\"float: top center; zoom:50%;\" /> | <img src=\"figs/facebook.png\" style=\"float: center; zoom:40%;\" /> |\n",
    "| --- | --- | --- |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Twitter\n",
    "\n",
    "* Mídia social de *Microblogging*\n",
    "* Mensagens limitadas a 280 caracteres\n",
    "* Uma das primeiras redes a disponibilizar uma API para extração de dados públicos em larga escala\n",
    "* Possibilidade de coleta de dados históricos ou em tempo real (*streaming*)\n",
    "* Qualquer dado público pode ser acessado, exceto os de perfis privados (menos de 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Característica proeminente\n",
    "\n",
    "Simplicidade nas interações e dicionário de dados:\n",
    "\n",
    "* tweets\n",
    "* hashtags #\n",
    "* menções @\n",
    "* retweets RT\n",
    "* respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "* **texto do tweet**\n",
    "* **timestamp**\n",
    "* **autor**\n",
    "    * nome\n",
    "    * localização\n",
    "    * se é verificado\n",
    "    * quantidade de seguidores, amigos, postagens\n",
    "    * data de criação da conta\n",
    "    * língua do perfil\n",
    "    * etc.\n",
    "* **geolocalização do tweet (GeoJson)**\n",
    "    * adicionada explícitamente\n",
    "    * ou capturada do dispositivo que gerou o tweet\n",
    "* **entidades**\n",
    "    * hashtags\n",
    "    * links\n",
    "    * menções\n",
    "    * mídias\n",
    "* **sinais sociais**\n",
    "    * quantidade de retweets\n",
    "    * quantidade de curtidas\n",
    "    * quantidade de respostas\n",
    "* etc. \n",
    "\n",
    "Conheça o [dicionário completo de dados de um tweet aqui](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de um tweet\n",
    "\n",
    "Alguns campos foram omitidos para facilitar a visualização. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"created_at\": \"Thu Apr 06 15:24:15 +0000 2017\",\n",
    "  \"id_str\": \"850006245121695744\",\n",
    "  \"text\": \"1\\/ Today we\\u2019re sharing our vision for the future of the Twitter API platform!\\nhttps:\\/\\/t.co\\/XweGngmxlP\",\n",
    "  \"user\": {\n",
    "    \"id\": 2244994945,\n",
    "    \"name\": \"Twitter Dev\",\n",
    "    \"screen_name\": \"TwitterDev\",\n",
    "    \"location\": \"Internet\",\n",
    "    \"url\": \"https:\\/\\/dev.twitter.com\\/\",\n",
    "    \"description\": \"Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\\/\\/twittercommunity.com\\/ \\u2328\\ufe0f #TapIntoTwitter\"\n",
    "  },\n",
    "  \"place\": {   \n",
    "  },\n",
    "  \"entities\": {\n",
    "    \"hashtags\": [      \n",
    "    ],\n",
    "    \"urls\": [\n",
    "    ],\n",
    "    \"user_mentions\": [     \n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Limite de 280 caracteres\n",
    "    * restringe capacidade argumentativa\n",
    "    * usuários contornam com uso de contrações de palavras, gírias da internet e emojis\n",
    "* Representatividade da população\n",
    "    * pode não representar bem o usuário médio de Internet\n",
    "    * [tendem a ser usadas por pessoas mais jovens, com maior renda e grau de escolaridade](https://blogs.oii.ox.ac.uk/policy/did-you-consider-twitters-lack-of-representativeness-before-doing-that-predictive-study/)\n",
    "* Representatividade do retorno da API\n",
    "    * a API de streaming se baseia na [**relevância** e não **completude** dos dados](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview)\n",
    "* Alta incidência de contas robô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Tweets\n",
    "\n",
    "- Utilizaremos a Twitter API v2 para coletar os tweets \n",
    "- Acesse a [página de desenvolverdores](https://developer.twitter.com/) e obtenha suas credenciais de acesso\n",
    "- Limitamos cada coleta a **10 tweets**, mas todo o conteúdo dos tweets é adicionado (*appending*) ao arquivo local data/tweets.json\n",
    "- Além dos campos *id* e *text* que estão presentes nos tweets por padrão, também solicitamos os campos *created_at, entities, geo, lang, public_metrics, source*. Para ver a lista completa de campos possíveis, acesse esta [página](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)\n",
    "- Filtramos para selecionar apenas os tweets escritos em Inglês ( *lang = \"en\"* ) e que contenham o termo \"nyc\", que referencia a cidade de Nova Iorque\n",
    "- Após coletar os tweets, extraímos os valores dos campos *text, timestamp_ms, ...* e retornamos no formato Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe seu 'API KEY'\n",
      "········\n",
      "Informe seu 'API KEY SECRET'\n",
      "········\n",
      "Informe seu 'ACCESS TOKEN KEY'\n",
      "········\n",
      "Informe seu 'ACCESS TOKEN SECRET'\n",
      "········\n",
      "Informe seu 'Bearer TOKEN'\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "# Credenciais da API do Twitter\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "twitter_consumer_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'API KEY SECRET'\")\n",
    "twitter_consumer_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN KEY'\")\n",
    "twitter_access_token_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN SECRET'\")\n",
    "twitter_access_token_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'Bearer TOKEN'\")\n",
    "twitter_bearer_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-01T19:38:01.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294130...</td>\n",
       "      <td>0</td>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>83</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-01T19:38:00.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294087...</td>\n",
       "      <td>0</td>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>241</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-01T19:38:03.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294203...</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>124</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-01T19:38:03.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294217...</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>121</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-01T19:38:03.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294210...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>132</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0  2022-11-01T19:38:01.000Z   \n",
       "1  2022-11-01T19:38:00.000Z   \n",
       "2  2022-11-01T19:38:03.000Z   \n",
       "3  2022-11-01T19:38:03.000Z   \n",
       "4  2022-11-01T19:38:03.000Z   \n",
       "\n",
       "                                                 url  score  \\\n",
       "0  https://twitter.com/anyuser/status/15875294130...      0   \n",
       "1  https://twitter.com/anyuser/status/15875294087...      0   \n",
       "2  https://twitter.com/anyuser/status/15875294203...      0   \n",
       "3  https://twitter.com/anyuser/status/15875294217...      0   \n",
       "4  https://twitter.com/anyuser/status/15875294210...      0   \n",
       "\n",
       "                                                text  length geo  \n",
       "0  NY. Every corner this past Sunday … https://t....      83  {}  \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...     241  {}  \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...     124  {}  \n",
       "3  RT @YB_2: The NYC transparency law goes into e...     121  {}  \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...     132  {}  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Coleta de tweets\n",
    "try:\n",
    "    df_tweets = DataExtraction().twitter(\n",
    "        twitter_consumer_key, \n",
    "        twitter_consumer_secret, \n",
    "        twitter_access_token_key, \n",
    "        twitter_access_token_secret, \n",
    "        twitter_bearer_token\n",
    "    )\n",
    "except:\n",
    "    ## Caso aconteça algum erro durante a coleta, é carregado o arquivo existente\n",
    "    with open(\"data/tweets.json\", \"r\") as tweet_file:\n",
    "        tweets = json.load(tweet_file)\n",
    "        data = [{\n",
    "                \"created_at\": item[\"created_at\"],\n",
    "                \"url\": \"https://twitter.com/anyuser/status/\" + item[\"id\"],\n",
    "                \"score\": item[\"public_metrics\"][\"like_count\"],\n",
    "                \"text\": item[\"text\"],\n",
    "                \"length\":len(item[\"text\"]),\n",
    "                \"geo\": item[\"geo\"]\n",
    "                } for item in tweets]\n",
    "        df_tweets = pd.DataFrame(data)\n",
    "    \n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Reddit\n",
    "\n",
    "* Mídia social baseada em fóruns de discussão\n",
    "* Comunidades/fóruns → \\subreddits\n",
    "* Mais de 100K comunidades e 50 mi de usuários ativos diariamente em [2020](https://www.redditinc.com/advertising/audience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* Sistema de moderação autoorganizável\n",
    "    * \\subreddits possuem regras próprias criadas pelos moderadores e membros\n",
    "    * algumas comunidades possuem alto nível de comprometimento com as regras propostas\n",
    "    * mecanismos de recompensa para colaboradores ativos\n",
    "* Possibilidade de coletar dados em stream e histórico\n",
    "    * vantagem de permitir a recuperação do histórico completo\n",
    "* Permite acesso à qualquer informação disponível publicamente\n",
    "    * inclui postagens, comentários, perfis, comunidades e suas respectivas metainformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "**[submission (postagem)](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html)**\n",
    "* id\n",
    "* url\n",
    "* permalink\n",
    "* created_utc\n",
    "* title\n",
    "* selftext (conteúdo da postagem)\n",
    "* score (número de upvotes)\n",
    "* [author](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (Redditor)\n",
    "    * name\n",
    "    * created_utc\n",
    "    * comment_karma (pontuação do usuário)\n",
    "    * has_verified_email\n",
    "    * etc.\n",
    "* [comments](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (árvore de comentários -- necessário percorrer com método específico para isso)\n",
    "    * author (Redditor)\n",
    "    * body\n",
    "    * distinguished\n",
    "    * etc.\n",
    "* distinguished (se a postagem foi destacada pelo moderador)\n",
    "* edited (se a postagem foi editada)\n",
    "* is_original_content (se foi marcada automaticamente como conteúdo original)\n",
    "* over_18 (se é conteúdo para maiores de 18 anos)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* a plataforma permite um alto grau de anonimidade\n",
    "    * é encorajado o uso de pseudônimo\n",
    "    * é possível fazer cadastro sem verificação\n",
    "    * abertura para comportamentos anti-éticos em comunidades não moderadas / permissivas\n",
    "* cada comunidade possui regras próprias\n",
    "    * práticas de moderação distintas\n",
    "    * dificultando a comparação \n",
    "* liberdade no formato\n",
    "    * campo aberto com possibiidade de uso de html e markdown\n",
    "* alta incidência de bots\n",
    "    * criam, fazem a curadoria e moderam conteúdos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts de \\subreddits\n",
    "\n",
    "- Utilizaremos a [API do Reddit](https://www.reddit.com/dev/api) para coletar posts\n",
    "- Você deverá criar uma conta para acessar a API em [reddit.com](https://reddit.com)\n",
    "- Depois de criar a conta, obtenha os Client ID e o Client Secret\n",
    "- No exemplo a seguir, coletamos os top 100 posts de 5 subreddits, contendo o texto, url, número de comentários, data de criação e score (número de upvotes do post)\n",
    "- Os dados são salvos no arquivo local data/reddit_posts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do Reddit\n",
    "\n",
    "print(\"Informe seu 'CLIENT ID'\")\n",
    "REDDIT_CLIENT_ID = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'CLIENT SECRET'\")\n",
    "REDDIT_CLIENT_SECRET = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Subreddits com discussões sérias sobre assuntos como política, história e ciência.\n",
    "subreddits = [\n",
    "    'politics',\n",
    "    'AskHistorians',\n",
    "    'changemyview',\n",
    "    'COVID19',\n",
    "    'EverythingScience',\n",
    "    'science'\n",
    "]\n",
    "\n",
    "# Coleta os top 100 posts de cada Subreddit\n",
    "df_reddit_posts = DataExtraction().reddit(\n",
    "    REDDIT_CLIENT_ID,\n",
    "    REDDIT_CLIENT_SECRET,\n",
    "    subreddits=subreddits,\n",
    "    top_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Apresenta alguns posts com texto\n",
    "df_reddit_posts[df_reddit_posts['length'] > 0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Facebook (Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* Alto grau de controle de privacidade\n",
    "* O anonimato é desencorajado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Formas de obter dados\n",
    "\n",
    "* Via API nativa (limitada)\n",
    "* Via web scraping (desencorajado)\n",
    "* Via polls com usuários (não escalável)\n",
    "* Via programa [Social Science One](https://socialscience.one/grant-process) (acesso direto à base do Facebook | difícil acesso | apenas para uso acadêmico)\n",
    "* Via plataforma do [CrowdTangle](https://crowdtangle.com) (dados limitados a páginas e grupos famosos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### [CrowdTangle](https://crowdtangle.com)\n",
    "\n",
    "* Iniciativa da Meta criada para jornalistas, agências de checagem de fatos, profissionais de marketing e pesquisadores\n",
    "* Possibilidade de consultar e visualizar dados em tempo real pela interface (dashboards)\n",
    "* Mesmos dados apresentados na interface podem ser obtidos via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Informações que **podem** ser coletadas:\n",
    "     * quando algo foi postado\n",
    "     * tipo do post (video, imagem, texto)\n",
    "     * página, conta ou grupo onde o conteúdo foi postado\n",
    "     * quantidade de interações (likes, reações, comentários, compartilhamentos, visualizações de videos)\n",
    "     * páginas públicas ou contas que compartilharam o conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Informações que **não podem** ser coletadas:\n",
    "    * alcance ou impressões de um post\n",
    "    * conteúdos efêmeros, como stories, por exemplo\n",
    "    * informações demográficas de usuários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* A base de dados disponível se limita a:\n",
    "    * contas famosas (aprox. ~7 mi de páginas, grupos ou perfis verificados em 08/06/2021), incluindo:\n",
    "        * páginas públicas com mais de 50K curtidas\n",
    "        * grupos públicos com mais de 95K membros\n",
    "        * grupos públicos dos Estados Unidos com mais de 2K membros\n",
    "        * todos os perfis verificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de retorno da API do CrowdTangle\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"status\": 200,\n",
    "    \"result\": {\n",
    "        \"posts\": [\n",
    "            {\n",
    "                \"platformId\": \"47657117525_10154014482272526\",\n",
    "                \"platform\": \"Facebook\",\n",
    "                \"date\": \"2016-02-12 23:38:14\",\n",
    "                \"updated\": \"2020-08-23 05:48:22\",\n",
    "                \"type\": \"live_video_complete\",\n",
    "                \"message\": \"Draymond at Foot Locker for #NBAAllStarTO with a special shoutout to #DubNation.\",\n",
    "                \"expandedLinks\": [\n",
    "                    {\n",
    "                        \"original\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                        \"expanded\": \"https://www.facebook.com/warriors/videos/10154014482272526/\"\n",
    "                    }\n",
    "                ],\n",
    "                \"link\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                \"postUrl\": \"https://www.facebook.com/warriors/posts/10154014482272526\",\n",
    "                \"subscriberCount\": 6041837,\n",
    "                \"score\": 4.750579867017164,\n",
    "                \"media\": [\n",
    "                    {\n",
    "                        \"type\": \"video\",\n",
    "                        \"url\": \"https://video-sea1-1.xx.fbcdn.net/v/t42.1790-29/12718926_1213464465334694_1083747983_n.mp4?_nc_cat=109&_nc_sid=985c63&efg=eyJybHIiOjQ0MiwicmxhIjoxNDIwLCJ2ZW5jb2RlX3RhZyI6InYyXzQwMF9jcmZfMjdfbWFpbl8zLjBfc2QifQ%3D%3D&_nc_ohc=e7Ygz2qv-24AX-wSWX2&rl=442&vabr=246&_nc_ht=video-sea1-1.xx&oh=889e0d776d92a84bb57099cad3d28d55&oe=5F43C879\",\n",
    "                        \"height\": 0,\n",
    "                        \"width\": 0\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"photo\",\n",
    "                        \"url\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\",\n",
    "                        \"height\": 400,\n",
    "                        \"width\": 400,\n",
    "                        \"full\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\"\n",
    "                    }\n",
    "                ],\n",
    "                \"statistics\": {\n",
    "                    \"actual\": {\n",
    "                        \"likeCount\": 24235,\n",
    "                        \"shareCount\": 753,\n",
    "                        \"commentCount\": 5675,\n",
    "                        \"loveCount\": 33,\n",
    "                        \"wowCount\": 18,\n",
    "                        \"hahaCount\": 3,\n",
    "                        \"sadCount\": 0,\n",
    "                        \"angryCount\": 5,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 0\n",
    "                    },\n",
    "                    \"expected\": {\n",
    "                        \"likeCount\": 3927,\n",
    "                        \"shareCount\": 279,\n",
    "                        \"commentCount\": 1041,\n",
    "                        \"loveCount\": 1046,\n",
    "                        \"wowCount\": 94,\n",
    "                        \"hahaCount\": 45,\n",
    "                        \"sadCount\": 14,\n",
    "                        \"angryCount\": 19,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 2\n",
    "                    }\n",
    "                },\n",
    "                \"account\": {\n",
    "                    \"id\": 19889,\n",
    "                    \"name\": \"Golden State Warriors\",\n",
    "                    \"handle\": \"warriors\",\n",
    "                    \"profileImage\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t1.0-1/p200x200/74788912_10158146665972526_3545220405897723904_n.jpg?_nc_cat=1&ccb=2&_nc_sid=dbb9e7&_nc_ohc=9snUpG_pdlQAX90IhWM&_nc_ht=scontent-sea1-1.xx&tp=6&oh=f8a3d3b62b507966ecc68de3b557fe84&oe=5FBF1185\",\n",
    "                    \"subscriberCount\": 11580228,\n",
    "                    \"url\": \"https://www.facebook.com/47657117525\",\n",
    "                    \"platform\": \"Facebook\",\n",
    "                    \"platformId\": \"47657117525\",\n",
    "                    \"accountType\": \"facebook_page\",\n",
    "                    \"pageAdminTopCountry\": \"US\",\n",
    "                    \"verified\": true\n",
    "                },\n",
    "                \"videoLengthMS\": 307968,\n",
    "                \"liveVideoStatus\": \"completed\",\n",
    "                \"Id\": \"19889|10154014482272526\",\n",
    "                \"legacyid\": 1686762829\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Dados limitados a contas famosas\n",
    "    * contas menos famosas são subrepresentadas\n",
    "* Não é possível saber quem reagiu ou comentou em posts\n",
    "* Ferramenta muito nova / pouco explorada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts no CrowdTangle\n",
    "\n",
    "* Utilizaremos a API do CrowdTangle para extrair posts do Facebook -- [documentação](https://github.com/CrowdTangle/API/wiki)\n",
    "* O primeiro passo é criar uma conta no CrowdTangle, depois criar um dashboard e obter o token da API para acessar os dados do dashboard\n",
    "* Para coletar posts via API, é necessário criar pelo menos uma lista em seu dashboard recém criado\n",
    "* Em nosso caso, criaremos uma lista para monitorar posts de páginas de mídias de notícias, incluindo CNN, NYT, BBC, NBC, NPR, Reuters, etc. \n",
    "* [Esse video explica como usar a interface do CrowdTangle para criar listas](https://vimeo.com/588999918). \n",
    "* [Esse video explica como acessar os dados via API](https://vimeo.com/453763307) explicando como executar todos os procedimentos acima\n",
    "\n",
    "> Observação: não é possível criar uma lista via API, somente pela interface do dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do CrowdTangle\n",
    "\n",
    "print(\"Informe seu 'API_TOKEN'\")\n",
    "CROWDTANGLE_API_TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui coletamos os top 100 posts em cada mês, iniciando em start_date e terminando em end_date\n",
    "df_facebook_posts = DataExtraction().facebook(\n",
    "    CROWDTANGLE_API_TOKEN, \n",
    "    search_term='covid-19',\n",
    "    start_date = '2020-04-01',\n",
    "    end_date = '2021-04-01'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Amostra de posts do Facebook\n",
    "df_facebook_posts.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>2. Pré-processamento Textual</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é um método para..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RT @MichelleKinney: The Russian Consulate in N...</td>\n",
       "      <td>rt @michellekinney: the russian consulate in n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "51  RT @MichelleKinney: The Russian Consulate in N...   \n",
       "4   Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                      normalized_text  \n",
       "51  rt @michellekinney: the russian consulate in n...  \n",
       "4   tune in to the @districtingnyc public meeting ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Preprocessing import Preprocessing\n",
    "\n",
    "pipeline = Preprocessing()\n",
    "\n",
    "data = df_tweets[['text']].copy()\n",
    "data[\"normalized_text\"] = data['text'].apply(pipeline.normalization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex (Expressoes regulares)\n",
    "\n",
    "As expressão regulares são..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , saving,  ...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , save,   ,...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , saving,  ...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , save,   ,...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_links = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_links, value='LINK')\n",
    "data['clean_text'] = data['normalized_text'].apply(apply_regex)\n",
    "\n",
    "re_mentions = r'@([A-Za-z0-9_]+)'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_mentions, value='USERNAME')\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "re_newline = '\\\\n'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_newline)\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "re_special_char = '\\W+'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_special_char)\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, LINK]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[USERNAME, LINK, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, USERNAME, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , save,   ,...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, USERNAME, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, USERNAME, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, LINK]   \n",
       "1  [USERNAME, LINK, please, donate, we, greatly, ...   \n",
       "2  [rt, USERNAME, daylight, saving, time, ends, t...   \n",
       "3  [rt, USERNAME, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, USERNAME, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , save,   ,...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['clean_text'].apply(pipeline.tokenization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stemmização é um método para..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, LINK]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[USERNAME, LINK, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, USERNAME, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , save,   ,...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, USERNAME, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, USERNAME, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, LINK]   \n",
       "1  [USERNAME, LINK, please, donate, we, greatly, ...   \n",
       "2  [rt, USERNAME, daylight, saving, time, ends, t...   \n",
       "3  [rt, USERNAME, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, USERNAME, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , save,   ,...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stems'] = data['tokens'].apply(pipeline.stemming)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, LINK]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, LINK]</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[USERNAME, LINK, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[USERNAME, LINK, please, donate, we, greatly, ...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, USERNAME, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt, USERNAME, daylight, saving, time, end, th...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, USERNAME, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt, USERNAME, the, nyc, transparency, law, go...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, USERNAME, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune, in, to, the, USERNAME, public, meeting,...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, LINK]   \n",
       "1  [USERNAME, LINK, please, donate, we, greatly, ...   \n",
       "2  [rt, USERNAME, daylight, saving, time, ends, t...   \n",
       "3  [rt, USERNAME, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, USERNAME, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0      [ny, every, corner, this, past, sunday, LINK]   \n",
       "1  [USERNAME, LINK, please, donate, we, greatly, ...   \n",
       "2  [rt, USERNAME, daylight, saving, time, end, th...   \n",
       "3  [rt, USERNAME, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, USERNAME, public, meeting,...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmas'] = data['tokens'].apply(pipeline.lemmatization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outra Alternativa: Pipeline de NLP do Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, link]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, link]</td>\n",
       "      <td>[PROPN, DET, NOUN, PRON, ADJ, PROPN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[username, link, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[username, link, please, donate, we, greatly, ...</td>\n",
       "      <td>[PROPN, PROPN, INTJ, VERB, PRON, ADV, VERB, DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, username, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt, username, daylight, save, time, end, this...</td>\n",
       "      <td>[PROPN, PROPN, NOUN, VERB, NOUN, VERB, DET, PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, username, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt, username, the, nyc, transparency, law, go...</td>\n",
       "      <td>[PROPN, PROPN, DET, PROPN, PROPN, NOUN, VERB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, username, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune, in, to, the, username, public, meeting,...</td>\n",
       "      <td>[NOUN, ADP, ADP, DET, PROPN, ADJ, NOUN, NOUN, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, link]   \n",
       "1  [username, link, please, donate, we, greatly, ...   \n",
       "2  [rt, username, daylight, saving, time, ends, t...   \n",
       "3  [rt, username, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, username, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0      [ny, every, corner, this, past, sunday, link]   \n",
       "1  [username, link, please, donate, we, greatly, ...   \n",
       "2  [rt, username, daylight, save, time, end, this...   \n",
       "3  [rt, username, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, username, public, meeting,...   \n",
       "\n",
       "                                            pos tags  \n",
       "0        [PROPN, DET, NOUN, PRON, ADJ, PROPN, PROPN]  \n",
       "1  [PROPN, PROPN, INTJ, VERB, PRON, ADV, VERB, DE...  \n",
       "2  [PROPN, PROPN, NOUN, VERB, NOUN, VERB, DET, PR...  \n",
       "3  [PROPN, PROPN, DET, PROPN, PROPN, NOUN, VERB, ...  \n",
       "4  [NOUN, ADP, ADP, DET, PROPN, ADJ, NOUN, NOUN, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fazendo tudo de uma só vez com Spacy\n",
    "\n",
    "data['tokens'], data['pos tags'], data['lemmas'] = zip(*data['clean_text'].apply(pipeline.nlp_pipeline))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>3. Representação de Textos <br> Utilizando Vetores Numéricos</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "- É a forma mais simples de representação de palavras para um algoritmo de aprendizado de máquina\n",
    "- Cada documento é representado por um vetor de tamanho N, onde N é a quantidade de tokens distintos no vocabulário\n",
    "- Cada token único é representado por uma posição no vetor, a ser preenchida com a quantidade de vezes que o token ocorre no documento\n",
    "- Caso não haja nenhuma ocorrência de um token no documento, sua respectiva posição recebe o valor 0\n",
    "\n",
    "<center> Tabela: Exemplo de representação utilizando BoW. Adaptade de Machine Learning Mastery<sup>1</sup> </center>\n",
    "\n",
    "| Documento | it | was | the | best | of| times | worst | age | wisdom | foolishness |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| it was the best of times | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "| it was the worst of times | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| it was the age of wisdom | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 |\n",
    "| it was the age of foolishness | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">1</a>: Disponível em https://machinelearningmastery.com/gentle-introduction-bag-words-model/. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - facilidade de implementação\n",
    "    - ajuda a identificar as palavras significativas de um texto, baseado em sua frequência\n",
    "\n",
    "- **Desvantagens**\n",
    "    - grande consumo de memória para vocabulários muito extensos\n",
    "    - representações esparsas e apresentando enviesamento em relação a termos muito frequentes\n",
    "    - não considera a posição das palavras no texto:\n",
    "    \n",
    "    \n",
    "| Documento | I | went | to | the | cinema | and | liked | movie | but | not | popcorn\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| I went to the cinema and liked the movie but not the popcorn | 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "| I went to the cinema and liked the popcorn but not the movie| 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<80x623 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1273 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelos_stats = ModelosEstatisticos()\n",
    "modelos_stats.bow(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "- Dado um conjunto de documentos textuais, **palavras que ocorrem em muitos destes documentos provavelmente não serão relevantes para distinguir o conteúdo de cada um deles** \\[Robertson 2004\\]\n",
    "- Para superar essa limitação (que ocorre com o BoW), o TF-IDF primeiro calcula a frequência com que um determinado termo ocorra em um documento (i.e., TF)\n",
    "\n",
    "- E, então, é realizada a ponderação com a frequência com que o mesmo termo ocorra em um conjunto de documentos (i.e., IDF):\n",
    "\n",
    "<img src=\"figs/idf.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "onde $N$ corresponde ao número de documentos analisados e $df_i$ corresponde ao número de documentos em que ocorre o termo em questão.\n",
    "\n",
    "Assim, podemos representar cada elemento de uma matriz termo × documento por:\n",
    "\n",
    "<img src=\"figs/tfidf.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - o TF-IDF representou um passo importante para o desenvolvimento de técnicas relacionadas à extração de informação\n",
    "    - Tendo depois se expandido para outras técnicas de NLP, como extração de tópicos e classificação de texto\n",
    "\n",
    "- **Desvantagens**\n",
    "    - representação vetorial resultante ter o mesmo tamanho do vocabulário do texto (grande consumo de memória)\n",
    "    - não considera as proximidades semânticas em que os termos ocorrem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<80x135 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelos_stats.tfidf(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#TO-DO: PCA/SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings\n",
    "\n",
    "- Com o Word2Vec, proposto por \\[Mikolov et al. 2013\\] inaugurou-se um novo paradigma de representação semântica vetorial: Word Embeddings\n",
    "- Principal característica é a atribuição de um vetor denso, de tamanho arbitrário, a cada palavra de um corpus, gerado a partir do treinamento de redes neurais\n",
    "- Esses vetores são gerados a partir da análise das **janelas semânticas** em que tais palavras venham a ocorrer\n",
    "- Resultou em uma série de inovações e vantagens:\n",
    "    - os vetores não precisam ser treinados apenas no corpus em que será feita a análise. Normalmente, são utilizados vetores pré-treinados sobre um corpus com vocabulário mais extenso, como a Wikipedia, e apenas são otimizados sobre o corpus avaliado\n",
    "    - É eliminado o problema da esparsidade de dados\n",
    "    - Resultados experimentais indicam que relações semânticas complexas podem ser capturadas, por exemplo: \n",
    "        - a relação entre os nomes de países e suas respectivas capitais\n",
    "        - sinônimos obtém representações mais próximas, enquanto antônimos se distanciam de forma equivalente no espaço vetorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "O Word2Vec é composto por dois modelos:\n",
    "- **Continuous Bag of Words (CBOW):** toma-se como entrada uma janela de palavras, e tenta-se prever qual palavra ocorreria naquele contexto\n",
    "- **Skip-Gram:** toma-se como entrada uma palavra, e a partir dela, tenta-se prever as palavras que venham a ocorrer em sua vizinhança\n",
    "\n",
    "Veja a arquitetura de ambos os modelos na ilustração a seguir:\n",
    "\n",
    "<img src=\"figs/word2vec.png\" style=\"float: center; zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Treinamento do modelo Word2Vec Skip-gram\n",
    "\n",
    " - toma como entrada um corpus de texto, com um tamanho $N$ de vocabulário\n",
    " - Inicialmente, são atribuídos valores aleatórios para cada um dos vetores das palavras do vocabulário\n",
    " - Os pesos são ajustados ao longo do treinamento, para que:\n",
    "     - palavras ocorrendo em contextos semelhantes obtenham representações vetoriais (embeddings) próximos\n",
    "     - e, palavras de significado distante, que não costumam ocorrer nos mesmos contextos, obtenham representações o mais distante possíveis entre si\n",
    " - Em cada palavra analisada por iteração, as instâncias positivas são palavras que realmente ocorrem em sua proximidade, e as negativas, uma seleção, de tamanho proporcional, de palavras que não ocorrem\n",
    " - O treinamento do algoritmo irá, então, minimizar a função de perda (Loss Function), cujo objetivo é maximizar o produto escalar entre uma palavra e um exemplo positivo de contexto, e minimizar em relação aos exemplos negativos\n",
    " \n",
    "- O tamanho do contexto observado é arbitrário, sendo definido como um parâmetro de treinamento\n",
    "\n",
    "- Ao final, duas representações são aprendidas:\n",
    "    - uma matriz $W$ contendo em cada vetor $w_i$ um word embedding para cada palavra do vocabulário\n",
    "    - e, uma matriz $C$ em que cada vetor $c_i$ é um embedding relativo ao contexto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemplo Word2Vec Skip-gram\n",
    "\n",
    "Adaptado de [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "- Arquitetura do modelo:\n",
    "\n",
    "<img src=\"figs/skipgramarch.png\" style=\"float: center; zoom:80%;\" />\n",
    "\n",
    "- Word embeddins de uma determinada palavra:\n",
    "\n",
    "<img src=\"figs/matrix_mult_w_one_hot.png\" style=\"float: center; zoom:80%;\" />\n",
    "\n",
    "- Cálculo do resultado na camada de saída:\n",
    "\n",
    "<img src=\"figs/output_weights_function.png\" style=\"float: center; zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Criando uma modelo de Word Embeddings com Word2Vec\n",
    "\n",
    "* Utilizaremos a biblioteca Gensim 4.2.0 -- [documentação](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* Vamos utilizar os tokens obtidos após a etapa de pré-processamento, uma vez que a entrada deve ser uma lista de tokens, como por exemplo:\n",
    "\n",
    "<center>$[[\"first\", \"sentence\"], [\"second\", \"sentence\"]]$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cria o modelo e mostra o tamanho do vocabulário\n",
    "w2v_model = WordEmbeddings().word2vec(data[\"tokens\"])\n",
    "len(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['username', 'nyc', 'link', 'rt', 'this', 'the', 'is', 'to', 'i', 'in', 'and', 've', 'you', 'times', 'watched', 'a', 'peak', 'ten', 'for', 'of', 's', 'are', 'on', 'at', 'my', 'it', 'me', 'next', 'an', 'have', 'city', 'we', 'be', 'new', 't', 'your', 'just', '10', 'can', 'november', 'if', 'their', 'from', 'time', 'all', 'day', 'was', 'help', 'state', 'mayor', 'has', 'they', 'not', 'up', 'cars', 'with', 'consulate', 'border', 'because', 'or', 'now', 'our', 'cities', 'take', 'today', 'them', 'as', 'security', 'by', 'sunday', 'amp', 'will', 'real', 'like', 'please', 'make', 'two', 'no', 'week', 'wow', 'should', 'social', 'payments', 'low', '19', '5th', '12', 'd', 'home', 'that', 'his', 'right', 'york', 'really', 'gt', 'angle', 'building', 'didn', 'always', 'down', '1st', 'do', 'want', 'when', 'what', 'tickets', 'more', 'appreciate', 'wei', 'get', 'but', '5', 'stuff', 'would', 'holes', 'over', 'unwittingly', 'rent', 'becoming', 'adams', 'nypd', 'man', 'crime', 'so', 'min', 'trip', 'rth', 'high', 'many', 'subway', 'voice', 'school', 'brandon', 'hope', 'say', 'judd', 'old', 're', 'tonight', 'train', 'story', 'first', 'sure', 'enter', '4', 'fx', 'see', 'here', 'don', 'got', 'order', 'russian', 'into', 'tomorrow', 'officer', 'nursing', 'benefits', 'allowing', 'thera', 'healthcare', 'cover', 'truetelehealth', 'warned', 'back', 'telehealth', 'happen', 'nightmare', '1980', 'telemedicine', 'grew', 'homes', 'shoot', 'turnstile', 'students', 'disturbed', 'cooked', 'loony', 'yourself', 'common', 'sense', 'shrink', 'support', 'go', 'wayne', 'rot', 'nj', 'who', 'lonely', 'happy', 'pa', 'paweek', 'wefindaway', 'fmty', 'available', 'seriously', 'democrats', 'jumpers', 'cop', 'then', 'polls', 'open', 'cast', 'democrat', 'took', 'away', 'qualified', 'immunity', 'risk', 'terrorists', 'throw', 'rocks', 'scream', 'faces', 'while', 'sitting', 'thank', 'antifa', 'blm', 'msm', 'couldn', 'pretty', 'reli', 'tune', 'public', 'meeting', '6', 'pm', 'acc', 'partner', 'rescues', 'there', 'dozens', 'only', 'handful', 'consistently', 'team', 'year', 'uncut', 'refusing', 'supreme', 'court', 'reinstate', 'unvaccinated', 'officers', 'could', 'posted', 'texas', 'needs', 'spend', 'billion', 'range', 'salary', 'max', 'required', 'every', 'corner', 'past', 'donate', 'greatly', 'any', 'aid', 'during', 'eviction', 'homelessness', 'imminent', 'autistic', 'son', 'daylight', 'saving', 'ends', '6th', '2022', 'sunset', 'evening', '46pm', 'transparency', 'law', 'goes', 'effect', 'job', 'postings', 'funds', 'secure', 'parade', 'music', 'jewish', 'movement', 'portable', 'educational', 'outreach', 'center', 'safe', 'ok', 'hurry', 'adoptdontshop', 'tursday', 'tacotuesday', 'adventure', 'movie', 'administration', 'fall', 'share', 'great', 'news', 'added', 'gregg', 'allman', '75th', 'birthday', 'jam', 'dec', '8th', 'halloween', 'chabad', 'la', 'used', 'vehicle', 'won', 'anyone', 'denver', 'miami', 'wants', 'collab', 'dm', 'll', 'tour', 'weeks', 'fwiw', 'buildings', 'commission', 'part', 'extortion', 'racket', 'forcing', 'pay', 'expediter', 'plans', 'approved', 'languish', 'few', 'years', 'soho', 'mitzvah', 'tank', 'oct', '3653', '13', '14th', 'straight', 'road', 'moving', 'sweat', 'water', 'rain', 'drenched', 'hulu', 'stream', 'premieres', 'ahs', 'hold', 'russia', 'impersonating', 'pulled', 'hell', 'nice', 'mind', 'clarence', 'match', 'excited', 'corningcommunitycollege', 'bus', 'host', 'ec', 'told', 'ordering', 'resident', 'harlem', 'yeh', 'fillin', 'kate', 'online', 'being', 'after', 'gunpoint', 'seokhwa', 'interstate', 'imagine', '위아이', 'yohan', '김요한', '요한', '강석화', 'running', '석화', 'busking', '220930', 'dropped', 'different', 'air', 'systems', 'through', 'watch', 'safer', 'chain', 'robbed', 'gets', 'driver', 'wise', 'accident', 'bearable', 'had', 'honestly', 'trams', 'trains', 'pedestrians', 'designed', 'walk', 'seniors', 'juniors', 'amazing', 'invite', 'lanes', 'bike', 'blocking', 'photos', 'submit', 'residents', 'bill', 'myself', 'interesting', 'well', 'omg', 'cryin', 'm', 'origin', 'annoyed', 'adjusting', '2018', 'ride', 'telling', 'meant', 'mosque', 'distancing', 'enforcing', 'he', 'average', 'horizon', 'experience', 'dutch', 'unedited', 'raw', 'whatever', 'flat', 'villain', 'since', 'affordable', 'parts', 'keeping', 'det', 'self', 'deserve', 'russians', 'ethnic', 'annexes', 'lizzo', 'putin', 'am', 'opportunity', 'collegiate', 'level', 'diverse', 'track', 'play', 'been', 'spraying', 'apartment', 'increase', 'giant', 'renewal', 'lease', 'drones', 'paint', 'pres', 'need', 'solange', 'flute', 'yr', '200', 'madison', 'idc', 'country', 'punitive', '15', 'influx', 'cope', 'struggle', 'schools', 'hilarious', 'tap', 'purchase', 'migrant', 'chance', 'lottery', 'importance', 'entered', 'arrive', 'migrants', '500', 'kids', 'emergency', 'consequences', 'local', 'one', 'stoked', 'free', 'people', 'giving', 'seeing', 'flood', 'b', 'concerns', 'representatives', 'problems', 'caused', 'hazards', 'thousand', 'says', 'exactly', 'nov', 'harrypotter', '03', '09', 'stonksteam', 'charleston', '7th', '21st', 'levels', '20', 'lake', 'salt', '17th', 'jose', 'san', 'key', '3695', 'sanctuary', '00', 'give', 'overnight', '3', 'swing', '75', '3571', '3579', '50', '25', 'she', '7', 'prints', 'single', 'bottom', 'too', 'communists', 'much', 'kinfos', 'private', 'started', 'which', 'bargaining', 'update', 'rtp_fm', 'realtimepayments', 'across', 'below', 'click', 'details', 'live', '18th', 'later', 'hospitals', 'presbyterian', 'annual', '3rd', 'ramifications', 'graffiti', 'terrible', 'recent', 'her', '4th', '2nd', 'nurses', 'vmas', 'photo', 'heart', 'polaroids', 'sofia', 'mana', 'summit', 'vp', 'create', 'likes', 'campaign', 'ad', 'current', 'foreverlove', 'ميو_سوباسيت', 'mewsuppasit', '400k', 'enough', 'posts', 'boy', 'america', 'system', 'overwhelm', 'chaos', 'distracted', 'board', 'kyriacou', 'where', 'lee', 'speaker', 'esteem', 'announce', 'proud', 'st', 'happening', 'wrong', 'life', 'premiere', 'biggest', 'baby', 'rock', 'three', 'ny']\n"
     ]
    }
   ],
   "source": [
    "## Mostra o vocabulário\n",
    "\n",
    "vocab = list(w2v_model.wv.index_to_key)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('law', 0.3727055490016937), ('here', 0.2287950962781906), ('6th', 0.22740225493907928), ('pm', 0.22448128461837769), ('consequences', 0.22198377549648285), ('on', 0.21809211373329163), ('miami', 0.21735580265522003), ('for', 0.2144017070531845), ('next', 0.21344465017318726), ('prints', 0.21184755861759186)]\n"
     ]
    }
   ],
   "source": [
    "## Mostra as palavras mais similares com a palavra informada.\n",
    "## Caso a palavra não esteja no vocabulário, i.e., OOV, retorna uma mensagem de erro\n",
    "\n",
    "try:\n",
    "    sims = w2v_model.wv.most_similar(\"nyc\")\n",
    "    print(sims)\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[-0.14854656  0.0703124   0.09368501  0.10793454  0.12523901 -0.1302203\n",
      "  0.02956743  0.12924846 -0.0651916  -0.11741234 -0.0093156  -0.16809571\n",
      " -0.09970358  0.12405372  0.06246262  0.10615756  0.12018472  0.11764802\n",
      " -0.06197973 -0.04261533  0.04335519 -0.06316292  0.13968146 -0.17848755\n",
      "  0.10671734  0.04927135 -0.10355555  0.07288527 -0.04439036  0.12664498\n",
      "  0.18437018 -0.06675369 -0.00425248 -0.1032754   0.05041345  0.06170192\n",
      "  0.11883572  0.08689056  0.14688557  0.13550428  0.14124466 -0.13105017\n",
      " -0.16456398 -0.00482567 -0.03922172  0.11528422  0.08363017 -0.03480077\n",
      "  0.03729555  0.03953395  0.14087081 -0.18496619  0.00314404  0.05704498\n",
      " -0.03785138  0.15237801  0.16376501  0.10867805 -0.02409237  0.14483503\n",
      " -0.13697939  0.05721758 -0.07918046 -0.09169526  0.04065131  0.11126306\n",
      "  0.13019778 -0.08137874  0.09976059  0.12705539 -0.07173817 -0.14155449\n",
      "  0.10773605  0.1097723   0.00196524 -0.10507842 -0.11929688 -0.0446636\n",
      "  0.070089   -0.06452617 -0.17084923  0.05780748  0.06345602 -0.08619292\n",
      "  0.01295946 -0.03656619 -0.00467899 -0.14709473  0.06273913 -0.07179453\n",
      "  0.03969624 -0.02286234  0.02696322 -0.1291057  -0.02394891  0.05882761\n",
      "  0.09955513 -0.06133453 -0.14796458  0.07753509]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_model.wv.get_vector(\"nyc\", norm=True)   # Retorna o numpy array da palavra\n",
    "print(vector.shape)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Global Vectors (GloVe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.fasttext(dados['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "1. <span style=\"color:red\">SkipThought</span>\n",
    "2. InferSent\n",
    "3. **Universal Sentence Encoder (USE)**\n",
    "4. **SentenceBERT (SBERT)**\n",
    "5. Language-Agnostic SEntence Representations (LASER)\n",
    "6. Multilingual Universal Sentence Encoder (mUSE)\n",
    "7. **Language-agnostic BERT Sentence Embedding (LaBSE)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## SkipThought\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## InferSent\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# InferSent\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 9GB disponíveis em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "infersent_embeddings = SentenceEmbeddings().infersent(sentences)\n",
    "infersent_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## USE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# USE\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 1GB disponível em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "use_embeddings = SentenceEmbeddings().use(sentences)\n",
    "use_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SBERT\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SBERT\n",
    "\n",
    "- Utilizaremos o modelo **all-MiniLM-L6-v2**, que é 5x mais rápido que sua versão base (**all-mpnet-base-v2**) e significativamente menor (de 420MB para 80MB), mas ainda mantém um bom desempenho\n",
    "- O termo **all-** indica que o modelo foi treinado com todos os dados disponíveis (mais de 1 bilhão de pares de treinamento) e são projetados como modelos de propósito geral\n",
    "- Para mais detalhes, acesse a página do [SBERT](https://www.sbert.net/docs/pretrained_models.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SBERT\n",
    "sbert_embeddings = SentenceEmbeddings().sbert(sentences)\n",
    "sbert_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## LASER\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# LASER\n",
    "# Antes de utilizar o LASER, você deve fazer o download do modelo.\n",
    "# Para isso, descomente a linha abaixo.\n",
    "#!python -m laserembeddings download-models \"data\"\n",
    "# Você pode informar o código de idioma (ISO 639-1), para cada sentença da lista.\n",
    "# Por padrão, consideramos que todas as sentenças estão escritas em inglês (\"en\").\n",
    "laser_embeddings = SentenceEmbeddings().laser(sentences)\n",
    "laser_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## mUSE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# mUSE\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 300MB disponível em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "muse_embeddings = SentenceEmbeddings().muse(sentences)\n",
    "muse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LaBSE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# LaBSE\n",
    "labse_embeddings = SentenceEmbeddings().labse(sentences)\n",
    "labse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similaridade entre sentenças\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo baseado em:\n",
    "# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "sent_emb = SentenceEmbeddings().labse(sentences) #escolha o modelo de sentence embeddings de sua preferência\n",
    "plot_similarity(sentences, sent_emb, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelagem e Extração de Conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agrupamento de Textos\n",
    "- k-means\n",
    "- Agrupamento Hierárquico\n",
    "- Detecção de Comunidades em Grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k-means\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agrupamento Hierárquico\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detecção de Comunidades em Grafos\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compreensão Semântica e Emocional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Detecção de Intenção\n",
    "\n",
    "<img src=\"figs/ir-example.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Detecção de intenção\n",
    "- Uma intenção fornece uma interpretação geral do significado de uma expressão\n",
    "- Normalmente, é uma tarefa abstraída em um processo de classificação\n",
    "- Para isso, o primeiro passo é a obtenção de um conjunto de expressões rotuladas (*corpus*) para treinamento do modelo de classificação\n",
    "- Como veremos a seguir, existem muitos *corpus* disponíveis publicamente para essa tarefa.\n",
    "- Mas, caso deseje preparar o seu próprio *corpus* com dados de mídias sociais, você pode utilizar a modelagem de tópicos para determinar quais assuntos estão presentes no *corpus* e realizar a separação dos dados e, então, revisar e rotular manualmente cada um deles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus\n",
    "- Iremos considerar o conjunto de dados disponível em [Wang, Jinpeng, et al.]<sup>[1](#footnoteIntentCorpus)</sup>\n",
    "- Categorias de intenção nos *tweets*: **Food & Drink, Travel, Career & Education, Goods & Services, Event & Activities, Trifle, Non-intent**\n",
    "- Veja na Tabela a seguir mais detalhes sobre os dados:\n",
    "\n",
    "<center> Tabela: Intenções e exemplos do conjunto de dados [Wang, Jinpeng, et al.]<sup>1</sup> </center>\n",
    "\n",
    "| Categoria | # (%) | Exemplo |\n",
    "| --- | --- | --- |\n",
    "| Food & Drink | 245 (11,5%) | hungry...i need a salad......four more days to the BEYONCE CONCERT... |\n",
    "| Travel | 187 (8,78%) | I need a vacation really bad. I need a trip to Disneyland! |\n",
    "| Career & Education | 159 (7,46%) | this makes me want to be a lawyer RT @someuser new favorite line from an ... |\n",
    "| Goods & Services | 251 (11,78%) | mhmmm, i wannna a new phoneeee. ... i have to go to the hospital. ... |\n",
    "| Event & Activities | 312 (15.07%) | on my way to go swimming with the twoon @someuser; i love her so muchhhhh! |\n",
    "| Trifle | 436 (20,47%) | I'm so happy that I get to take a shower with myself. :D |\n",
    "| Non-intent | 531 (24,92%) | So sad that Ronaldo will be leaving these shores...http://URL |\n",
    "\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">1</a>: Wang, Jinpeng, et al. \"Mining user intents in twitter: A semi-supervised approach to inferring intent categories for tweets.\" Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelo de Classificação de Intenções\n",
    "<br>\n",
    "\n",
    "<center>Redes Neurais Recorrentes (RNN) X Sentence Embeddings (imagem de [Feng et al. 2020])</center>\n",
    "\n",
    "<img src=\"figs/bilstm-ir.png\" style=\"float: left; zoom:60%;\" />\n",
    "<img src=\"figs/labse.png\" style=\"float: right; zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#temporario\n",
    "intents = [\n",
    "    \"Smartphones\",\n",
    "    \"Smartphones\",\n",
    "    \"Smartphones\",\n",
    "\n",
    "    \"Weather\",\n",
    "    \"Weather\",\n",
    "    \"Weather\",\n",
    "\n",
    "    \n",
    "    \"Food and health\",\n",
    "    \"Food and health\",\n",
    "    \"Food and health\",\n",
    "    \n",
    "    \"Asking about age\",\n",
    "    \"Asking about age\"\n",
    "]\n",
    "\n",
    "intent_labse_model, X_test_labse, y_test_labse, classes = SemanticComprehension().training_intents(\"labse\", sentences, intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs = 20\n",
    "# Batch size = 32\n",
    "# Hidden layers = 300\n",
    "# Max sequence length = 280 --> tweet size\n",
    "intent_bilstm_model, X_test_rnn, y_test_rnn, classes = SemanticComprehension().training_intents(\"bilstm\", sentences, intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaBSE's performance\n",
    "y_hat = intent_labse_model.predict(X_test_labse)\n",
    "SemanticComprehension().plot_confusion_matrix(y_test_labse, y_hat, classes, \"figs/ir-labse-cm.png\")\n",
    "\n",
    "# RNN's performance\n",
    "y_hat = intent_bilstm_model.predict(X_test_rnn)        \n",
    "SemanticComprehension().plot_confusion_matrix(np.argmax(y_test_rnn,axis=1), np.argmax(y_hat, axis=1), classes, \"figs/ir-rnn-cm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix de Confusão\n",
    "<br>\n",
    "\n",
    "<center>Redes Neurais Recorrentes (RNN) X Sentence Embeddings</center>\n",
    "\n",
    "<img src=\"figs/ir-rnn-cm.png\" style=\"float: left; zoom:22%;\" />\n",
    "<img src=\"figs/ir-labse-cm.png\" style=\"float: right; zoom:22%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Predição de intents para os tweets coletados\n",
    "intents = CompreensaoSemantica().predicao_intencoes(intent_labse_model, sentences) ## configurado apenas para o modelo LaBSE\n",
    "intents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconhecimento de Entidades Nomeadas\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus\n",
    "- Iremos considerar o conjunto de dados disponível em  https://www.kaggle.com/code/amoghjrules/twitter-entity-recognition-using-bilstms\n",
    "- abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "for sentence in tqdm(sentences):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    entity = {}\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity[i] = {\n",
    "                        \"value\":ent.text,\n",
    "                        \"entity\":ent.label_,\n",
    "                        \"start\":ent.start_char,\n",
    "                        \"end\":ent.end_char\n",
    "                    }\n",
    "    entities.append(entity)\n",
    "\n",
    "print(entities[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Análise de Sentimentos (AS)\n",
    "\n",
    "* Também conhecida como **Mineração de Opiniões**\n",
    "* AS é o \"estudo computacional das opiniões (e.g. polaridade), atitudes e emoções de pessoas em relação a uma entidade\" [Medhat et al. 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tarefas de AS\n",
    "\n",
    "* **Detecção de sentimento**\n",
    "    - e.g. positivo, negativo ou neutro\n",
    "* **Identificação de emoções**\n",
    "    - e.g. sentimentos como raiva, antecipação, nojo, medo, alegria, tristeza, surpresa, confiança, etc. \n",
    "* **Detecção de toxicidade** \n",
    "    - e.g. categorias como insulto, profanidade, conteúdo sexualmente explícito, etc. [Jigsaw 2022]\n",
    "* **Análise multilíngue de sentimentos**\n",
    "* **Detecção de sarcasmo**\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplos de aplicações de AS\n",
    "\n",
    "* Identificação de comentários agressivos em notícias [Jigsaw 2022]\n",
    "* Extração da opinião pública sobre um candidato ou partido político [Pang et al. 2008]\n",
    "* Priorização de respostas a avaliações negativas de produtos [Bougie et al. 2003] \n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vantages da AS\n",
    "\n",
    "* Permite análises em larga escala\n",
    "* Reduz a subjetividade provocada por avaliadores humanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Níveis da AS\n",
    "\n",
    "1. **Nível de documento** - premissa: documento expressa opinião sobre apenas uma entidade; \n",
    "2. **Nível de frase** - premissa: frase expressa opinião sobre apenas uma entidade;  \n",
    "3. **Nível de aspecto** - múltiplas opiniões sobre múltiplos aspectos (ou alvos)\n",
    "    - e.g. “A <span style=\"color:#f00\">qualidade de voz</span> deste telefone <span style=\"color:#f00\">não é boa</span>, mas a <span style=\"color:#00f\">vida útil da bateria</span> é <span style=\"color:#00f\">longa</span>”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Abordagens para criação de modelos de AS\n",
    "\n",
    "1. **Usando Léxicos** \n",
    "    * <span style=\"color: #088B00\">VANTAGEM</span>: independência de domínio\n",
    "    * <span style=\"color: #f00\">DESVANTAGEM</span>: menor precisão e baixa escalabilidade\n",
    "2. **Aprendizado de máquina (ML)**\n",
    "    * <span style=\"color: #088B00\">VANTAGEM</span>: maior precisão\n",
    "    * <span style=\"color: #f00\">DESVANTAGEM</span>: maior dependência de domínio\n",
    "3. **Híbrido (léxico + ML)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### A seguir iremos apresentar\n",
    "\n",
    "* EmoLex\n",
    "* LIWC\n",
    "* Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frases que vamos usar como exemplo para os modelos de AS\n",
    "\n",
    "emotional_sentences = [\n",
    "\n",
    "    # exemplo de frase positiva\n",
    "    \"How good it is to live in Curitiba!\",\n",
    "\n",
    "    # exemplo de frase neutra\n",
    "    \"This car is grey.\",\n",
    "\n",
    "    # exemplo de frase negativa\n",
    "    \"Shut up, you're an idiot!\",\n",
    "\n",
    "    # exemplo de frase negativa, mas com palavras que podem confundir o modelo de AS como \"friend\"\n",
    "    \"It must be so sad to have you as a friend\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### EmoLex\n",
    "\n",
    "* Criado em 2013, é um dos maiores léxicos disponíveis em língua Inglesa\n",
    "* Baseado em unigramas e bigramas dos léxicos\n",
    "    * General Inquirer Lexicon\n",
    "    * WordNet Affect Lexicon\n",
    "* Anotações feitas via crowdsourcing pelo Mechanical Turk\n",
    "* Associa palavras com as oito emoções da teoria de Plutchik [Plutchik 1980]\n",
    "    * raiva, medo, antecipação, confiança, surpresa, tristeza, alegria e desgosto\n",
    "* Também inclui as catergorias de sentimento negativo e positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de sentenças processadas com o Emolex (frequência de emoções nas sentenças)\n",
    "EmotionComprehension.emolex(emotional_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LIWC\n",
    "\n",
    "* Ferramenta para identificar características linguísticas, psicológicas e sociais em textos [Pennebaker et al. 2001]\n",
    "* AS baseada em léxico (um dos maiores e mais completos na quantidade de termos e categorias cobertas)\n",
    "* Léxicos separados para línguas diferentes, sendo o inglês a língua padrão\n",
    "* Disponível apenas para Windows via interface gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Categorias de conteúdo disponíveis no LIWC\n",
    "\n",
    "* **emoções positivas** (e.g. amor, legal, doce, etc.)\n",
    "* **emoções negativas** (e.g. ferido, feio, desagradável, etc.)\n",
    "* **processos sociais** (e.g. filha, marido, vizinho, adulto, bebê, etc.)\n",
    "* **processos cognitivos** (e.g. pensar, conhecer, causa, etc.)\n",
    "* **processos perceptivos** (e.g. observar, escutar, sentir, etc.)\n",
    "* **processos biológicos** (e.g. comer, sangue, dor, etc.)\n",
    "* **relatividade** (e.g. chega, vai, embaixo, ontem, até, fim, etc.)\n",
    "* **preocupações sociais** (e.g. auditar, igreja, cozinhar, trabalhar, mestrado, etc.)\n",
    "* **consentimento** (e.g. concordar, ok, etc.)\n",
    "* **não-fluências e palavras de preenchimento** (e.g. hm, er, umm, certo, etc.)\n",
    "* entre outras [Tausczik and Pennebaker 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Categorias de função disponíveis no LIWC\n",
    "\n",
    "* **pronomes** \n",
    "* **preposições** \n",
    "* **artigos** \n",
    "* **conjunções** \n",
    "* **verbos auxiliares** \n",
    "* entre outras [Tausczik and Pennebaker 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Interface do LIWC\n",
    "\n",
    "<img src=\"figs/liwc.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Exemplo de processamento com o LIWC\n",
    "\n",
    "Para executar o processamento das sentenças de exemplo, será necessário [adquirir uma licença](https://www.liwc.app/buy), instalar o LIWC em um computador com Windows e processar o arquivo .csv gerado na célula a seguir. \n",
    "\n",
    "Você também poderá usar a versão de [teste online](https://www.liwc.app/demo), inserindo manualmente cada uma das sentenças geradas no .csv a seguir (essa versão é limitada a algumas poucas categorias, mas estão disponíveis as categorias de \"Negative tone\" e \"Positive tone\", bastante úteis em tarefas de AS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cria um .csv com exemplos para ser processado pelo LIWC\n",
    "pd.DataFrame({'text': emotional_sentences}).to_csv('data/emotional_sentences_LIWC.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carrega o arquivo com exemplos processado pelo LIWC\n",
    "df_LIWC = pd.read_csv('emotional_sentences_LIWC.csv')\n",
    "\n",
    "# apresenta apenas algumas colunas mais interessantes para AS\n",
    "df_LIWC[['text', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Perspective API\n",
    "\n",
    "* Usado para identificação de toxicidade em comentários online\n",
    "* Modelo multilíngue\n",
    "* Disponibilizado gratuitamente por meio de uma iniciativa da Jigsaw, uma empresa da Google [Jigsaw 2022]\n",
    "* Acessado via API pública\n",
    "* Adotada pelos principais veículos jornalísticos internacionais para moderar comentários em seus portais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Desafios para identificar toxicidade em comentários\n",
    "\n",
    "* Geralmente são textos curtos - ex.: “sei… 😏”, “¬¬”, “no way!”\n",
    "* Uso de emojis ambíguos - ex.: 😏,🌚,😋 \n",
    "* Erros de ortografia propositais (ou não) - ex.: “çocorro”;\n",
    "* Gírias da internet - ex.: “vc”, “pq”, “lol”, “iti malia”\n",
    "* Sutilezas inerentes à língua ou localidade - ex.: “oxi”, “p**ra!”;\n",
    "* Especificidades de contexto - ex.: “ex-presidiário”, “bozo”;\n",
    "* Uso de figuras de linguagem - ex.: metáfora, ironia, etc.;\n",
    "* Susceptíveis a ataques adversários - ex.: “st.Up1d”\n",
    "* Podem ocorrer de forma esparsa no conjunto de dados;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Definição de comentário tóxico\n",
    "\n",
    "<center>\"Um comentário rude, desrespeitoso ou irracional que provavelmente fará você sair de uma discussão.\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Funcionamento do Perspective API\n",
    "\n",
    "* Atribui uma pontuação contínua entre 0 e 1 para diferentes categorias de toxicidade de acordo com o % no texto\n",
    "* Uma pontuação mais alta para uma determinada categoria (ou atributo), indica uma maior probabilidade de um leitor perceber que o comentário possui este atributo\n",
    "* e.g. “Você é um idiota” pode receber uma pontuação de $0.8$ para o atributo TOXICIDADE, indicando que 8 entre 10 pessoas perceberiam esse comentário como tóxico\n",
    "* Portanto um comentário com pontuação de TOXICIDADE de $0.9$ não necessariamente é mais tóxico  que uma com $0.7$\n",
    "\n",
    " [Jigsaw 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"figs/perspective.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "Fonte: https://perspectiveapi.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Atributos de produção (multilíngue)\n",
    "\n",
    "- **TOXICITY** - “Um comentário rude, desrespeitoso ou irracional que provavelmente fará com que as pessoas deixem uma discussão”; \n",
    "- **SEVERE_TOXICITY** - “Um comentário que é muito odioso, agressivo, desrespeitoso, ou muito provável de fazer um usuário sair de uma discussão, ou desistir de compartilhar sua perspectiva. Este atributo é muito menos sensível a formas mais leves de toxicidade, como comentários que incluem usos positivos de palavrões”; \n",
    "- **IDENTITY_ATTACK** - “Comentários negativos ou de ódio direcionados a alguém por causa de sua identidade”; INSULT - “Comentário ofensivo, inflamatório ou negativo para uma pessoa ou grupo de pessoas”; \n",
    "- **PROFANITY** - “Xingamentos, palavrões ou outras linguagens obscenas, ou profanas”; \n",
    "- **THREAT** - “Descreve a intenção de infligir dor, lesão ou violência contra um indivíduo, ou grupo”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Acessando o Perspective API\n",
    "\n",
    "* Para executar os exemplos a seguir, você deverá solicitar acesso ao Perspective API no Google Cloud seguindo [esse passo a passo](https://developers.perspectiveapi.com/s/docs-get-started)\n",
    "* Obtenha a chave da API para usar nos próximos passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credenciais do Perspective API\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "PERSPECTIVE_API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemplo de sentenças processadas com o Perspective API\n",
    "EmotionComprehension.perspective(emotional_sentences, PERSPECTIVE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "0c9dfea1575e8b44ff0615653062e3f77db49c736187dedf2087539b29eac2fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
