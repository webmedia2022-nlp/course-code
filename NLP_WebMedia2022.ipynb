{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <center>Processamento de Linguagem Natural em Textos de Mídias Sociais: Fundamentos, Ferramentas e Aplicações</center>\n",
    "\n",
    "### <center>XXVIII Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia 2022)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Frances A. Santos (UNICAMP), Jordan Kobellarz (UTFPR), Fábio R. de Souza (USP), Leandro A. Villas (UNICAMP), Thiago H. Silva (UTFPR)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Curitiba, PR</center>\n",
    "<center>07 de Novembro de 2022</center>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/webmedia2022-nlp/course-code/blob/main/NLP_WebMedia2022.ipynb\" target=\"_parent\"><img style=\"float: right;\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir no Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.21.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.3.4)\n",
      "Requirement already satisfied: tweepy==4.10.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.10.1)\n",
      "Requirement already satisfied: praw==7.6.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (7.6.0)\n",
      "Requirement already satisfied: matplotlib==3.6.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: seaborn==0.12.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: protobuf==3.20.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: PyCrowdTangle==0.5.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: sentence-transformers==2.0.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: laserembeddings==1.1.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.1.2)\n",
      "Requirement already satisfied: iso639-lang==2.1.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (1.0.2)\n",
      "Requirement already satisfied: nltk==3.4.5 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (3.4.5)\n",
      "Requirement already satisfied: spacy==3.4.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (3.4.1)\n",
      "Requirement already satisfied: torch==1.12.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (1.12.1)\n",
      "Requirement already satisfied: tensorflow==2.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow_text==2.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow_hub==0.12.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (0.12.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (3.2.2)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: torchvision in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 9)) (0.13.1)\n",
      "Requirement already satisfied: scipy in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 9)) (0.10.1)\n",
      "Requirement already satisfied: sentencepiece in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 9)) (0.1.97)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 9)) (4.23.1)\n",
      "Requirement already satisfied: tqdm in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 9)) (4.64.1)\n",
      "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 10)) (0.3.8)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 10)) (0.0.35)\n",
      "Requirement already satisfied: transliterate==1.10.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 10)) (1.10.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 12)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: six in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from nltk==3.4.5->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (1.9.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (63.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (3.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (8.1.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 14)) (2.4.5)\n",
      "Requirement already satisfied: typing-extensions in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from torch==1.12.1->-r requirements.txt (line 15)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (2.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (0.3.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (1.50.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (2.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 16)) (0.37.1)\n",
      "Requirement already satisfied: click in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings==1.1.2->-r requirements.txt (line 10)) (8.1.3)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from pathy>=0.3.5->spacy==3.4.1->-r requirements.txt (line 14)) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: mock in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from subword-nmt<0.4.0,>=0.3.6->laserembeddings==1.1.2->-r requirements.txt (line 10)) (4.0.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (2.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (3.4.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->-r requirements.txt (line 14)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->-r requirements.txt (line 14)) (0.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 9)) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 9)) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: filelock in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 9)) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from jinja2->spacy==3.4.1->-r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (4.9)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (3.9.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 16)) (0.4.8)\n",
      "2022-10-18 17:01:05.589718: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-18 17:01:05.589752: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-18 17:01:05.589771: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NOTE-907): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (63.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm #Instalando dependências específicas do spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-10-18 18:08:17.740062: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-18 18:08:17.740080: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-18 18:08:17.740092: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NOTE-907): /proc/driver/nvidia/version does not exist\n",
      "/home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import getpass\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação do diretório \"data/\"\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from ExtracaoDados import ExtracaoDados\n",
    "from PreProcessamento import PreProcessamento\n",
    "from ModelosRepresentacao import ModelosEstatisticos, SentenceEmbeddings, WordEmbeddings\n",
    "from ExtracaoConhecimento import Agrupamento, CompreensaoSemantica, CompreensaoEmocional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Introdução \n",
    "2. Texto de mídias sociais: suas principais características e como coletá-los\n",
    "3. Pré-processamento textual\n",
    "4. Representação de textos utilizando vetores numéricos\n",
    "5. Modelagem e extração de conhecimento\n",
    "6. Compreensão semântica e emocional\n",
    "7. Possíveis aplicações\n",
    "8. Perguntas & Respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>1. Introdução</center>\n",
    "\n",
    "<img src=\"figs/social-media.jpeg\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Mídias sociais são acessadas por aproximadamente 4,7 bilhões de usuários em todo o planeta (i.e., 59% da população) [Kemp 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo (Twitter):\n",
    "\n",
    "* 200 Bilhões de postagens por ano\n",
    "* equivalente a 6 mil postagens por segundo\n",
    "\n",
    "Fonte: [Twitter Usage Statustics](https://www.internetlivestats.com/twitter-statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Possibilidades para acessar dados públicos em larga escala\n",
    "\n",
    "* **Twitter** - API (stream e histórico)\n",
    "* **Reddit** - API (stream e histórico)\n",
    "* **Meta (Instagram e Facebook)** - Plataforma CrowdTangle\n",
    "* **Swarm (Forsquare)** - API\n",
    "* **Flickr** - API\n",
    "* **Google Places** - API\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Aplicações na academia\n",
    "\n",
    "* análise de fenômenos sociais\n",
    "* sensoriamento social\n",
    "* detecção de notícias falsas\n",
    "* discurso de ódio\n",
    "* polarização política\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Aplicações na indústria\n",
    "\n",
    "* benchmarking (comparação com concorrentes)\n",
    "* forecasting (análise de tendências)\n",
    "* sistemas de recomendação\n",
    "* personalização / customização em larga escala\n",
    "* análise de risco\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>2. Textos de mídias sociais</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Onde encontrar dados textuais?\n",
    "\n",
    "* postagens\n",
    "* artigos\n",
    "* mensagens / comentários\n",
    "* metainformações de páginas, imagens, videos, perfis, postagens, mensagens, etc. \n",
    "* extração de texto em imagem, áudio e video\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Twitter\n",
    "\n",
    "* Mídia social de *Microblogging*\n",
    "* Mensagens limitadas a 280 caracteres\n",
    "* Uma das primeiras redes a disponibilizar uma API para extração de dados públicos em larga escala\n",
    "* Possibilidade de coleta de dados históricos ou em tempo real (*streaming*)\n",
    "* Qualquer dado público pode ser acessado, exceto os de perfis privados (menos de 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Característica proeminente\n",
    "\n",
    "Simplicidade nas interações e dicionário de dados:\n",
    "\n",
    "* tweets\n",
    "* hashtags #\n",
    "* menções @\n",
    "* retweets RT\n",
    "* respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "* **texto do tweet**\n",
    "* **timestamp**\n",
    "* **autor**\n",
    "    * nome\n",
    "    * localização\n",
    "    * se é verificado\n",
    "    * quantidade de seguidores, amigos, postagens\n",
    "    * data de criação da conta\n",
    "    * língua do perfil\n",
    "    * etc.\n",
    "* **geolocalização do tweet (GeoJson)**\n",
    "    * adicionada explícitamente\n",
    "    * ou capturada do dispositivo que gerou o tweet\n",
    "* **entidades**\n",
    "    * hashtags\n",
    "    * links\n",
    "    * menções\n",
    "    * mídias\n",
    "* **sinais sociais**\n",
    "    * quantidade de retweets\n",
    "    * quantidade de curtidas\n",
    "    * quantidade de respostas\n",
    "* etc. \n",
    "\n",
    "Conheça o [dicionário completo de dados de um tweet aqui](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de um tweet\n",
    "\n",
    "Alguns campos foram omitidos para facilitar a visualização. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"created_at\": \"Thu Apr 06 15:24:15 +0000 2017\",\n",
    "  \"id_str\": \"850006245121695744\",\n",
    "  \"text\": \"1\\/ Today we\\u2019re sharing our vision for the future of the Twitter API platform!\\nhttps:\\/\\/t.co\\/XweGngmxlP\",\n",
    "  \"user\": {\n",
    "    \"id\": 2244994945,\n",
    "    \"name\": \"Twitter Dev\",\n",
    "    \"screen_name\": \"TwitterDev\",\n",
    "    \"location\": \"Internet\",\n",
    "    \"url\": \"https:\\/\\/dev.twitter.com\\/\",\n",
    "    \"description\": \"Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\\/\\/twittercommunity.com\\/ \\u2328\\ufe0f #TapIntoTwitter\"\n",
    "  },\n",
    "  \"place\": {   \n",
    "  },\n",
    "  \"entities\": {\n",
    "    \"hashtags\": [      \n",
    "    ],\n",
    "    \"urls\": [\n",
    "    ],\n",
    "    \"user_mentions\": [     \n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Limite de 280 caracteres\n",
    "    * restringe capacidade argumentativa\n",
    "    * usuários contornam com uso de contrações de palavras, gírias da internet e emojis\n",
    "* Representatividade da população\n",
    "    * pode não representar bem o usuário médio de internet\n",
    "    * [tendem a ser usadas por pessoas mais jovens, com maior renda e grau de escolaridade](https://blogs.oii.ox.ac.uk/policy/did-you-consider-twitters-lack-of-representativeness-before-doing-that-predictive-study/)\n",
    "* Representatividade do retorno da API\n",
    "    * a API de streaming se baseia na [**relevância** e não **completude** dos dados](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview)\n",
    "* Alta incidência de contas robô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Tweets\n",
    "\n",
    "- Utilizaremos a Twitter API v2 para coletar os tweets \n",
    "- Acesse a [página de desenvolverdores](https://developer.twitter.com/) e obtenha suas credenciais de acesso\n",
    "- Limitamos cada coleta a 10 tweets, mas todo o conteúdo dos tweets é adicionado (*appending*) ao arquivo local data/tweets.json\n",
    "- Além dos campos *id* e *text* que estão presentes nos tweets por padrão, também solicitamos os campos *created_at, entities, geo, lang, public_metrics, source*. Para ver a lista completa de campos possíveis, acesse esta [página](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)\n",
    "- Filtramos para selecionar apenas os tweets escritos em Inglês ( *lang = \"en\"* ) e que contenham o termo \"nyc\", que referencia a cidade de Nova Iorque\n",
    "- Após coletar os tweets, extraímos os valores dos campos *text, timestamp_ms, ...* e retornamos no formato Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Get Twitter API credentials\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "twitter_consumer_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'API KEY SECRET'\")\n",
    "twitter_consumer_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN KEY'\")\n",
    "twitter_access_token_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN SECRET'\")\n",
    "twitter_access_token_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'Bearer TOKEN'\")\n",
    "twitter_bearer_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# collect tweets\n",
    "df_tweets = ExtracaoDados().twitter(\n",
    "    twitter_consumer_key, \n",
    "    twitter_consumer_secret, \n",
    "    twitter_access_token_key, \n",
    "    twitter_access_token_secret, \n",
    "    twitter_bearer_token\n",
    ")\n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Reddit\n",
    "\n",
    "* mídia social baseada em fóruns de discussão\n",
    "* comunidades/fóruns → \\subreddits\n",
    "* mais de 100K comunidades e 50 mi de usuários ativos diariamente em [2020](https://www.redditinc.com/advertising/audience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* sistema de moderação autoorganizável\n",
    "    * \\subreddits possuem regras próprias criadas pelos moderadores e membros\n",
    "    * algumas comunidades possuem alto nível de comprometimento com as regras propostas\n",
    "    * mecanismos de recompensa para colaboradores ativos\n",
    "* possibilidade de coletar dados em stream e histórico\n",
    "    * vantagem de permitir a recuperação do histórico completo\n",
    "* permite acesso à qualquer informação disponível publicamente\n",
    "    * inclui postagens, comentários, perfis, comunidades e suas respectivas metainformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "**[submission (postagem)](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html)**\n",
    "* id\n",
    "* url\n",
    "* permalink\n",
    "* created_utc\n",
    "* title\n",
    "* selftext (conteúdo da postagem)\n",
    "* score (número de upvotes)\n",
    "* [author](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (Redditor)\n",
    "    * name\n",
    "    * created_utc\n",
    "    * comment_karma (pontuação do usuário)\n",
    "    * has_verified_email\n",
    "    * etc.\n",
    "* [comments](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (árvore de comentários -- necessário percorrer com método específico para isso)\n",
    "    * author (Redditor)\n",
    "    * body\n",
    "    * distinguished\n",
    "    * etc.\n",
    "* distinguished (se a postagem foi destacada pelo moderador)\n",
    "* edited (se a postagem foi editada)\n",
    "* is_original_content (se foi marcada automaticamente como conteúdo original)\n",
    "* over_18 (se é conteúdo para maiores de 18 anos)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* a plataforma permite um alto grau de anonimidade\n",
    "    * é encorajado o uso de pseudônimo\n",
    "    * é possível fazer cadastro sem verificação\n",
    "    * abertura para comportamentos anti-éticos em comunidades não moderadas / permissivas\n",
    "* cada comunidade possui regras próprias\n",
    "    * práticas de moderação distintas\n",
    "    * dificultando a comparação \n",
    "* liberdade no formato\n",
    "    * campo aberto com possibiidade de uso de html e markdown\n",
    "* alta incidência de bots\n",
    "    * criam, fazem a curadoria e moderam conteúdos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts de \\subreddits\n",
    "\n",
    "- Utilizaremos a [API do Reddit](https://www.reddit.com/dev/api) para coletar posts\n",
    "- Você deverá criar uma conta para acessar a API em [reddit.com](https://reddit.com)\n",
    "- Depois de criar a conta, obtenha os Client ID e o Client Secret\n",
    "- No exemplo a seguir, coletamos os top 100 posts de 5 subreddits, contendo o texto, url, número de comentários, data de criação e score (número de upvotes do post)\n",
    "- Os dados são salvos no arquivo local data/reddit_posts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Get Reddit API credentials\n",
    "\n",
    "print(\"Informe seu 'CLIENT ID'\")\n",
    "REDDIT_CLIENT_ID = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'CLIENT SECRET'\")\n",
    "REDDIT_CLIENT_SECRET = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Subreddits with serious/relevant discussions\n",
    "subreddits = [\n",
    "    'politics',\n",
    "    'AskHistorians',\n",
    "    'changemyview',\n",
    "    'COVID19',\n",
    "    'EverythingScience',\n",
    "    'science'\n",
    "]\n",
    "\n",
    "# collect top 100 posts from each subreddit\n",
    "df_reddit_posts = ExtracaoDados().reddit(\n",
    "    REDDIT_CLIENT_ID, \n",
    "    REDDIT_CLIENT_SECRET,\n",
    "    subreddits=subreddits,\n",
    "    top_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# show some posts with text\n",
    "df_reddit_posts[df_reddit_posts['length'] > 0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Facebook (Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* alto grau de controle de privacidade\n",
    "* o anonimato é desencorajado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Formas de obter dados\n",
    "\n",
    "* via API nativa (limitada)\n",
    "* via web scraping (desencorajado)\n",
    "* via polls com usuários (não escalável)\n",
    "* via programa [Social Science One](https://socialscience.one/grant-process) (acesso direto à base do Facebook | difícil acesso | apenas para uso acadêmico)\n",
    "* via plataforma do [CrowdTangle](https://crowdtangle.com) (dados limitados a páginas e grupos famosos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### [CrowdTangle](https://crowdtangle.com)\n",
    "\n",
    "* iniciativa da Meta criada para jornalistas, agências de checagem de fatos, profissionais de marketing e pesquisadores\n",
    "* possibilidade de consultar e visualizar dados em tempo real pela interface (dashboards)\n",
    "* mesmos dados apresentados na interface podem ser obtidos via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* informações que **podem** ser coletadas:\n",
    "     * quando algo foi postado\n",
    "     * tipo do post (video, imagem, texto)\n",
    "     * página, conta ou grupo onde o conteúdo foi postado\n",
    "     * quantidade de interações (likes, reações, comentários, compartilhamentos, visualizações de videos)\n",
    "     * páginas públicas ou contas que compartilharam o conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* informações que **não podem** ser coletadas:\n",
    "    * alcance ou impressões de um post\n",
    "    * conteúdos efêmeros, como stories, por exemplo\n",
    "    * informações demográficas de usuários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* a base de dados disponível se limita a:\n",
    "    * contas famosas (aprox. ~7 mi de páginas, grupos ou perfis verificados em 08/06/2021), incluindo:\n",
    "        * páginas públicas com mais de 50K curtidas\n",
    "        * grupos públicos com mais de 95K membros\n",
    "        * grupos públicos dos Estados Unidos com mais de 2K membros\n",
    "        * todos os perfis verificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de retorno da API do CrowdTangle\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"status\": 200,\n",
    "    \"result\": {\n",
    "        \"posts\": [\n",
    "            {\n",
    "                \"platformId\": \"47657117525_10154014482272526\",\n",
    "                \"platform\": \"Facebook\",\n",
    "                \"date\": \"2016-02-12 23:38:14\",\n",
    "                \"updated\": \"2020-08-23 05:48:22\",\n",
    "                \"type\": \"live_video_complete\",\n",
    "                \"message\": \"Draymond at Foot Locker for #NBAAllStarTO with a special shoutout to #DubNation.\",\n",
    "                \"expandedLinks\": [\n",
    "                    {\n",
    "                        \"original\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                        \"expanded\": \"https://www.facebook.com/warriors/videos/10154014482272526/\"\n",
    "                    }\n",
    "                ],\n",
    "                \"link\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                \"postUrl\": \"https://www.facebook.com/warriors/posts/10154014482272526\",\n",
    "                \"subscriberCount\": 6041837,\n",
    "                \"score\": 4.750579867017164,\n",
    "                \"media\": [\n",
    "                    {\n",
    "                        \"type\": \"video\",\n",
    "                        \"url\": \"https://video-sea1-1.xx.fbcdn.net/v/t42.1790-29/12718926_1213464465334694_1083747983_n.mp4?_nc_cat=109&_nc_sid=985c63&efg=eyJybHIiOjQ0MiwicmxhIjoxNDIwLCJ2ZW5jb2RlX3RhZyI6InYyXzQwMF9jcmZfMjdfbWFpbl8zLjBfc2QifQ%3D%3D&_nc_ohc=e7Ygz2qv-24AX-wSWX2&rl=442&vabr=246&_nc_ht=video-sea1-1.xx&oh=889e0d776d92a84bb57099cad3d28d55&oe=5F43C879\",\n",
    "                        \"height\": 0,\n",
    "                        \"width\": 0\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"photo\",\n",
    "                        \"url\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\",\n",
    "                        \"height\": 400,\n",
    "                        \"width\": 400,\n",
    "                        \"full\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\"\n",
    "                    }\n",
    "                ],\n",
    "                \"statistics\": {\n",
    "                    \"actual\": {\n",
    "                        \"likeCount\": 24235,\n",
    "                        \"shareCount\": 753,\n",
    "                        \"commentCount\": 5675,\n",
    "                        \"loveCount\": 33,\n",
    "                        \"wowCount\": 18,\n",
    "                        \"hahaCount\": 3,\n",
    "                        \"sadCount\": 0,\n",
    "                        \"angryCount\": 5,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 0\n",
    "                    },\n",
    "                    \"expected\": {\n",
    "                        \"likeCount\": 3927,\n",
    "                        \"shareCount\": 279,\n",
    "                        \"commentCount\": 1041,\n",
    "                        \"loveCount\": 1046,\n",
    "                        \"wowCount\": 94,\n",
    "                        \"hahaCount\": 45,\n",
    "                        \"sadCount\": 14,\n",
    "                        \"angryCount\": 19,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 2\n",
    "                    }\n",
    "                },\n",
    "                \"account\": {\n",
    "                    \"id\": 19889,\n",
    "                    \"name\": \"Golden State Warriors\",\n",
    "                    \"handle\": \"warriors\",\n",
    "                    \"profileImage\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t1.0-1/p200x200/74788912_10158146665972526_3545220405897723904_n.jpg?_nc_cat=1&ccb=2&_nc_sid=dbb9e7&_nc_ohc=9snUpG_pdlQAX90IhWM&_nc_ht=scontent-sea1-1.xx&tp=6&oh=f8a3d3b62b507966ecc68de3b557fe84&oe=5FBF1185\",\n",
    "                    \"subscriberCount\": 11580228,\n",
    "                    \"url\": \"https://www.facebook.com/47657117525\",\n",
    "                    \"platform\": \"Facebook\",\n",
    "                    \"platformId\": \"47657117525\",\n",
    "                    \"accountType\": \"facebook_page\",\n",
    "                    \"pageAdminTopCountry\": \"US\",\n",
    "                    \"verified\": true\n",
    "                },\n",
    "                \"videoLengthMS\": 307968,\n",
    "                \"liveVideoStatus\": \"completed\",\n",
    "                \"Id\": \"19889|10154014482272526\",\n",
    "                \"legacyid\": 1686762829\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* dados limitados a contas famosas\n",
    "    * contas menos famosas são subrepresentadas\n",
    "* não é possível saber quem reagiu ou comentou em posts\n",
    "* ferramenta muito nova / pouco explorada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts no CrowdTangle\n",
    "\n",
    "* Utilizaremos a API do CrowdTangle para extrair posts do Facebook -- [documentação](https://github.com/CrowdTangle/API/wiki)\n",
    "* O primeiro passo é criar uma conta no CrowdTangle, depois criar um dashboard e obter o token da API para acessar os dados do dashboard\n",
    "* Para coletar posts via API, é necessário criar pelo menos uma lista em seu dashboard recém criado\n",
    "* Em nosso caso, criaremos uma lista para monitorar posts de páginas de mídias de notícias, incluindo CNN, NYT, BBC, NBC, NPR, Reuters, etc. \n",
    "* [Esse video explica como usar a interface do CrowdTangle para criar listas](https://vimeo.com/588999918). \n",
    "* [Esse video explica como acessar os dados via API](https://vimeo.com/453763307) explicando como executar todos os procedimentos acima\n",
    "\n",
    "> Observação: não é possível criar uma lista via API, somente pela interface do dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Get CrowdTangle API credentials\n",
    "\n",
    "print(\"Informe seu 'API_TOKEN'\")\n",
    "CROWDTANGLE_API_TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# collect top 100 posts in each month from start to end date\n",
    "df_facebook_posts = ExtracaoDados().facebook(\n",
    "    CROWDTANGLE_API_TOKEN, \n",
    "    search_term='covid-19',\n",
    "    start_date = '2020-04-01',\n",
    "    end_date = '2021-04-01'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# show some posts \n",
    "df_facebook_posts.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>3. Pré-processamento</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é um método para..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APAGAR\n",
    "\n",
    "import json\n",
    "\n",
    "example_file = 'data/tweets.json'\n",
    "tweets = open(example_file).read()\n",
    "tweets = json.loads(tweets)\n",
    "\n",
    "dados = [d['text'] for d in tweets]\n",
    "import pandas as pd\n",
    "dados = pd.DataFrame(dados, columns=['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio.rezende/anaconda3/envs/mltutorial/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-10-18 17:44:59.426913: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-18 17:44:59.426934: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-18 17:44:59.426949: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NOTE-907): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from PreProcessamento import PreProcessamento\n",
    "\n",
    "pipeline = PreProcessamento(fonte='twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>texto normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sofia and polaroids have my heart 🖤\\n\\n-&amp;gt;&amp;g...</td>\n",
       "      <td>sofia and polaroids have my heart 🖤\\n\\n-&amp;gt;&amp;g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MichelleKinney: The Russian Consulate in N...</td>\n",
       "      <td>rt @michellekinney: the russian consulate in n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  \\\n",
       "2  Sofia and polaroids have my heart 🖤\\n\\n-&gt;&g...   \n",
       "1  RT @MichelleKinney: The Russian Consulate in N...   \n",
       "\n",
       "                                   texto normalizado  \n",
       "2  sofia and polaroids have my heart 🖤\\n\\n-&gt;&g...  \n",
       "1  rt @michellekinney: the russian consulate in n...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados['texto normalizado'] = dados['texto'].apply(pipeline.normalizacao)\n",
    "\n",
    "dados.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex (Expressoes regulares)\n",
    "\n",
    "As expressão regulares são..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>texto normalizado</th>\n",
       "      <th>texto limpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>@Nivsick annoyed at myself for not adjusting t...</td>\n",
       "      <td>@nivsick annoyed at myself for not adjusting t...</td>\n",
       "      <td>USERNAME  annoyed at myself for not adjusting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @casperOne: I've watched this ten times.\\n\\...</td>\n",
       "      <td>rt @casperone: i've watched this ten times.\\n\\...</td>\n",
       "      <td>rt  USERNAME : i've watched this ten times.\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texto  \\\n",
       "32  @Nivsick annoyed at myself for not adjusting t...   \n",
       "8   RT @casperOne: I've watched this ten times.\\n\\...   \n",
       "\n",
       "                                    texto normalizado  \\\n",
       "32  @nivsick annoyed at myself for not adjusting t...   \n",
       "8   rt @casperone: i've watched this ten times.\\n\\...   \n",
       "\n",
       "                                          texto limpo  \n",
       "32   USERNAME  annoyed at myself for not adjusting...  \n",
       "8   rt  USERNAME : i've watched this ten times.\\n\\...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remover_links = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "aplicar_regex = lambda x: pipeline.limpeza_regex(x, remover_links, valor='LINK')\n",
    "dados['texto limpo'] = dados['texto normalizado'].apply(aplicar_regex)\n",
    "\n",
    "remover_mentions = r'@([A-Za-z0-9_]+)'\n",
    "aplicar_regex = lambda x: pipeline.limpeza_regex(x, remover_mentions, valor='USERNAME')\n",
    "dados['texto limpo'] = dados['texto limpo'].apply(aplicar_regex)\n",
    "\n",
    "dados.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>texto normalizado</th>\n",
       "      <th>texto limpo</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @casperOne: I've watched this ten times.\\n\\...</td>\n",
       "      <td>rt @casperone: i've watched this ten times.\\n\\...</td>\n",
       "      <td>rt  USERNAME : i've watched this ten times.\\n\\...</td>\n",
       "      <td>[rt, USERNAME, :, i, 've, watched, this, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NYC. Driver gets robbed for his chain and watc...</td>\n",
       "      <td>nyc. driver gets robbed for his chain and watc...</td>\n",
       "      <td>nyc. driver gets robbed for his chain and watc...</td>\n",
       "      <td>[nyc, ., driver, gets, robbed, for, his, chain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texto  \\\n",
       "16  RT @casperOne: I've watched this ten times.\\n\\...   \n",
       "9   NYC. Driver gets robbed for his chain and watc...   \n",
       "\n",
       "                                    texto normalizado  \\\n",
       "16  rt @casperone: i've watched this ten times.\\n\\...   \n",
       "9   nyc. driver gets robbed for his chain and watc...   \n",
       "\n",
       "                                          texto limpo  \\\n",
       "16  rt  USERNAME : i've watched this ten times.\\n\\...   \n",
       "9   nyc. driver gets robbed for his chain and watc...   \n",
       "\n",
       "                                               tokens  \n",
       "16  [rt, USERNAME, :, i, 've, watched, this, ten, ...  \n",
       "9   [nyc, ., driver, gets, robbed, for, his, chain...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados['tokens'] = dados['texto limpo'].apply(pipeline.tokenizacao)\n",
    "\n",
    "dados.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stemmização é um método para..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>texto normalizado</th>\n",
       "      <th>texto limpo</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @casperOne: I've watched this ten times.\\n\\...</td>\n",
       "      <td>rt @casperone: i've watched this ten times.\\n\\...</td>\n",
       "      <td>rt  USERNAME : i've watched this ten times.\\n\\...</td>\n",
       "      <td>[rt, USERNAME, :, i, 've, watched, this, ten, ...</td>\n",
       "      <td>[rt, usernam, :, i, 've, watch, thi, ten, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RT @DavidZipper: Well, this is interesting:\\n\\...</td>\n",
       "      <td>rt @davidzipper: well, this is interesting:\\n\\...</td>\n",
       "      <td>rt  USERNAME : well, this is interesting:\\n\\ni...</td>\n",
       "      <td>[rt, USERNAME, :, well, ,, this, is, interesti...</td>\n",
       "      <td>[rt, usernam, :, well, ,, thi, is, interest, :...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texto  \\\n",
       "25  RT @casperOne: I've watched this ten times.\\n\\...   \n",
       "30  RT @DavidZipper: Well, this is interesting:\\n\\...   \n",
       "\n",
       "                                    texto normalizado  \\\n",
       "25  rt @casperone: i've watched this ten times.\\n\\...   \n",
       "30  rt @davidzipper: well, this is interesting:\\n\\...   \n",
       "\n",
       "                                          texto limpo  \\\n",
       "25  rt  USERNAME : i've watched this ten times.\\n\\...   \n",
       "30  rt  USERNAME : well, this is interesting:\\n\\ni...   \n",
       "\n",
       "                                               tokens  \\\n",
       "25  [rt, USERNAME, :, i, 've, watched, this, ten, ...   \n",
       "30  [rt, USERNAME, :, well, ,, this, is, interesti...   \n",
       "\n",
       "                                                stems  \n",
       "25  [rt, usernam, :, i, 've, watch, thi, ten, time...  \n",
       "30  [rt, usernam, :, well, ,, thi, is, interest, :...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados['stems'] = dados['tokens'].apply(pipeline.stemmizacao)\n",
    "\n",
    "dados.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>texto normalizado</th>\n",
       "      <th>texto limpo</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @casperOne: I've watched this ten times.\\n\\...</td>\n",
       "      <td>rt @casperone: i've watched this ten times.\\n\\...</td>\n",
       "      <td>rt  USERNAME : i've watched this ten times.\\n\\...</td>\n",
       "      <td>[rt, USERNAME, :, i, 've, watched, this, ten, ...</td>\n",
       "      <td>[rt, usernam, :, i, 've, watch, thi, ten, time...</td>\n",
       "      <td>[rt, USERNAME, :, i, 've, watched, this, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @casperOne: I've watched this ten times.\\n\\...</td>\n",
       "      <td>rt @casperone: i've watched this ten times.\\n\\...</td>\n",
       "      <td>rt  USERNAME : i've watched this ten times.\\n\\...</td>\n",
       "      <td>[rt, USERNAME, :, i, 've, watched, this, ten, ...</td>\n",
       "      <td>[rt, usernam, :, i, 've, watch, thi, ten, time...</td>\n",
       "      <td>[rt, USERNAME, :, i, 've, watched, this, ten, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texto  \\\n",
       "3   RT @casperOne: I've watched this ten times.\\n\\...   \n",
       "29  RT @casperOne: I've watched this ten times.\\n\\...   \n",
       "\n",
       "                                    texto normalizado  \\\n",
       "3   rt @casperone: i've watched this ten times.\\n\\...   \n",
       "29  rt @casperone: i've watched this ten times.\\n\\...   \n",
       "\n",
       "                                          texto limpo  \\\n",
       "3   rt  USERNAME : i've watched this ten times.\\n\\...   \n",
       "29  rt  USERNAME : i've watched this ten times.\\n\\...   \n",
       "\n",
       "                                               tokens  \\\n",
       "3   [rt, USERNAME, :, i, 've, watched, this, ten, ...   \n",
       "29  [rt, USERNAME, :, i, 've, watched, this, ten, ...   \n",
       "\n",
       "                                                stems  \\\n",
       "3   [rt, usernam, :, i, 've, watch, thi, ten, time...   \n",
       "29  [rt, usernam, :, i, 've, watch, thi, ten, time...   \n",
       "\n",
       "                                               lemmas  \n",
       "3   [rt, USERNAME, :, i, 've, watched, this, ten, ...  \n",
       "29  [rt, USERNAME, :, i, 've, watched, this, ten, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados['lemmas'] = dados['tokens'].apply(pipeline.lemmatizacao)\n",
    "\n",
    "dados.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outra Alternativa: Pipeline de NLP do Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos tags</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RT @casperOne: I've watched this ten times.\\n\\...</td>\n",
       "      <td>[RT, @casperOne, :, I, 've, watched, this, ten...</td>\n",
       "      <td>[PROPN, PROPN, PUNCT, PRON, AUX, VERB, DET, NU...</td>\n",
       "      <td>[RT, @casperOne, :, I, 've, watch, this, ten, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RT @lunchshift: @WButler607 @TheRickWilson @CP...</td>\n",
       "      <td>[RT, @lunchshift, :, @WButler607, @TheRickWils...</td>\n",
       "      <td>[PROPN, PROPN, PUNCT, PROPN, PROPN, PROPN, PRO...</td>\n",
       "      <td>[RT, @lunchshift, :, @WButler607, @TheRickWils...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                texto  \\\n",
       "38  RT @casperOne: I've watched this ten times.\\n\\...   \n",
       "21  RT @lunchshift: @WButler607 @TheRickWilson @CP...   \n",
       "\n",
       "                                               tokens  \\\n",
       "38  [RT, @casperOne, :, I, 've, watched, this, ten...   \n",
       "21  [RT, @lunchshift, :, @WButler607, @TheRickWils...   \n",
       "\n",
       "                                             pos tags  \\\n",
       "38  [PROPN, PROPN, PUNCT, PRON, AUX, VERB, DET, NU...   \n",
       "21  [PROPN, PROPN, PUNCT, PROPN, PROPN, PROPN, PRO...   \n",
       "\n",
       "                                               lemmas  \n",
       "38  [RT, @casperOne, :, I, 've, watch, this, ten, ...  \n",
       "21  [RT, @lunchshift, :, @WButler607, @TheRickWils...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fazendo tudo de uma só vez com Spacy\n",
    "\n",
    "dados['tokens'], dados['pos tags'], dados['lemmas'] = zip(*dados['texto'].apply(pipeline.nlp_pipeline))\n",
    "\n",
    "dados.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Representação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos Estatísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40x322 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 650 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelos_stats = ModelosEstatisticos()\n",
    "modelos_stats.bow(dados['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40x59 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 387 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelos_stats.tfidf(dados['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO: PCA/SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = WordEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f27d41c0f40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.word2vec(dados['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x7f27d41c5b80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.fasttext(dados['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "1. <span style=\"color:red\">SkipThought</span>\n",
    "2. InferSent\n",
    "3. **Universal Sentence Encoder (USE)**\n",
    "4. **SentenceBERT (SBERT)**\n",
    "5. Language-Agnostic SEntence Representations (LASER)\n",
    "6. Multilingual Universal Sentence Encoder (mUSE)\n",
    "7. **Language-agnostic BERT Sentence Embedding (LaBSE)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## SkipThought\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## InferSent\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# InferSent\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 9GB disponíveis em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "infersent_embeddings = SentenceEmbeddings().infersent(sentences)\n",
    "infersent_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## USE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# USE\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 1GB disponível em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "use_embeddings = SentenceEmbeddings().use(sentences)\n",
    "use_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SBERT\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SBERT\n",
    "\n",
    "- Utilizaremos o modelo **all-MiniLM-L6-v2**, que é 5x mais rápido que sua versão base (**all-mpnet-base-v2**) e significativamente menor (de 420MB para 80MB), mas ainda mantém um bom desempenho\n",
    "- O termo **all-** indica que o modelo foi treinado com todos os dados disponíveis (mais de 1 bilhão de pares de treinamento) e são projetados como modelos de propósito geral\n",
    "- Para mais detalhes, acesse a página do [SBERT](https://www.sbert.net/docs/pretrained_models.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SBERT\n",
    "sbert_embeddings = SentenceEmbeddings().sbert(sentences)\n",
    "sbert_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## LASER\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# LASER\n",
    "# Antes de utilizar o LASER, você deve fazer o download do modelo.\n",
    "# Para isso, descomente a linha abaixo.\n",
    "#!python -m laserembeddings download-models \"data\"\n",
    "# Você pode informar o código de idioma (ISO 639-1), para cada sentença da lista.\n",
    "# Por padrão, consideramos que todas as sentenças estão escritas em inglês (\"en\").\n",
    "laser_embeddings = SentenceEmbeddings().laser(sentences)\n",
    "laser_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## mUSE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# mUSE\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 300MB disponível em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "muse_embeddings = SentenceEmbeddings().muse(sentences)\n",
    "muse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LaBSE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# LaBSE\n",
    "labse_embeddings = SentenceEmbeddings().labse(sentences)\n",
    "labse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similaridade entre sentenças\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo baseado em:\n",
    "# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "sent_emb = SentenceEmbeddings().labse(sentences) #escolha o modelo de sentence embeddings de sua preferência\n",
    "plot_similarity(sentences, sent_emb, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de Conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicações"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.8.0 ('mltutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "0c9dfea1575e8b44ff0615653062e3f77db49c736187dedf2087539b29eac2fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
