{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <center>Processamento de Linguagem Natural em Textos de Mídias Sociais: Fundamentos, Ferramentas e Aplicações</center>\n",
    "\n",
    "### <center>XXVIII Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia 2022)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Frances A. Santos (UNICAMP), Jordan Kobellarz (UTFPR), Fábio R. de Souza (USP), Leandro A. Villas (UNICAMP), Thiago H. Silva (UTFPR)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Curitiba, PR</center>\n",
    "<center>07 de Novembro de 2022</center>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/webmedia2022-nlp/course-code/blob/main/NLP_WebMedia2022.ipynb\" target=\"_parent\"><img style=\"float: right;\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir no Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.21.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.3.4)\n",
      "Requirement already satisfied: tweepy==4.10.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.10.1)\n",
      "Requirement already satisfied: praw==7.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (7.6.0)\n",
      "Requirement already satisfied: matplotlib==3.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: seaborn==0.12.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: protobuf==3.20.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: PyCrowdTangle==0.5.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: keras==2.3.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (2.3.1)\n",
      "Requirement already satisfied: sentence-transformers==2.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: laserembeddings==1.1.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: iso639-lang==2.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (1.0.2)\n",
      "Requirement already satisfied: nltk==3.4.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (3.4.5)\n",
      "Requirement already satisfied: spacy==3.4.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (3.4.1)\n",
      "Requirement already satisfied: torch==1.12.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (1.12.1)\n",
      "Requirement already satisfied: tensorflow==2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow_text==2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow_hub==0.12.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (0.12.0)\n",
      "Requirement already satisfied: gensim==4.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: bert==2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (2.2.0)\n",
      "Requirement already satisfied: bert-for-tf2==0.14.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.14.4)\n",
      "Collecting NRCLex==3.0.0\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-python-client==2.65.0\n",
      "  Downloading google_api_python_client-2.65.0-py2.py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from matplotlib==3.6.0->-r requirements.txt (line 5)) (4.37.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: h5py in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from keras==2.3.1->-r requirements.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: sentencepiece in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.1.97)\n",
      "Requirement already satisfied: tqdm in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.10.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (4.23.1)\n",
      "Requirement already satisfied: torchvision in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.13.1)\n",
      "Requirement already satisfied: transliterate==1.10.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 11)) (1.10.2)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 11)) (0.0.35)\n",
      "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from laserembeddings==1.1.2->-r requirements.txt (line 11)) (0.3.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 13)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from scikit-learn==1.0.2->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (8.1.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (1.9.2)\n",
      "Requirement already satisfied: setuptools in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (63.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (0.6.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.0.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy==3.4.1->-r requirements.txt (line 15)) (3.0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from torch==1.12.1->-r requirements.txt (line 16)) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.14.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.50.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (2.0.1)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (2.2.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (1.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (0.37.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorflow==2.2.0->-r requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from gensim==4.2.0->-r requirements.txt (line 20)) (5.2.1)\n",
      "Requirement already satisfied: erlastic in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from bert==2.2.0->-r requirements.txt (line 21)) (2.0.0)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from bert-for-tf2==0.14.4->-r requirements.txt (line 22)) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from bert-for-tf2==0.14.4->-r requirements.txt (line 22)) (0.10.2)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-api-python-client==2.65.0->-r requirements.txt (line 24)) (1.35.0)\n",
      "Requirement already satisfied: click in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from sacremoses==0.0.35->laserembeddings==1.1.2->-r requirements.txt (line 11)) (8.1.3)\n",
      "  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (4.2.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: mock in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from subword-nmt<0.4.0,>=0.3.6->laserembeddings==1.1.2->-r requirements.txt (line 11)) (4.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->-r requirements.txt (line 15)) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->-r requirements.txt (line 15)) (0.7.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 10)) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 10)) (0.13.1)\n",
      "Requirement already satisfied: filelock in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.0.0->-r requirements.txt (line 10)) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from jinja2->spacy==3.4.1->-r requirements.txt (line 15)) (2.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.65.0->-r requirements.txt (line 24)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirements.txt (line 17)) (3.9.0)\n",
      "Building wheels for collected packages: NRCLex\n",
      "  Building wheel for NRCLex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43310 sha256=64dd5dff345cf78329538d214e770a8d651de1b29074ba2c9d8f2dcf30d6a54e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-myv880p2/wheels/83/95/c0/42b43fb15eb48e4f5a67cba8915540cb2783591c59c037a9e5\n",
      "Successfully built NRCLex\n",
      "Installing collected packages: uritemplate, httplib2, googleapis-common-protos, textblob, NRCLex, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed NRCLex-3.0.0 google-api-core-2.8.2 google-api-python-client-2.65.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.56.4 httplib2-0.21.0 textblob-0.17.1 uritemplate-4.1.1\n",
      "2022-11-01 15:46:47.203291: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-01 15:46:47.203312: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-01 15:46:47.203339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (NOTE-665): /proc/driver/nvidia/version does not exist\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/frances.santos/anaconda3/envs/nlp-course/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.4.0\n",
      "    Uninstalling en-core-web-sm-3.4.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.4.0\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm #Instalando dependências específicas do spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import getpass\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação do diretório \"data/\"\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from DataExtraction import DataExtraction\n",
    "from Preprocessing import Preprocessing\n",
    "from TextRepresentation import statisticalModels, SentenceEmbeddings, WordEmbeddings\n",
    "from KnowledgeExtraction import Clustering, SemanticComprehension, SentimentAnalysis\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download de arquivos usados por bibliotecas\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Agenda\n",
    "\n",
    "<img src=\"figs/agenda.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>Introdução</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Por que mídias sociais?\n",
    "\n",
    "Mídias sociais são acessadas por aproximadamente 4,7 bilhões de usuários em todo o planeta (i.e., 59% da população) [Kemp 2022]\n",
    "\n",
    "<img src=\"figs/social-media.jpeg\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo (Twitter):\n",
    "\n",
    "* 200 Bilhões de postagens por ano\n",
    "* equivalente a 6 mil postagens por segundo\n",
    "\n",
    "Fonte: [Twitter Usage Statistics](https://www.internetlivestats.com/twitter-statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Possibilidades para acessar dados públicos em larga escala\n",
    "\n",
    "* **Twitter** - API (stream e histórico)\n",
    "* **Reddit** - API (stream e histórico)\n",
    "* **Meta (Instagram e Facebook)** - Plataforma CrowdTangle\n",
    "* **Swarm (Forsquare)** - API\n",
    "* **Flickr** - API\n",
    "* **Google Places** - API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Valiosa fonte de dados para diversas aplicações\n",
    "\n",
    "* **Na Academia**\n",
    "    * Análise de fenômenos sociais\n",
    "    * Sensoriamento social\n",
    "    * Detecção de notícias falsas\n",
    "    * Discurso de ódio\n",
    "    * Polarização política\n",
    "\n",
    "* **Na Indústria**\n",
    "    * Benchmarking (comparação com concorrentes)\n",
    "    * Forecasting (análise de tendências)\n",
    "    * Sistemas de recomendação\n",
    "    * Personalização / customização em larga escala\n",
    "    * Análise de risco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Por que textos?\n",
    "\n",
    "Textos escritos em linguagem natural, <br>estão presentes na maioria dos dados disponíveis de mídias sociais\n",
    "\n",
    "<img src=\"figs/social-media-content.png\" style=\"float: right; zoom:80%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>1. Textos de mídias sociais <br>Suas principais características e como coletá-los</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Onde encontrar dados textuais?\n",
    "\n",
    "* Postagens\n",
    "* Artigos\n",
    "* Mensagens / comentários\n",
    "* Metainformações de páginas, imagens, videos, perfis, postagens, mensagens, etc. \n",
    "* Extração de texto em imagem, áudio e video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mídias sociais consideradas\n",
    "\n",
    "\n",
    "| <img src=\"figs/twitter.png\" style=\"float: center; zoom:20%;\" /> | <img src=\"figs/reddit.png\" style=\"float: top center; zoom:50%;\" /> | <img src=\"figs/facebook.png\" style=\"float: center; zoom:40%;\" /> |\n",
    "| --- | --- | --- |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Twitter\n",
    "\n",
    "* Mídia social de *Microblogging*\n",
    "* Mensagens limitadas a 280 caracteres\n",
    "* Uma das primeiras redes a disponibilizar uma API para extração de dados públicos em larga escala\n",
    "* Possibilidade de coleta de dados históricos ou em tempo real (*streaming*)\n",
    "* Qualquer dado público pode ser acessado, exceto os de perfis privados (menos de 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Característica proeminente\n",
    "\n",
    "Simplicidade nas interações e dicionário de dados:\n",
    "\n",
    "* tweets\n",
    "* hashtags #\n",
    "* menções @\n",
    "* retweets RT\n",
    "* respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "* **texto do tweet**\n",
    "* **timestamp**\n",
    "* **autor**\n",
    "    * nome\n",
    "    * localização\n",
    "    * se é verificado\n",
    "    * quantidade de seguidores, amigos, postagens\n",
    "    * data de criação da conta\n",
    "    * língua do perfil\n",
    "    * etc.\n",
    "* **geolocalização do tweet (GeoJson)**\n",
    "    * adicionada explícitamente\n",
    "    * ou capturada do dispositivo que gerou o tweet\n",
    "* **entidades**\n",
    "    * hashtags\n",
    "    * links\n",
    "    * menções\n",
    "    * mídias\n",
    "* **sinais sociais**\n",
    "    * quantidade de retweets\n",
    "    * quantidade de curtidas\n",
    "    * quantidade de respostas\n",
    "* etc. \n",
    "\n",
    "Conheça o [dicionário completo de dados de um tweet aqui](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de um tweet\n",
    "\n",
    "Alguns campos foram omitidos para facilitar a visualização. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"created_at\": \"Thu Apr 06 15:24:15 +0000 2017\",\n",
    "  \"id_str\": \"850006245121695744\",\n",
    "  \"text\": \"1\\/ Today we\\u2019re sharing our vision for the future of the Twitter API platform!\\nhttps:\\/\\/t.co\\/XweGngmxlP\",\n",
    "  \"user\": {\n",
    "    \"id\": 2244994945,\n",
    "    \"name\": \"Twitter Dev\",\n",
    "    \"screen_name\": \"TwitterDev\",\n",
    "    \"location\": \"Internet\",\n",
    "    \"url\": \"https:\\/\\/dev.twitter.com\\/\",\n",
    "    \"description\": \"Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\\/\\/twittercommunity.com\\/ \\u2328\\ufe0f #TapIntoTwitter\"\n",
    "  },\n",
    "  \"place\": {   \n",
    "  },\n",
    "  \"entities\": {\n",
    "    \"hashtags\": [      \n",
    "    ],\n",
    "    \"urls\": [\n",
    "    ],\n",
    "    \"user_mentions\": [     \n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Limite de 280 caracteres\n",
    "    * restringe capacidade argumentativa\n",
    "    * usuários contornam com uso de contrações de palavras, gírias da internet e emojis\n",
    "* Representatividade da população\n",
    "    * pode não representar bem o usuário médio de Internet\n",
    "    * [tendem a ser usadas por pessoas mais jovens, com maior renda e grau de escolaridade](https://blogs.oii.ox.ac.uk/policy/did-you-consider-twitters-lack-of-representativeness-before-doing-that-predictive-study/)\n",
    "* Representatividade do retorno da API\n",
    "    * a API de streaming se baseia na [**relevância** e não **completude** dos dados](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview)\n",
    "* Alta incidência de contas robô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Tweets\n",
    "\n",
    "- Utilizaremos a Twitter API v2 para coletar os tweets \n",
    "- Acesse a [página de desenvolverdores](https://developer.twitter.com/) e obtenha suas credenciais de acesso\n",
    "- Limitamos cada coleta a **10 tweets**, mas todo o conteúdo dos tweets é adicionado (*appending*) ao arquivo local data/tweets.json\n",
    "- Além dos campos *id* e *text* que estão presentes nos tweets por padrão, também solicitamos os campos *created_at, entities, geo, lang, public_metrics, source*. Para ver a lista completa de campos possíveis, acesse esta [página](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)\n",
    "- Filtramos para selecionar apenas os tweets escritos em Inglês ( *lang = \"en\"* ) e que contenham o termo \"nyc\", que referencia a cidade de Nova Iorque\n",
    "- Após coletar os tweets, extraímos os valores dos campos *text, timestamp_ms, ...* e retornamos no formato Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe seu 'API KEY'\n",
      "········\n",
      "Informe seu 'API KEY SECRET'\n",
      "········\n",
      "Informe seu 'ACCESS TOKEN KEY'\n",
      "········\n",
      "Informe seu 'ACCESS TOKEN SECRET'\n",
      "········\n",
      "Informe seu 'Bearer TOKEN'\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "# Credenciais da API do Twitter\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "twitter_consumer_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'API KEY SECRET'\")\n",
    "twitter_consumer_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN KEY'\")\n",
    "twitter_access_token_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN SECRET'\")\n",
    "twitter_access_token_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'Bearer TOKEN'\")\n",
    "twitter_bearer_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-01T19:38:01.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294130...</td>\n",
       "      <td>0</td>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>83</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-01T19:38:00.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294087...</td>\n",
       "      <td>0</td>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>241</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-01T19:38:03.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294203...</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>124</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-01T19:38:03.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294217...</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>121</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-01T19:38:03.000Z</td>\n",
       "      <td>https://twitter.com/anyuser/status/15875294210...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>132</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0  2022-11-01T19:38:01.000Z   \n",
       "1  2022-11-01T19:38:00.000Z   \n",
       "2  2022-11-01T19:38:03.000Z   \n",
       "3  2022-11-01T19:38:03.000Z   \n",
       "4  2022-11-01T19:38:03.000Z   \n",
       "\n",
       "                                                 url  score  \\\n",
       "0  https://twitter.com/anyuser/status/15875294130...      0   \n",
       "1  https://twitter.com/anyuser/status/15875294087...      0   \n",
       "2  https://twitter.com/anyuser/status/15875294203...      0   \n",
       "3  https://twitter.com/anyuser/status/15875294217...      0   \n",
       "4  https://twitter.com/anyuser/status/15875294210...      0   \n",
       "\n",
       "                                                text  length geo  \n",
       "0  NY. Every corner this past Sunday … https://t....      83  {}  \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...     241  {}  \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...     124  {}  \n",
       "3  RT @YB_2: The NYC transparency law goes into e...     121  {}  \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...     132  {}  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Coleta de tweets\n",
    "try:\n",
    "    df_tweets = DataExtraction().twitter(\n",
    "        twitter_consumer_key, \n",
    "        twitter_consumer_secret, \n",
    "        twitter_access_token_key, \n",
    "        twitter_access_token_secret, \n",
    "        twitter_bearer_token\n",
    "    )\n",
    "except:\n",
    "    ## Caso aconteça algum erro durante a coleta, é carregado o arquivo existente\n",
    "    with open(\"data/tweets.json\", \"r\") as tweet_file:\n",
    "        tweets = json.load(tweet_file)\n",
    "        data = [{\n",
    "                \"created_at\": item[\"created_at\"],\n",
    "                \"url\": \"https://twitter.com/anyuser/status/\" + item[\"id\"],\n",
    "                \"score\": item[\"public_metrics\"][\"like_count\"],\n",
    "                \"text\": item[\"text\"],\n",
    "                \"length\":len(item[\"text\"]),\n",
    "                \"geo\": item[\"geo\"]\n",
    "                } for item in tweets]\n",
    "        df_tweets = pd.DataFrame(data)\n",
    "    \n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Reddit\n",
    "\n",
    "* Mídia social baseada em fóruns de discussão\n",
    "* Comunidades/fóruns → \\subreddits\n",
    "* Mais de 100K comunidades e 50 mi de usuários ativos diariamente em [2020](https://www.redditinc.com/advertising/audience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* Sistema de moderação autoorganizável\n",
    "    * \\subreddits possuem regras próprias criadas pelos moderadores e membros\n",
    "    * algumas comunidades possuem alto nível de comprometimento com as regras propostas\n",
    "    * mecanismos de recompensa para colaboradores ativos\n",
    "* Possibilidade de coletar dados em stream e histórico\n",
    "    * vantagem de permitir a recuperação do histórico completo\n",
    "* Permite acesso à qualquer informação disponível publicamente\n",
    "    * inclui postagens, comentários, perfis, comunidades e suas respectivas metainformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "**[submission (postagem)](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html)**\n",
    "* id\n",
    "* url\n",
    "* permalink\n",
    "* created_utc\n",
    "* title\n",
    "* selftext (conteúdo da postagem)\n",
    "* score (número de upvotes)\n",
    "* [author](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (Redditor)\n",
    "    * name\n",
    "    * created_utc\n",
    "    * comment_karma (pontuação do usuário)\n",
    "    * has_verified_email\n",
    "    * etc.\n",
    "* [comments](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (árvore de comentários -- necessário percorrer com método específico para isso)\n",
    "    * author (Redditor)\n",
    "    * body\n",
    "    * distinguished\n",
    "    * etc.\n",
    "* distinguished (se a postagem foi destacada pelo moderador)\n",
    "* edited (se a postagem foi editada)\n",
    "* is_original_content (se foi marcada automaticamente como conteúdo original)\n",
    "* over_18 (se é conteúdo para maiores de 18 anos)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* a plataforma permite um alto grau de anonimidade\n",
    "    * é encorajado o uso de pseudônimo\n",
    "    * é possível fazer cadastro sem verificação\n",
    "    * abertura para comportamentos anti-éticos em comunidades não moderadas / permissivas\n",
    "* cada comunidade possui regras próprias\n",
    "    * práticas de moderação distintas\n",
    "    * dificultando a comparação \n",
    "* liberdade no formato\n",
    "    * campo aberto com possibiidade de uso de html e markdown\n",
    "* alta incidência de bots\n",
    "    * criam, fazem a curadoria e moderam conteúdos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts de \\subreddits\n",
    "\n",
    "- Utilizaremos a [API do Reddit](https://www.reddit.com/dev/api) para coletar posts\n",
    "- Você deverá criar uma conta para acessar a API em [reddit.com](https://reddit.com)\n",
    "- Depois de criar a conta, obtenha os Client ID e o Client Secret\n",
    "- No exemplo a seguir, coletamos os top 100 posts de 5 subreddits, contendo o texto, url, número de comentários, data de criação e score (número de upvotes do post)\n",
    "- Os dados são salvos no arquivo local data/reddit_posts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do Reddit\n",
    "\n",
    "print(\"Informe seu 'CLIENT ID'\")\n",
    "REDDIT_CLIENT_ID = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'CLIENT SECRET'\")\n",
    "REDDIT_CLIENT_SECRET = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Subreddits com discussões sérias sobre assuntos como política, história e ciência.\n",
    "subreddits = [\n",
    "    'politics',\n",
    "    'AskHistorians',\n",
    "    'changemyview',\n",
    "    'COVID19',\n",
    "    'EverythingScience',\n",
    "    'science'\n",
    "]\n",
    "\n",
    "# Coleta os top 100 posts de cada Subreddit\n",
    "df_reddit_posts = DataExtraction().reddit(\n",
    "    REDDIT_CLIENT_ID,\n",
    "    REDDIT_CLIENT_SECRET,\n",
    "    subreddits=subreddits,\n",
    "    top_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Apresenta alguns posts com texto\n",
    "df_reddit_posts[df_reddit_posts['length'] > 0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Facebook (Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* Alto grau de controle de privacidade\n",
    "* O anonimato é desencorajado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Formas de obter dados\n",
    "\n",
    "* Via API nativa (limitada)\n",
    "* Via web scraping (desencorajado)\n",
    "* Via polls com usuários (não escalável)\n",
    "* Via programa [Social Science One](https://socialscience.one/grant-process) (acesso direto à base do Facebook | difícil acesso | apenas para uso acadêmico)\n",
    "* Via plataforma do [CrowdTangle](https://crowdtangle.com) (dados limitados a páginas e grupos famosos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### [CrowdTangle](https://crowdtangle.com)\n",
    "\n",
    "* Iniciativa da Meta criada para jornalistas, agências de checagem de fatos, profissionais de marketing e pesquisadores\n",
    "* Possibilidade de consultar e visualizar dados em tempo real pela interface (dashboards)\n",
    "* Mesmos dados apresentados na interface podem ser obtidos via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Informações que **podem** ser coletadas:\n",
    "     * quando algo foi postado\n",
    "     * tipo do post (video, imagem, texto)\n",
    "     * página, conta ou grupo onde o conteúdo foi postado\n",
    "     * quantidade de interações (likes, reações, comentários, compartilhamentos, visualizações de videos)\n",
    "     * páginas públicas ou contas que compartilharam o conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Informações que **não podem** ser coletadas:\n",
    "    * alcance ou impressões de um post\n",
    "    * conteúdos efêmeros, como stories, por exemplo\n",
    "    * informações demográficas de usuários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* A base de dados disponível se limita a:\n",
    "    * contas famosas (aprox. ~7 mi de páginas, grupos ou perfis verificados em 08/06/2021), incluindo:\n",
    "        * páginas públicas com mais de 50K curtidas\n",
    "        * grupos públicos com mais de 95K membros\n",
    "        * grupos públicos dos Estados Unidos com mais de 2K membros\n",
    "        * todos os perfis verificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de retorno da API do CrowdTangle\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"status\": 200,\n",
    "    \"result\": {\n",
    "        \"posts\": [\n",
    "            {\n",
    "                \"platformId\": \"47657117525_10154014482272526\",\n",
    "                \"platform\": \"Facebook\",\n",
    "                \"date\": \"2016-02-12 23:38:14\",\n",
    "                \"updated\": \"2020-08-23 05:48:22\",\n",
    "                \"type\": \"live_video_complete\",\n",
    "                \"message\": \"Draymond at Foot Locker for #NBAAllStarTO with a special shoutout to #DubNation.\",\n",
    "                \"expandedLinks\": [\n",
    "                    {\n",
    "                        \"original\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                        \"expanded\": \"https://www.facebook.com/warriors/videos/10154014482272526/\"\n",
    "                    }\n",
    "                ],\n",
    "                \"link\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                \"postUrl\": \"https://www.facebook.com/warriors/posts/10154014482272526\",\n",
    "                \"subscriberCount\": 6041837,\n",
    "                \"score\": 4.750579867017164,\n",
    "                \"media\": [\n",
    "                    {\n",
    "                        \"type\": \"video\",\n",
    "                        \"url\": \"https://video-sea1-1.xx.fbcdn.net/v/t42.1790-29/12718926_1213464465334694_1083747983_n.mp4?_nc_cat=109&_nc_sid=985c63&efg=eyJybHIiOjQ0MiwicmxhIjoxNDIwLCJ2ZW5jb2RlX3RhZyI6InYyXzQwMF9jcmZfMjdfbWFpbl8zLjBfc2QifQ%3D%3D&_nc_ohc=e7Ygz2qv-24AX-wSWX2&rl=442&vabr=246&_nc_ht=video-sea1-1.xx&oh=889e0d776d92a84bb57099cad3d28d55&oe=5F43C879\",\n",
    "                        \"height\": 0,\n",
    "                        \"width\": 0\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"photo\",\n",
    "                        \"url\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\",\n",
    "                        \"height\": 400,\n",
    "                        \"width\": 400,\n",
    "                        \"full\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\"\n",
    "                    }\n",
    "                ],\n",
    "                \"statistics\": {\n",
    "                    \"actual\": {\n",
    "                        \"likeCount\": 24235,\n",
    "                        \"shareCount\": 753,\n",
    "                        \"commentCount\": 5675,\n",
    "                        \"loveCount\": 33,\n",
    "                        \"wowCount\": 18,\n",
    "                        \"hahaCount\": 3,\n",
    "                        \"sadCount\": 0,\n",
    "                        \"angryCount\": 5,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 0\n",
    "                    },\n",
    "                    \"expected\": {\n",
    "                        \"likeCount\": 3927,\n",
    "                        \"shareCount\": 279,\n",
    "                        \"commentCount\": 1041,\n",
    "                        \"loveCount\": 1046,\n",
    "                        \"wowCount\": 94,\n",
    "                        \"hahaCount\": 45,\n",
    "                        \"sadCount\": 14,\n",
    "                        \"angryCount\": 19,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 2\n",
    "                    }\n",
    "                },\n",
    "                \"account\": {\n",
    "                    \"id\": 19889,\n",
    "                    \"name\": \"Golden State Warriors\",\n",
    "                    \"handle\": \"warriors\",\n",
    "                    \"profileImage\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t1.0-1/p200x200/74788912_10158146665972526_3545220405897723904_n.jpg?_nc_cat=1&ccb=2&_nc_sid=dbb9e7&_nc_ohc=9snUpG_pdlQAX90IhWM&_nc_ht=scontent-sea1-1.xx&tp=6&oh=f8a3d3b62b507966ecc68de3b557fe84&oe=5FBF1185\",\n",
    "                    \"subscriberCount\": 11580228,\n",
    "                    \"url\": \"https://www.facebook.com/47657117525\",\n",
    "                    \"platform\": \"Facebook\",\n",
    "                    \"platformId\": \"47657117525\",\n",
    "                    \"accountType\": \"facebook_page\",\n",
    "                    \"pageAdminTopCountry\": \"US\",\n",
    "                    \"verified\": true\n",
    "                },\n",
    "                \"videoLengthMS\": 307968,\n",
    "                \"liveVideoStatus\": \"completed\",\n",
    "                \"Id\": \"19889|10154014482272526\",\n",
    "                \"legacyid\": 1686762829\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Dados limitados a contas famosas\n",
    "    * contas menos famosas são subrepresentadas\n",
    "* Não é possível saber quem reagiu ou comentou em posts\n",
    "* Ferramenta muito nova / pouco explorada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts no CrowdTangle\n",
    "\n",
    "* Utilizaremos a API do CrowdTangle para extrair posts do Facebook -- [documentação](https://github.com/CrowdTangle/API/wiki)\n",
    "* O primeiro passo é criar uma conta no CrowdTangle, depois criar um dashboard e obter o token da API para acessar os dados do dashboard\n",
    "* Para coletar posts via API, é necessário criar pelo menos uma lista em seu dashboard recém criado\n",
    "* Em nosso caso, criaremos uma lista para monitorar posts de páginas de mídias de notícias, incluindo CNN, NYT, BBC, NBC, NPR, Reuters, etc. \n",
    "* [Esse video explica como usar a interface do CrowdTangle para criar listas](https://vimeo.com/588999918). \n",
    "* [Esse video explica como acessar os dados via API](https://vimeo.com/453763307) explicando como executar todos os procedimentos acima\n",
    "\n",
    "> Observação: não é possível criar uma lista via API, somente pela interface do dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do CrowdTangle\n",
    "\n",
    "print(\"Informe seu 'API_TOKEN'\")\n",
    "CROWDTANGLE_API_TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui coletamos os top 100 posts em cada mês, iniciando em start_date e terminando em end_date\n",
    "df_facebook_posts = DataExtraction().facebook(\n",
    "    CROWDTANGLE_API_TOKEN, \n",
    "    search_term='covid-19',\n",
    "    start_date = '2020-04-01',\n",
    "    end_date = '2021-04-01'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Amostra de posts do Facebook\n",
    "df_facebook_posts.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>2. Pré-processamento Textual</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, passamos pela série de tarefas necessárias para posterior aplicação de modelos de Aprendizado de Máquina.\n",
    "\n",
    "Mais especificamente, passaremos pelas seguintes etapas:\n",
    "\n",
    "- Normalização\n",
    "- Tokenização\n",
    "- Lemmatização\n",
    "- Stemmização\n",
    "- POS Tagging \n",
    "- Padrões Regex para Limpeza de Texto \n",
    "\n",
    "Para isso, apresentaremos duas versões: a biblioteca [NLTK](https://www.nltk.org/howto/portuguese_en.html), e o Spacy(https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização**\n",
    "\n",
    "Em primeiro lugar, realizamos a normalização de texto, para que consigamos computar a quantidade de palavras independente de terem sido escritas em letras maiúsculas ou minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RT @MichelleKinney: The Russian Consulate in N...</td>\n",
       "      <td>rt @michellekinney: the russian consulate in n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "51  RT @MichelleKinney: The Russian Consulate in N...   \n",
       "4   Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                      normalized_text  \n",
       "51  rt @michellekinney: the russian consulate in n...  \n",
       "4   tune in to the @districtingnyc public meeting ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Preprocessing import Preprocessing\n",
    "\n",
    "pipeline = Preprocessing()\n",
    "\n",
    "data = df_tweets[['text']].copy()\n",
    "data[\"normalized_text\"] = data['text'].apply(pipeline.normalization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex (Expressoes regulares)\n",
    "\n",
    "As Expressões Regulares (do inglês, Regular Expressions, cujos acrônimos são RE ou RegEx), são comandos que especificam padrões de busca em texto.\n",
    "\n",
    "São muito versáteis para limpeza de texto, uma vez que é permitido especificar através delas exatamente os padrões que são importantes para o nosso contexto.\n",
    "\n",
    "Alguns sites são muito úteis como referência para montar e testar padrões Regex, como o [Regex101](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , saving,  ...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , save,   ,...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , saving,  ...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , save,   ,...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_links = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_links, value='LINK')\n",
    "data['clean_text'] = data['normalized_text'].apply(apply_regex)\n",
    "\n",
    "re_mentions = r'@([A-Za-z0-9_]+)'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_mentions, value='USERNAME')\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "re_newline = '\\\\n'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_newline)\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "re_special_char = '\\W+'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_special_char)\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização\n",
    "\n",
    "A etapa de tokenização consiste em transformar um texto em uma lista de palavras e símbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, LINK]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[USERNAME, LINK, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, USERNAME, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , save,   ,...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, USERNAME, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, USERNAME, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, LINK]   \n",
       "1  [USERNAME, LINK, please, donate, we, greatly, ...   \n",
       "2  [rt, USERNAME, daylight, saving, time, ends, t...   \n",
       "3  [rt, USERNAME, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, USERNAME, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , save,   ,...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['clean_text'].apply(pipeline.tokenization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging**\n",
    "\n",
    "A extração de POS Tags (Part of Speech, ou Partes-da-Fala), consiste em identificar o papel de cada termo dentro da estrutura sintática de uma frase.\n",
    "\n",
    "Alguns exemplos de POS Tag são: \n",
    "\n",
    "- ADJ: Adjetivo\n",
    "- ADP: Preposição (do inglês Adposition)\n",
    "- ADV: Advérbio\n",
    "- NOUN: Substantivo\n",
    "- VERB: Verbo\n",
    "- PROPN: Nomes próprios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_tags'] = data['tokens'].apply(pipeline.pos_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemmização**\n",
    "\n",
    "A stemmização consiste em extrair o afixo de uma palavra. Isso permite que se reduza a variabilidade do corpus, ao agruparmos todas as referências a uma palavra única independente das variações de gênero, número e grau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, LINK]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny,   , every,   , corner,   , this,   , past...</td>\n",
       "      <td>[PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[USERNAME, LINK, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[   , username,   , link,   , please,   , dona...</td>\n",
       "      <td>[SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, USERNAME, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt,   , username,   , daylight,   , save,   ,...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, USERNAME, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt,   , username,   , the,   , nyc,   , trans...</td>\n",
       "      <td>[PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, USERNAME, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune,   , in,   , to,   , the,   , username, ...</td>\n",
       "      <td>[NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, LINK]   \n",
       "1  [USERNAME, LINK, please, donate, we, greatly, ...   \n",
       "2  [rt, USERNAME, daylight, saving, time, ends, t...   \n",
       "3  [rt, USERNAME, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, USERNAME, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [ny,   , every,   , corner,   , this,   , past...   \n",
       "1  [   , username,   , link,   , please,   , dona...   \n",
       "2  [rt,   , username,   , daylight,   , save,   ,...   \n",
       "3  [rt,   , username,   , the,   , nyc,   , trans...   \n",
       "4  [tune,   , in,   , to,   , the,   , username, ...   \n",
       "\n",
       "                                            pos tags  \n",
       "0  [PROPN, SPACE, DET, SPACE, NOUN, SPACE, PRON, ...  \n",
       "1  [SPACE, PROPN, SPACE, PROPN, SPACE, INTJ, SPAC...  \n",
       "2  [PROPN, SPACE, PROPN, SPACE, NOUN, SPACE, VERB...  \n",
       "3  [PROPN, SPACE, PROPN, SPACE, DET, SPACE, PROPN...  \n",
       "4  [NOUN, SPACE, ADP, SPACE, ADP, SPACE, DET, SPA...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stems'] = data['tokens'].apply(pipeline.stemming)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatização**\n",
    "\n",
    "A lemmatização consiste em uma tarefa cujo objetivo é o mesmo da Stemmização, com a diferença de que, ao invés de reduzir as palavras ao seu radical (se tornando, às vezes, ilegível), as palavras neste caso são reduzidas a sua inflexão mínima (*estamos, estaremos e estava*, para *está*, por exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmas'] = data['tokens'].apply(pipeline.lemmatization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outra Alternativa: Pipeline de NLP do Spacy\n",
    "\n",
    "O Spacy permite que todas as etapas realizadas de pré-processamento sejam realizadas em uma única chamada, num processo conhecido como *pipeline* de NLP. \n",
    "\n",
    "A diferença principal em relação ao NLTK está no fato de que, internamente, são utilizados modelos de Machine Learning, e modelos adicionais, como Entidades Nomeadas, são também oferecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stems</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>pos tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NY. Every corner this past Sunday … https://t....</td>\n",
       "      <td>ny. every corner this past sunday … https://t....</td>\n",
       "      <td>ny   every   corner   this   past   sunday   LINK</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, link]</td>\n",
       "      <td>[ny, everi, corner, thi, past, sunday, link]</td>\n",
       "      <td>[ny, every, corner, this, past, sunday, link]</td>\n",
       "      <td>[PROPN, DET, NOUN, PRON, ADJ, PROPN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...</td>\n",
       "      <td>@adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...</td>\n",
       "      <td>USERNAME   LINK   please   donate   we   great...</td>\n",
       "      <td>[username, link, please, donate, we, greatly, ...</td>\n",
       "      <td>[usernam, link, pleas, donat, we, greatli, app...</td>\n",
       "      <td>[username, link, please, donate, we, greatly, ...</td>\n",
       "      <td>[PROPN, PROPN, INTJ, VERB, PRON, ADV, VERB, DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @nymetrowx: Daylight saving time ends this ...</td>\n",
       "      <td>rt @nymetrowx: daylight saving time ends this ...</td>\n",
       "      <td>rt   USERNAME   daylight   saving   time   end...</td>\n",
       "      <td>[rt, username, daylight, saving, time, ends, t...</td>\n",
       "      <td>[rt, usernam, daylight, save, time, end, thi, ...</td>\n",
       "      <td>[rt, username, daylight, save, time, end, this...</td>\n",
       "      <td>[PROPN, PROPN, NOUN, VERB, NOUN, VERB, DET, PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @YB_2: The NYC transparency law goes into e...</td>\n",
       "      <td>rt @yb_2: the nyc transparency law goes into e...</td>\n",
       "      <td>rt   USERNAME   the   nyc   transparency   law...</td>\n",
       "      <td>[rt, username, the, nyc, transparency, law, go...</td>\n",
       "      <td>[rt, usernam, the, nyc, transpar, law, goe, in...</td>\n",
       "      <td>[rt, username, the, nyc, transparency, law, go...</td>\n",
       "      <td>[PROPN, PROPN, DET, PROPN, PROPN, NOUN, VERB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tune in to the @DistrictingNYC Public Meeting ...</td>\n",
       "      <td>tune in to the @districtingnyc public meeting ...</td>\n",
       "      <td>tune   in   to   the   USERNAME   public   mee...</td>\n",
       "      <td>[tune, in, to, the, username, public, meeting,...</td>\n",
       "      <td>[tune, in, to, the, usernam, public, meet, ton...</td>\n",
       "      <td>[tune, in, to, the, username, public, meeting,...</td>\n",
       "      <td>[NOUN, ADP, ADP, DET, PROPN, ADJ, NOUN, NOUN, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  NY. Every corner this past Sunday … https://t....   \n",
       "1  @AdrienneMaloof \\n\\nhttps://t.co/UUDW1wlARR\\n\\...   \n",
       "2  RT @nymetrowx: Daylight saving time ends this ...   \n",
       "3  RT @YB_2: The NYC transparency law goes into e...   \n",
       "4  Tune in to the @DistrictingNYC Public Meeting ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ny. every corner this past sunday … https://t....   \n",
       "1  @adriennemaloof \\n\\nhttps://t.co/uudw1wlarr\\n\\...   \n",
       "2  rt @nymetrowx: daylight saving time ends this ...   \n",
       "3  rt @yb_2: the nyc transparency law goes into e...   \n",
       "4  tune in to the @districtingnyc public meeting ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ny   every   corner   this   past   sunday   LINK   \n",
       "1  USERNAME   LINK   please   donate   we   great...   \n",
       "2  rt   USERNAME   daylight   saving   time   end...   \n",
       "3  rt   USERNAME   the   nyc   transparency   law...   \n",
       "4  tune   in   to   the   USERNAME   public   mee...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0      [ny, every, corner, this, past, sunday, link]   \n",
       "1  [username, link, please, donate, we, greatly, ...   \n",
       "2  [rt, username, daylight, saving, time, ends, t...   \n",
       "3  [rt, username, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, username, public, meeting,...   \n",
       "\n",
       "                                               stems  \\\n",
       "0       [ny, everi, corner, thi, past, sunday, link]   \n",
       "1  [usernam, link, pleas, donat, we, greatli, app...   \n",
       "2  [rt, usernam, daylight, save, time, end, thi, ...   \n",
       "3  [rt, usernam, the, nyc, transpar, law, goe, in...   \n",
       "4  [tune, in, to, the, usernam, public, meet, ton...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0      [ny, every, corner, this, past, sunday, link]   \n",
       "1  [username, link, please, donate, we, greatly, ...   \n",
       "2  [rt, username, daylight, save, time, end, this...   \n",
       "3  [rt, username, the, nyc, transparency, law, go...   \n",
       "4  [tune, in, to, the, username, public, meeting,...   \n",
       "\n",
       "                                            pos tags  \n",
       "0        [PROPN, DET, NOUN, PRON, ADJ, PROPN, PROPN]  \n",
       "1  [PROPN, PROPN, INTJ, VERB, PRON, ADV, VERB, DE...  \n",
       "2  [PROPN, PROPN, NOUN, VERB, NOUN, VERB, DET, PR...  \n",
       "3  [PROPN, PROPN, DET, PROPN, PROPN, NOUN, VERB, ...  \n",
       "4  [NOUN, ADP, ADP, DET, PROPN, ADJ, NOUN, NOUN, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fazendo tudo de uma só vez com Spacy\n",
    "\n",
    "data['tokens'], data['pos tags'], data['lemmas'] = zip(*data['clean_text'].apply(pipeline.nlp_pipeline))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>3. Representação de Textos <br> Utilizando Vetores Numéricos</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "- É a forma mais simples de representação de palavras para um algoritmo de aprendizado de máquina\n",
    "- Cada documento é representado por um vetor de tamanho N, onde N é a quantidade de tokens distintos no vocabulário\n",
    "- Cada token único é representado por uma posição no vetor, a ser preenchida com a quantidade de vezes que o token ocorre no documento\n",
    "- Caso não haja nenhuma ocorrência de um token no documento, sua respectiva posição recebe o valor 0\n",
    "\n",
    "<center> Tabela: Exemplo de representação utilizando BoW. Adaptade de Machine Learning Mastery<sup>1</sup> </center>\n",
    "\n",
    "| Documento | it | was | the | best | of| times | worst | age | wisdom | foolishness |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| it was the best of times | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "| it was the worst of times | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| it was the age of wisdom | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 |\n",
    "| it was the age of foolishness | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">1</a>: Disponível em https://machinelearningmastery.com/gentle-introduction-bag-words-model/. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - facilidade de implementação\n",
    "    - ajuda a identificar as palavras significativas de um texto, baseado em sua frequência\n",
    "\n",
    "- **Desvantagens**\n",
    "    - grande consumo de memória para vocabulários muito extensos\n",
    "    - representações esparsas e apresentando enviesamento em relação a termos muito frequentes\n",
    "    - não considera a posição das palavras no texto:\n",
    "    \n",
    "    \n",
    "| Documento | I | went | to | the | cinema | and | liked | movie | but | not | popcorn\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| I went to the cinema and liked the movie but not the popcorn | 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "| I went to the cinema and liked the popcorn but not the movie| 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<80x623 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1273 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelos_stats = ModelosEstatisticos()\n",
    "modelos_stats.bow(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "- Dado um conjunto de documentos textuais, **palavras que ocorrem em muitos destes documentos provavelmente não serão relevantes para distinguir o conteúdo de cada um deles** \\[Robertson 2004\\]\n",
    "- Para superar essa limitação (que ocorre com o BoW), o TF-IDF primeiro calcula a frequência com que um determinado termo ocorra em um documento (i.e., TF)\n",
    "\n",
    "- E, então, é realizada a ponderação com a frequência com que o mesmo termo ocorra em um conjunto de documentos (i.e., IDF):\n",
    "\n",
    "<img src=\"figs/idf.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "onde $N$ corresponde ao número de documentos analisados e $df_i$ corresponde ao número de documentos em que ocorre o termo em questão.\n",
    "\n",
    "Assim, podemos representar cada elemento de uma matriz termo × documento por:\n",
    "\n",
    "<img src=\"figs/tfidf.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - o TF-IDF representou um passo importante para o desenvolvimento de técnicas relacionadas à extração de informação\n",
    "    - Tendo depois se expandido para outras técnicas de NLP, como extração de tópicos e classificação de texto\n",
    "\n",
    "- **Desvantagens**\n",
    "    - representação vetorial resultante ter o mesmo tamanho do vocabulário do texto (grande consumo de memória)\n",
    "    - não considera as proximidades semânticas em que os termos ocorrem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<80x135 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelos_stats.tfidf(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**\n",
    "\n",
    "Como vimos, o TF-IDF, apesar de muito versátil, tem como principal limitação o tamanho dos vetores gerados, que correspondem na quantidade de documentos (ou sentenças) analisadas.\n",
    "\n",
    "Para resolver este problema, uma técnica que pode ser utilizada é o PCA (Principal Component Analysis), cuja função é reduzir a dimensionalidade dos vetores mantendo a informação codificada por ele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](http://www.nlpca.org/fig_pca_principal_component_analysis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = modelos_stats.tfidf(data[\"clean_text\"])\n",
    "\n",
    "pca_modelo, pca_transformação = modelos_stats.pca(tf_idf, n_components = 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings\n",
    "\n",
    "- Com o Word2Vec, proposto por \\[Mikolov et al. 2013\\] inaugurou-se um novo paradigma de representação semântica vetorial: Word Embeddings\n",
    "- Principal característica é a atribuição de um vetor denso, de tamanho arbitrário, a cada palavra de um corpus, gerado a partir do treinamento de redes neurais\n",
    "- Esses vetores são gerados a partir da análise das **janelas semânticas** em que tais palavras venham a ocorrer\n",
    "- Resultou em uma série de inovações e vantagens:\n",
    "    - os vetores não precisam ser treinados apenas no corpus em que será feita a análise. Normalmente, são utilizados vetores pré-treinados sobre um corpus com vocabulário mais extenso, como a Wikipedia, e apenas são otimizados sobre o corpus avaliado\n",
    "    - É eliminado o problema da esparsidade de dados\n",
    "    - Resultados experimentais indicam que relações semânticas complexas podem ser capturadas, por exemplo: \n",
    "        - a relação entre os nomes de países e suas respectivas capitais\n",
    "        - sinônimos obtém representações mais próximas, enquanto antônimos se distanciam de forma equivalente no espaço vetorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "O Word2Vec é composto por dois modelos:\n",
    "- **Continuous Bag of Words (CBOW):** toma-se como entrada uma janela de palavras, e tenta-se prever qual palavra ocorreria naquele contexto\n",
    "- **Skip-Gram:** toma-se como entrada uma palavra, e a partir dela, tenta-se prever as palavras que venham a ocorrer em sua vizinhança\n",
    "\n",
    "Veja a arquitetura de ambos os modelos na ilustração a seguir:\n",
    "\n",
    "<img src=\"figs/word2vec.png\" style=\"float: center; zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Treinamento do modelo Word2Vec Skip-gram\n",
    "\n",
    " - toma como entrada um corpus de texto, com um tamanho $N$ de vocabulário\n",
    " - Inicialmente, são atribuídos valores aleatórios para cada um dos vetores das palavras do vocabulário\n",
    " - Os pesos são ajustados ao longo do treinamento, para que:\n",
    "     - palavras ocorrendo em contextos semelhantes obtenham representações vetoriais (embeddings) próximos\n",
    "     - e, palavras de significado distante, que não costumam ocorrer nos mesmos contextos, obtenham representações o mais distante possíveis entre si\n",
    " - Em cada palavra analisada por iteração, as instâncias positivas são palavras que realmente ocorrem em sua proximidade, e as negativas, uma seleção, de tamanho proporcional, de palavras que não ocorrem\n",
    " - O treinamento do algoritmo irá, então, minimizar a função de perda (Loss Function), cujo objetivo é maximizar o produto escalar entre uma palavra e um exemplo positivo de contexto, e minimizar em relação aos exemplos negativos\n",
    " \n",
    "- O tamanho do contexto observado é arbitrário, sendo definido como um parâmetro de treinamento\n",
    "\n",
    "- Ao final, duas representações são aprendidas:\n",
    "    - uma matriz $W$ contendo em cada vetor $w_i$ um word embedding para cada palavra do vocabulário\n",
    "    - e, uma matriz $C$ em que cada vetor $c_i$ é um embedding relativo ao contexto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemplo Word2Vec Skip-gram\n",
    "\n",
    "Adaptado de [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "- Arquitetura do modelo:\n",
    "\n",
    "<img src=\"figs/skipgramarch.png\" style=\"float: center; zoom:80%;\" />\n",
    "\n",
    "- Word embeddins de uma determinada palavra:\n",
    "\n",
    "<img src=\"figs/matrix_mult_w_one_hot.png\" style=\"float: center; zoom:80%;\" />\n",
    "\n",
    "- Cálculo do resultado na camada de saída:\n",
    "\n",
    "<img src=\"figs/output_weights_function.png\" style=\"float: center; zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Criando uma modelo de Word Embeddings com Word2Vec\n",
    "\n",
    "* Utilizaremos a biblioteca Gensim 4.2.0 -- [documentação](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* Vamos utilizar os tokens obtidos após a etapa de pré-processamento, uma vez que a entrada deve ser uma lista de tokens, como por exemplo:\n",
    "\n",
    "<center>$[[\"first\", \"sentence\"], [\"second\", \"sentence\"]]$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cria o modelo e mostra o tamanho do vocabulário\n",
    "w2v_model = WordEmbeddings().word2vec(data[\"tokens\"])\n",
    "len(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['username', 'nyc', 'link', 'rt', 'this', 'the', 'is', 'to', 'i', 'in', 'and', 've', 'you', 'times', 'watched', 'a', 'peak', 'ten', 'for', 'of', 's', 'are', 'on', 'at', 'my', 'it', 'me', 'next', 'an', 'have', 'city', 'we', 'be', 'new', 't', 'your', 'just', '10', 'can', 'november', 'if', 'their', 'from', 'time', 'all', 'day', 'was', 'help', 'state', 'mayor', 'has', 'they', 'not', 'up', 'cars', 'with', 'consulate', 'border', 'because', 'or', 'now', 'our', 'cities', 'take', 'today', 'them', 'as', 'security', 'by', 'sunday', 'amp', 'will', 'real', 'like', 'please', 'make', 'two', 'no', 'week', 'wow', 'should', 'social', 'payments', 'low', '19', '5th', '12', 'd', 'home', 'that', 'his', 'right', 'york', 'really', 'gt', 'angle', 'building', 'didn', 'always', 'down', '1st', 'do', 'want', 'when', 'what', 'tickets', 'more', 'appreciate', 'wei', 'get', 'but', '5', 'stuff', 'would', 'holes', 'over', 'unwittingly', 'rent', 'becoming', 'adams', 'nypd', 'man', 'crime', 'so', 'min', 'trip', 'rth', 'high', 'many', 'subway', 'voice', 'school', 'brandon', 'hope', 'say', 'judd', 'old', 're', 'tonight', 'train', 'story', 'first', 'sure', 'enter', '4', 'fx', 'see', 'here', 'don', 'got', 'order', 'russian', 'into', 'tomorrow', 'officer', 'nursing', 'benefits', 'allowing', 'thera', 'healthcare', 'cover', 'truetelehealth', 'warned', 'back', 'telehealth', 'happen', 'nightmare', '1980', 'telemedicine', 'grew', 'homes', 'shoot', 'turnstile', 'students', 'disturbed', 'cooked', 'loony', 'yourself', 'common', 'sense', 'shrink', 'support', 'go', 'wayne', 'rot', 'nj', 'who', 'lonely', 'happy', 'pa', 'paweek', 'wefindaway', 'fmty', 'available', 'seriously', 'democrats', 'jumpers', 'cop', 'then', 'polls', 'open', 'cast', 'democrat', 'took', 'away', 'qualified', 'immunity', 'risk', 'terrorists', 'throw', 'rocks', 'scream', 'faces', 'while', 'sitting', 'thank', 'antifa', 'blm', 'msm', 'couldn', 'pretty', 'reli', 'tune', 'public', 'meeting', '6', 'pm', 'acc', 'partner', 'rescues', 'there', 'dozens', 'only', 'handful', 'consistently', 'team', 'year', 'uncut', 'refusing', 'supreme', 'court', 'reinstate', 'unvaccinated', 'officers', 'could', 'posted', 'texas', 'needs', 'spend', 'billion', 'range', 'salary', 'max', 'required', 'every', 'corner', 'past', 'donate', 'greatly', 'any', 'aid', 'during', 'eviction', 'homelessness', 'imminent', 'autistic', 'son', 'daylight', 'saving', 'ends', '6th', '2022', 'sunset', 'evening', '46pm', 'transparency', 'law', 'goes', 'effect', 'job', 'postings', 'funds', 'secure', 'parade', 'music', 'jewish', 'movement', 'portable', 'educational', 'outreach', 'center', 'safe', 'ok', 'hurry', 'adoptdontshop', 'tursday', 'tacotuesday', 'adventure', 'movie', 'administration', 'fall', 'share', 'great', 'news', 'added', 'gregg', 'allman', '75th', 'birthday', 'jam', 'dec', '8th', 'halloween', 'chabad', 'la', 'used', 'vehicle', 'won', 'anyone', 'denver', 'miami', 'wants', 'collab', 'dm', 'll', 'tour', 'weeks', 'fwiw', 'buildings', 'commission', 'part', 'extortion', 'racket', 'forcing', 'pay', 'expediter', 'plans', 'approved', 'languish', 'few', 'years', 'soho', 'mitzvah', 'tank', 'oct', '3653', '13', '14th', 'straight', 'road', 'moving', 'sweat', 'water', 'rain', 'drenched', 'hulu', 'stream', 'premieres', 'ahs', 'hold', 'russia', 'impersonating', 'pulled', 'hell', 'nice', 'mind', 'clarence', 'match', 'excited', 'corningcommunitycollege', 'bus', 'host', 'ec', 'told', 'ordering', 'resident', 'harlem', 'yeh', 'fillin', 'kate', 'online', 'being', 'after', 'gunpoint', 'seokhwa', 'interstate', 'imagine', '위아이', 'yohan', '김요한', '요한', '강석화', 'running', '석화', 'busking', '220930', 'dropped', 'different', 'air', 'systems', 'through', 'watch', 'safer', 'chain', 'robbed', 'gets', 'driver', 'wise', 'accident', 'bearable', 'had', 'honestly', 'trams', 'trains', 'pedestrians', 'designed', 'walk', 'seniors', 'juniors', 'amazing', 'invite', 'lanes', 'bike', 'blocking', 'photos', 'submit', 'residents', 'bill', 'myself', 'interesting', 'well', 'omg', 'cryin', 'm', 'origin', 'annoyed', 'adjusting', '2018', 'ride', 'telling', 'meant', 'mosque', 'distancing', 'enforcing', 'he', 'average', 'horizon', 'experience', 'dutch', 'unedited', 'raw', 'whatever', 'flat', 'villain', 'since', 'affordable', 'parts', 'keeping', 'det', 'self', 'deserve', 'russians', 'ethnic', 'annexes', 'lizzo', 'putin', 'am', 'opportunity', 'collegiate', 'level', 'diverse', 'track', 'play', 'been', 'spraying', 'apartment', 'increase', 'giant', 'renewal', 'lease', 'drones', 'paint', 'pres', 'need', 'solange', 'flute', 'yr', '200', 'madison', 'idc', 'country', 'punitive', '15', 'influx', 'cope', 'struggle', 'schools', 'hilarious', 'tap', 'purchase', 'migrant', 'chance', 'lottery', 'importance', 'entered', 'arrive', 'migrants', '500', 'kids', 'emergency', 'consequences', 'local', 'one', 'stoked', 'free', 'people', 'giving', 'seeing', 'flood', 'b', 'concerns', 'representatives', 'problems', 'caused', 'hazards', 'thousand', 'says', 'exactly', 'nov', 'harrypotter', '03', '09', 'stonksteam', 'charleston', '7th', '21st', 'levels', '20', 'lake', 'salt', '17th', 'jose', 'san', 'key', '3695', 'sanctuary', '00', 'give', 'overnight', '3', 'swing', '75', '3571', '3579', '50', '25', 'she', '7', 'prints', 'single', 'bottom', 'too', 'communists', 'much', 'kinfos', 'private', 'started', 'which', 'bargaining', 'update', 'rtp_fm', 'realtimepayments', 'across', 'below', 'click', 'details', 'live', '18th', 'later', 'hospitals', 'presbyterian', 'annual', '3rd', 'ramifications', 'graffiti', 'terrible', 'recent', 'her', '4th', '2nd', 'nurses', 'vmas', 'photo', 'heart', 'polaroids', 'sofia', 'mana', 'summit', 'vp', 'create', 'likes', 'campaign', 'ad', 'current', 'foreverlove', 'ميو_سوباسيت', 'mewsuppasit', '400k', 'enough', 'posts', 'boy', 'america', 'system', 'overwhelm', 'chaos', 'distracted', 'board', 'kyriacou', 'where', 'lee', 'speaker', 'esteem', 'announce', 'proud', 'st', 'happening', 'wrong', 'life', 'premiere', 'biggest', 'baby', 'rock', 'three', 'ny']\n"
     ]
    }
   ],
   "source": [
    "## Mostra o vocabulário\n",
    "\n",
    "vocab = list(w2v_model.wv.index_to_key)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('law', 0.3727055490016937), ('here', 0.2287950962781906), ('6th', 0.22740225493907928), ('pm', 0.22448128461837769), ('consequences', 0.22198377549648285), ('on', 0.21809211373329163), ('miami', 0.21735580265522003), ('for', 0.2144017070531845), ('next', 0.21344465017318726), ('prints', 0.21184755861759186)]\n"
     ]
    }
   ],
   "source": [
    "## Mostra as palavras mais similares com a palavra informada.\n",
    "## Caso a palavra não esteja no vocabulário, i.e., OOV, retorna uma mensagem de erro\n",
    "\n",
    "try:\n",
    "    sims = w2v_model.wv.most_similar(\"nyc\")\n",
    "    print(sims)\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[-0.14854656  0.0703124   0.09368501  0.10793454  0.12523901 -0.1302203\n",
      "  0.02956743  0.12924846 -0.0651916  -0.11741234 -0.0093156  -0.16809571\n",
      " -0.09970358  0.12405372  0.06246262  0.10615756  0.12018472  0.11764802\n",
      " -0.06197973 -0.04261533  0.04335519 -0.06316292  0.13968146 -0.17848755\n",
      "  0.10671734  0.04927135 -0.10355555  0.07288527 -0.04439036  0.12664498\n",
      "  0.18437018 -0.06675369 -0.00425248 -0.1032754   0.05041345  0.06170192\n",
      "  0.11883572  0.08689056  0.14688557  0.13550428  0.14124466 -0.13105017\n",
      " -0.16456398 -0.00482567 -0.03922172  0.11528422  0.08363017 -0.03480077\n",
      "  0.03729555  0.03953395  0.14087081 -0.18496619  0.00314404  0.05704498\n",
      " -0.03785138  0.15237801  0.16376501  0.10867805 -0.02409237  0.14483503\n",
      " -0.13697939  0.05721758 -0.07918046 -0.09169526  0.04065131  0.11126306\n",
      "  0.13019778 -0.08137874  0.09976059  0.12705539 -0.07173817 -0.14155449\n",
      "  0.10773605  0.1097723   0.00196524 -0.10507842 -0.11929688 -0.0446636\n",
      "  0.070089   -0.06452617 -0.17084923  0.05780748  0.06345602 -0.08619292\n",
      "  0.01295946 -0.03656619 -0.00467899 -0.14709473  0.06273913 -0.07179453\n",
      "  0.03969624 -0.02286234  0.02696322 -0.1291057  -0.02394891  0.05882761\n",
      "  0.09955513 -0.06133453 -0.14796458  0.07753509]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_model.wv.get_vector(\"nyc\", norm=True)   # Retorna o numpy array da palavra\n",
    "print(vector.shape)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FastText\n",
    "\n",
    "No FastText, cada palavra é representada por uma sequência de N-gramas de caracteres. Por exemplo, dada uma palavra como “<farol>”, onde os caracteres especiais “<” e “>” indicam o início e o final da palavra, respectivamente, e uma janela (de tamanho arbitrário, escolhido pelo usuário) n = 3 , o FastText toma como entrada os 3-gramas:\n",
    "\n",
    "- *“<fa”, “far”, “aro”, “rol”, “ol>”* além da palavra completa *“farol”*.\n",
    "\n",
    "A cada um destes N-gramas, é associada uma representação vetorial única. Então, a representação vetorial\n",
    "final da palavra, consistirá na soma dos vetores de representação de todos os seus N-gramas\n",
    "[Joulin et al. 2016, Bojanowski et al. 2017].\n",
    "\n",
    "Isso ocorre para resolver uma limitação do Word2Vec original: o fato de que, nele, não há suporte para palavras OOV (out-of-vocabulary, ou fora do vocabulário.)\n",
    "\n",
    "Com o FastText, a cada uma destas palavras originalmente fora do vocabulário, se atribui uma representação próxima àquelas de grafia próxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.fasttext(dados['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "1. <span style=\"color:red\">SkipThought</span>\n",
    "2. InferSent\n",
    "3. **Universal Sentence Encoder (USE)**\n",
    "4. **SentenceBERT (SBERT)**\n",
    "5. Language-Agnostic SEntence Representations (LASER)\n",
    "6. Multilingual Universal Sentence Encoder (mUSE)\n",
    "7. **Language-agnostic BERT Sentence Embedding (LaBSE)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## SkipThought\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## InferSent\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# InferSent\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 9GB disponíveis em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "infersent_embeddings = SentenceEmbeddings().infersent(sentences)\n",
    "infersent_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## USE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# USE\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 1GB disponível em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "use_embeddings = SentenceEmbeddings().use(sentences)\n",
    "use_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SBERT\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SBERT\n",
    "\n",
    "- Utilizaremos o modelo **all-MiniLM-L6-v2**, que é 5x mais rápido que sua versão base (**all-mpnet-base-v2**) e significativamente menor (de 420MB para 80MB), mas ainda mantém um bom desempenho\n",
    "- O termo **all-** indica que o modelo foi treinado com todos os dados disponíveis (mais de 1 bilhão de pares de treinamento) e são projetados como modelos de propósito geral\n",
    "- Para mais detalhes, acesse a página do [SBERT](https://www.sbert.net/docs/pretrained_models.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SBERT\n",
    "sbert_embeddings = SentenceEmbeddings().sbert(sentences)\n",
    "sbert_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## LASER\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# LASER\n",
    "# Antes de utilizar o LASER, você deve fazer o download do modelo.\n",
    "# Para isso, descomente a linha abaixo.\n",
    "#!python -m laserembeddings download-models \"data\"\n",
    "# Você pode informar o código de idioma (ISO 639-1), para cada sentença da lista.\n",
    "# Por padrão, consideramos que todas as sentenças estão escritas em inglês (\"en\").\n",
    "laser_embeddings = SentenceEmbeddings().laser(sentences)\n",
    "laser_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## mUSE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# mUSE\n",
    "# Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "# Certifique-se de ter pelo menos 300MB disponível em disco para isso.\n",
    "# Devido ao download, a primeira execução é lenta\n",
    "muse_embeddings = SentenceEmbeddings().muse(sentences)\n",
    "muse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LaBSE\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# LaBSE\n",
    "labse_embeddings = SentenceEmbeddings().labse(sentences)\n",
    "labse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similaridade entre sentenças\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo baseado em:\n",
    "# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "sent_emb = SentenceEmbeddings().labse(sentences) #escolha o modelo de sentence embeddings de sua preferência\n",
    "plot_similarity(sentences, sent_emb, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelagem e Extração de Conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agrupamento de Textos\n",
    "- k-means\n",
    "- Agrupamento Hierárquico\n",
    "- Detecção de Comunidades em Grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k-means\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agrupamento Hierárquico\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detecção de Comunidades em Grafos\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelagem de Tópicos\n",
    "\n",
    "* Objetivo: identificar estruturas semânticas em um corpus\n",
    "* Processo não supervisionado\n",
    "* A ordem das palavras não importa, portanto podem ser usadas representações como o Bag of Words (BoW) ou Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos em um corpus de notícias\n",
    "\n",
    "* delimitar diferentes eventos (e.g., confronto entre Rússia e Ucrânia, Copa do Mundo, etc.)\n",
    "* identificar grandes temas (e.g., economia, política, educação, etc.)\n",
    "* pautas de discussão (e.g., aborto, questões climáticas, combate ao crime, etc.)\n",
    "* !!! Ou todos os casos acima misturados -- o que é comum de acontecer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Funcionamento de um modelo de tópicos\n",
    "\n",
    "* cada documento pode ser representado por um histograma com a contagem de cada termo contido nele\n",
    "* a forma desse histograma é proveniente de uma distribuição entre $k$ tópicos \n",
    "* os $k$ tópicos são distribuídos entre os termos no vocabulário do corpus\n",
    "\n",
    "O objetivo da modelagem de tópicos, então, é aprender essas distribuições.\n",
    "\n",
    "<img src=\"figs/topic-model.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "Fonte: https://pyro.ai/examples/prodlda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Técnicas para modelagem de tópicos\n",
    "\n",
    "* Latent Dirichlet Allocation (LDA)\n",
    "* Latent Semantic Analysis (LSA) ou LSI (redução dimencional)\n",
    "* Probabilistic Semantic Analysis (pLSA) (versão probabilística do LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Pré-processamento para modelagem de tópicos\n",
    "\n",
    "* tentar preservar palavras que podem ser representativas para o domínio\n",
    "* não é necessário preservar a sequência dos termos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Desafios\n",
    "\n",
    "Obs: os exemplos a seguir são de tópicos identificados em um corpus de páginas web sobre as Eleições de 2018 no Brasil e Eleições de 2019 no Canadá.\n",
    "\n",
    "* Muitas vezes os tópicos são difíceis de serem caracterizados, necessitando conhecimento específico de domínio\n",
    "    * e.g. \"battisti, pf, italiano, italia, grafico, recurso, utc, extradicao, moro, deus\"\n",
    "    * e.g. \"pipeline, climate, oil, energy, fossil_fuel, carbon, france, kinsella, people_party, industry\"\n",
    "* As top N palavras de um tópico nem sempre delimitam claramente o assunto\n",
    "    * e.g. \"ex_presidente, ciro_gomes, ciro, artista, cunha, universidade, dilma, eduardo, classificar, reeleicao\"\n",
    "* O valor $k$ é escolhido manualmente\n",
    "    * mesmo usando métricas como score de coerência e distribuição dos tópicos ao longo do corpus, ainda é necessário o julgamento humano da adequabilidade do valor $k$\n",
    "* Tópico evoluem ao longo do tempo (para isso existem modelos temporais de tópicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplo prático\n",
    "\n",
    "Ao final deste notebook, colocamos um exemplo prático usando o LDA em um dataset de notícias sobre as eleições canadenses de 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compreensão Semântica e Emocional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Detecção de Intenção\n",
    "\n",
    "<img src=\"figs/ir-example.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Detecção de intenção\n",
    "- Uma intenção fornece uma interpretação geral do significado de uma expressão\n",
    "- Normalmente, é uma tarefa abstraída em um processo de classificação\n",
    "- Para isso, o primeiro passo é a obtenção de um conjunto de expressões rotuladas (*corpus*) para treinamento do modelo de classificação\n",
    "- Como veremos a seguir, existem muitos *corpus* disponíveis publicamente para essa tarefa.\n",
    "- Mas, caso deseje preparar o seu próprio *corpus* com dados de mídias sociais, você pode utilizar a modelagem de tópicos para determinar quais assuntos estão presentes no *corpus* e realizar a separação dos dados e, então, revisar e rotular manualmente cada um deles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus\n",
    "- Iremos considerar o conjunto de dados disponível em [Wang, Jinpeng, et al.]<sup>[1](#footnoteIntentCorpus)</sup>\n",
    "- Categorias de intenção nos *tweets*: **Food & Drink, Travel, Career & Education, Goods & Services, Event & Activities, Trifle, Non-intent**\n",
    "- Veja na Tabela a seguir mais detalhes sobre os dados:\n",
    "\n",
    "<center> Tabela: Intenções e exemplos do conjunto de dados [Wang, Jinpeng, et al.]<sup>1</sup> </center>\n",
    "\n",
    "| Categoria | # (%) | Exemplo |\n",
    "| --- | --- | --- |\n",
    "| Food & Drink | 245 (11,5%) | hungry...i need a salad......four more days to the BEYONCE CONCERT... |\n",
    "| Travel | 187 (8,78%) | I need a vacation really bad. I need a trip to Disneyland! |\n",
    "| Career & Education | 159 (7,46%) | this makes me want to be a lawyer RT @someuser new favorite line from an ... |\n",
    "| Goods & Services | 251 (11,78%) | mhmmm, i wannna a new phoneeee. ... i have to go to the hospital. ... |\n",
    "| Event & Activities | 312 (15.07%) | on my way to go swimming with the twoon @someuser; i love her so muchhhhh! |\n",
    "| Trifle | 436 (20,47%) | I'm so happy that I get to take a shower with myself. :D |\n",
    "| Non-intent | 531 (24,92%) | So sad that Ronaldo will be leaving these shores...http://URL |\n",
    "\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">1</a>: Wang, Jinpeng, et al. \"Mining user intents in twitter: A semi-supervised approach to inferring intent categories for tweets.\" Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelo de Classificação de Intenções\n",
    "<br>\n",
    "\n",
    "<center>Redes Neurais Recorrentes (RNN) X Sentence Embeddings (imagem de [Feng et al. 2020])</center>\n",
    "\n",
    "<img src=\"figs/bilstm-ir.png\" style=\"float: left; zoom:60%;\" />\n",
    "<img src=\"figs/labse.png\" style=\"float: right; zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#temporario\n",
    "intents = [\n",
    "    \"Smartphones\",\n",
    "    \"Smartphones\",\n",
    "    \"Smartphones\",\n",
    "\n",
    "    \"Weather\",\n",
    "    \"Weather\",\n",
    "    \"Weather\",\n",
    "\n",
    "    \n",
    "    \"Food and health\",\n",
    "    \"Food and health\",\n",
    "    \"Food and health\",\n",
    "    \n",
    "    \"Asking about age\",\n",
    "    \"Asking about age\"\n",
    "]\n",
    "\n",
    "intent_labse_model, X_test_labse, y_test_labse, classes = SemanticComprehension().training_intents(\"labse\", sentences, intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs = 20\n",
    "# Batch size = 32\n",
    "# Hidden layers = 300\n",
    "# Max sequence length = 280 --> tweet size\n",
    "intent_bilstm_model, X_test_rnn, y_test_rnn, classes = SemanticComprehension().training_intents(\"bilstm\", sentences, intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaBSE's performance\n",
    "y_hat = intent_labse_model.predict(X_test_labse)\n",
    "SemanticComprehension().plot_confusion_matrix(y_test_labse, y_hat, classes, \"figs/ir-labse-cm.png\")\n",
    "\n",
    "# RNN's performance\n",
    "y_hat = intent_bilstm_model.predict(X_test_rnn)        \n",
    "SemanticComprehension().plot_confusion_matrix(np.argmax(y_test_rnn,axis=1), np.argmax(y_hat, axis=1), classes, \"figs/ir-rnn-cm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix de Confusão\n",
    "<br>\n",
    "\n",
    "<center>Redes Neurais Recorrentes (RNN) X Sentence Embeddings</center>\n",
    "\n",
    "<img src=\"figs/ir-rnn-cm.png\" style=\"float: left; zoom:22%;\" />\n",
    "<img src=\"figs/ir-labse-cm.png\" style=\"float: right; zoom:22%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Predição de intents para os tweets coletados\n",
    "intents = CompreensaoSemantica().predicao_intencoes(intent_labse_model, sentences) ## configurado apenas para o modelo LaBSE\n",
    "intents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconhecimento de Entidades Nomeadas\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus\n",
    "- Iremos considerar o conjunto de dados disponível em  https://www.kaggle.com/code/amoghjrules/twitter-entity-recognition-using-bilstms\n",
    "- abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "for sentence in tqdm(sentences):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    entity = {}\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity[i] = {\n",
    "                        \"value\":ent.text,\n",
    "                        \"entity\":ent.label_,\n",
    "                        \"start\":ent.start_char,\n",
    "                        \"end\":ent.end_char\n",
    "                    }\n",
    "    entities.append(entity)\n",
    "\n",
    "print(entities[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Análise de Sentimentos (AS)\n",
    "\n",
    "* Também conhecida como **Mineração de Opiniões**\n",
    "* AS é o \"estudo computacional das opiniões, atitudes e emoções de pessoas em relação a uma entidade\" [Medhat et al. 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tarefas de AS\n",
    "\n",
    "* **Detecção de sentimento**\n",
    "    - e.g. positivo, negativo ou neutro\n",
    "* **Identificação de emoções**\n",
    "    - e.g. sentimentos como raiva, antecipação, nojo, medo, alegria, tristeza, surpresa, confiança, etc. \n",
    "* **Detecção de toxicidade** \n",
    "    - e.g. categorias como insulto, profanidade, conteúdo sexualmente explícito, etc. [Jigsaw 2022]\n",
    "* **Análise multilíngue de sentimentos**\n",
    "* **Detecção de sarcasmo**\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplos de aplicações de AS\n",
    "\n",
    "* Identificação de comentários agressivos em notícias [Jigsaw 2022]\n",
    "* Extração da opinião pública sobre um candidato ou partido político [Pang et al. 2008]\n",
    "* Priorização de respostas a avaliações negativas de produtos [Bougie et al. 2003] \n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vantages da AS\n",
    "\n",
    "* Permite análises em larga escala\n",
    "* Reduz a subjetividade provocada por avaliadores humanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Níveis da AS\n",
    "\n",
    "1. **Nível de documento** - premissa: documento expressa opinião sobre apenas uma entidade; \n",
    "2. **Nível de frase** - premissa: frase expressa opinião sobre apenas uma entidade;  \n",
    "3. **Nível de aspecto** - múltiplas opiniões sobre múltiplos aspectos (ou alvos)\n",
    "    - e.g. “A <span style=\"color:#f00\">qualidade de voz</span> deste telefone <span style=\"color:#f00\">não é boa</span>, mas a <span style=\"color:#00f\">vida útil da bateria</span> é <span style=\"color:#00f\">longa</span>”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Abordagens para criação de modelos de AS\n",
    "\n",
    "1. **Usando Léxicos** \n",
    "    * <span style=\"color: #088B00\">VANTAGEM</span>: independência de domínio\n",
    "    * <span style=\"color: #f00\">DESVANTAGEM</span>: menor precisão e baixa escalabilidade\n",
    "2. **Aprendizado de máquina (ML)**\n",
    "    * <span style=\"color: #088B00\">VANTAGEM</span>: maior precisão\n",
    "    * <span style=\"color: #f00\">DESVANTAGEM</span>: maior dependência de domínio\n",
    "3. **Híbrido (léxico + ML)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### A seguir iremos apresentar\n",
    "\n",
    "* EmoLex\n",
    "* LIWC\n",
    "* Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frases que vamos usar como exemplo para os modelos de AS\n",
    "\n",
    "emotional_sentences = [\n",
    "\n",
    "    # exemplo de frase positiva\n",
    "    \"How good it is to live in Curitiba!\",\n",
    "\n",
    "    # exemplo de frase neutra\n",
    "    \"This car is grey.\",\n",
    "\n",
    "    # exemplo de frase negativa\n",
    "    \"Shut up, you're an idiot!\",\n",
    "\n",
    "    # exemplo de frase negativa, mas com palavras que podem confundir o modelo de AS como \"friend\"\n",
    "    \"It must be so sad to have you as a friend\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### EmoLex\n",
    "\n",
    "* Criado em 2013, é um dos maiores léxicos disponíveis em língua Inglesa\n",
    "* Baseado em unigramas e bigramas dos léxicos\n",
    "    * General Inquirer Lexicon\n",
    "    * WordNet Affect Lexicon\n",
    "* Anotações feitas via crowdsourcing pelo Mechanical Turk\n",
    "* Associa palavras com as oito emoções da teoria de Plutchik [Plutchik 1980]\n",
    "    * raiva, medo, antecipação, confiança, surpresa, tristeza, alegria e desgosto\n",
    "* Também inclui as catergorias de sentimento negativo e positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de sentenças processadas com o Emolex (frequência de emoções nas sentenças)\n",
    "EmotionComprehension.emolex(emotional_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LIWC\n",
    "\n",
    "* Ferramenta para identificar características linguísticas, psicológicas e sociais em textos [Pennebaker et al. 2001]\n",
    "* AS baseada em léxico (um dos maiores e mais completos na quantidade de termos e categorias cobertas)\n",
    "* Léxicos separados para línguas diferentes, sendo o inglês a língua padrão\n",
    "* Disponível apenas para Windows via interface gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Categorias de conteúdo disponíveis no LIWC\n",
    "\n",
    "* **emoções positivas** (e.g. amor, legal, doce, etc.)\n",
    "* **emoções negativas** (e.g. ferido, feio, desagradável, etc.)\n",
    "* **processos sociais** (e.g. filha, marido, vizinho, adulto, bebê, etc.)\n",
    "* **processos cognitivos** (e.g. pensar, conhecer, causa, etc.)\n",
    "* **processos perceptivos** (e.g. observar, escutar, sentir, etc.)\n",
    "* **processos biológicos** (e.g. comer, sangue, dor, etc.)\n",
    "* **relatividade** (e.g. chega, vai, embaixo, ontem, até, fim, etc.)\n",
    "* **preocupações sociais** (e.g. auditar, igreja, cozinhar, trabalhar, mestrado, etc.)\n",
    "* **consentimento** (e.g. concordar, ok, etc.)\n",
    "* **não-fluências e palavras de preenchimento** (e.g. hm, er, umm, certo, etc.)\n",
    "* entre outras [Tausczik and Pennebaker 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Categorias de função disponíveis no LIWC\n",
    "\n",
    "* **pronomes** \n",
    "* **preposições** \n",
    "* **artigos** \n",
    "* **conjunções** \n",
    "* **verbos auxiliares** \n",
    "* entre outras [Tausczik and Pennebaker 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Interface do LIWC\n",
    "\n",
    "<img src=\"figs/liwc.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Exemplo de processamento com o LIWC\n",
    "\n",
    "Para executar o processamento das sentenças de exemplo, será necessário [adquirir uma licença](https://www.liwc.app/buy), instalar o LIWC em um computador com Windows e processar o arquivo .csv gerado na célula a seguir. \n",
    "\n",
    "Você também poderá usar a versão de [teste online](https://www.liwc.app/demo), inserindo manualmente cada uma das sentenças geradas no .csv a seguir (essa versão é limitada a algumas poucas categorias, mas estão disponíveis as categorias de \"Negative tone\" e \"Positive tone\", bastante úteis em tarefas de AS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cria um .csv com exemplos para ser processado pelo LIWC\n",
    "pd.DataFrame({'text': emotional_sentences}).to_csv('data/emotional_sentences_LIWC.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# carrega o arquivo com exemplos processado pelo LIWC\n",
    "df_LIWC = pd.read_csv('emotional_sentences_LIWC.csv')\n",
    "\n",
    "# apresenta apenas algumas colunas mais interessantes para AS\n",
    "df_LIWC[['text', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Perspective API\n",
    "\n",
    "* Usado para identificação de toxicidade em comentários online\n",
    "* Modelo multilíngue\n",
    "* Disponibilizado gratuitamente por meio de uma iniciativa da Jigsaw, uma empresa da Google [Jigsaw 2022]\n",
    "* Acessado via API pública\n",
    "* Adotada pelos principais veículos jornalísticos internacionais para moderar comentários em seus portais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Desafios para identificar toxicidade em comentários\n",
    "\n",
    "* Geralmente são textos curtos - ex.: “sei… 😏”, “¬¬”, “no way!”\n",
    "* Uso de emojis ambíguos - ex.: 😏,🌚,😋 \n",
    "* Erros de ortografia propositais (ou não) - ex.: “çocorro”;\n",
    "* Gírias da internet - ex.: “vc”, “pq”, “lol”, “iti malia”\n",
    "* Sutilezas inerentes à língua ou localidade - ex.: “oxi”, “p**ra!”;\n",
    "* Especificidades de contexto - ex.: “ex-presidiário”, “bozo”;\n",
    "* Uso de figuras de linguagem - ex.: metáfora, ironia, etc.;\n",
    "* Susceptíveis a ataques adversários - ex.: “st.Up1d”\n",
    "* Podem ocorrer de forma esparsa no conjunto de dados;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Definição de comentário tóxico\n",
    "\n",
    "<center>\"Um comentário rude, desrespeitoso ou irracional que provavelmente fará você sair de uma discussão.\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Funcionamento do Perspective API\n",
    "\n",
    "* Atribui uma pontuação contínua entre 0 e 1 para diferentes categorias de toxicidade de acordo com o % no texto\n",
    "* Uma pontuação mais alta para uma determinada categoria (ou atributo), indica uma maior probabilidade de um leitor perceber que o comentário possui este atributo\n",
    "* e.g. “Você é um idiota” pode receber uma pontuação de $0.8$ para o atributo TOXICIDADE, indicando que 8 entre 10 pessoas perceberiam esse comentário como tóxico\n",
    "* Portanto um comentário com pontuação de TOXICIDADE de $0.9$ não necessariamente é mais tóxico  que uma com $0.7$\n",
    "\n",
    " [Jigsaw 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"figs/perspective.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "Fonte: https://perspectiveapi.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Atributos de produção (multilíngue)\n",
    "\n",
    "- **TOXICITY** - “Um comentário rude, desrespeitoso ou irracional que provavelmente fará com que as pessoas deixem uma discussão”; \n",
    "- **SEVERE_TOXICITY** - “Um comentário que é muito odioso, agressivo, desrespeitoso, ou muito provável de fazer um usuário sair de uma discussão, ou desistir de compartilhar sua perspectiva. Este atributo é muito menos sensível a formas mais leves de toxicidade, como comentários que incluem usos positivos de palavrões”; \n",
    "- **IDENTITY_ATTACK** - “Comentários negativos ou de ódio direcionados a alguém por causa de sua identidade”; \n",
    "- **INSULT** - “Comentário ofensivo, inflamatório ou negativo para uma pessoa ou grupo de pessoas”; \n",
    "- **PROFANITY** - “Xingamentos, palavrões ou outras linguagens obscenas, ou profanas”; \n",
    "- **THREAT** - “Descreve a intenção de infligir dor, lesão ou violência contra um indivíduo, ou grupo”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Acessando o Perspective API\n",
    "\n",
    "* Para executar os exemplos a seguir, você deverá solicitar acesso ao Perspective API no Google Cloud seguindo [esse passo a passo](https://developers.perspectiveapi.com/s/docs-get-started)\n",
    "* Obtenha a chave da API para usar nos próximos passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credenciais do Perspective API\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "PERSPECTIVE_API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemplo de sentenças processadas com o Perspective API\n",
    "EmotionComprehension.perspective(emotional_sentences, PERSPECTIVE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>6. Aplicações</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 6.1 Polarização Política\n",
    "\n",
    "TODO: contextualizar a seção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Análise e pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Carregando os conjuntos de dados de notícias e retweets com links para as notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataframe_from_drive_csv(url):\n",
    "    path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# news dataset\n",
    "url = 'https://drive.google.com/file/d/1AQjdqe9QRFK7ydNteZPM_IJk1eH-H3n_/view?usp=share_link'\n",
    "df_news = load_dataframe_from_drive_csv(url)\n",
    "\n",
    "# retweeted links dataset\n",
    "url = 'https://drive.google.com/file/d/1Nn0I_tZnBWgUTNDeeE4bGzVTBhrTOJo1/view?usp=share_link'\n",
    "df_retweeted_urls = load_dataframe_from_drive_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Quantidade de notícias no conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_news[['retweets_count', 'title', 'description', 'article']].sort_values('retweets_count', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Remoção de notícias duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news = df_news.drop_duplicates(subset=['title', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Combinação do título + descrição + corpo do artigo em uma única string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'] = df_news['title'].astype(str) + ' ' + df_news['description'].astype(str) + ' ' + df_news['article'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Estatísticas sobre o tamanho dos textos\n",
    "\n",
    "* Textos longos (> 4500 caracteres em média)\n",
    "* O menor texto tem 310 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text_length'] = df_news['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de notícias\n",
    "\n",
    "* Escrita formal\n",
    "* Sem erros de ortografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for content in df_news.sample(3)['text'].to_list():\n",
    "    print(content[:500], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Pré-processamento dos textos\n",
    "\n",
    "* Remoção de \n",
    "    * links \n",
    "    * caracteres isolados \n",
    "    * stop-words\n",
    "* Lematização usando Spacy\n",
    "* Transoformação em lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "\n",
    "# Instalação das dependências em inglês para a biblioteca Spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Stopwords em inglês contidas na biblioteca Spacy\n",
    "stop_words = list(spacy_nlp.Defaults.stop_words)\n",
    "\n",
    "# Stopwords em inglês contidas na biblioteca do NLTK\n",
    "stop_words += nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Algumas outras palavras para serem removidas além das stop-words\n",
    "stop_words += [\n",
    "    '-pron-', 'video', 'try', 'refresh', 'continue', 'article', 'load', 'browser', 'say', 'will', \n",
    "    'would', 'content', 'news', 'sign', 'register', 'home', 'page', 'advertisement'\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Remoção de links \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Transforma o texto em um documento Spacy\n",
    "    spacy_doc = spacy_nlp(text)\n",
    "\n",
    "    # Usa o Spacy para lematizar o texto e remover stop words\n",
    "    tokens = [token.lemma_.lower() for token in spacy_doc if token.lemma_.lower() not in stop_words]\n",
    "\n",
    "    # Remove caracteres isolados\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'] = df_news['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de notícias pré-processadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Frequência de termos mais representativos após o pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seta os padrões default para o matplot\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "def get_all_terms(corpus):\n",
    "    terms = []\n",
    "    for text in corpus:\n",
    "        terms = terms + text\n",
    "    return terms\n",
    "\n",
    "\n",
    "def term_frequency(corpus, num_plot=50, num_show=1000):\n",
    "    plt.figure(figsize=(15, 5)) \n",
    "\n",
    "    terms = get_all_terms(corpus)\n",
    "    fdist = nltk.FreqDist(terms)\n",
    "\n",
    "    if num_plot > 0:\n",
    "        fdist.plot(num_plot)\n",
    "\n",
    "    # return a dataframe with terms and frequencies\n",
    "    data = [[term, frequency] for term, frequency in fdist.most_common(num_show)]\n",
    "    return pd.DataFrame(data, columns=['TERM', 'FREQUENCY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_terms = term_frequency(df_news['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Geração de bi e trigramas\n",
    "\n",
    "Usando o Gemsim para formar bi e trigramas caso os termos coocorram pelo menos 10 vezes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "def make_trigrams(corpus, min_count=5, threshold=10):\n",
    "\n",
    "    # Criar os modelos de bi e trigramas\n",
    "\n",
    "    # Obs.: quanto maior o threshold, menos N-gramas são formados\n",
    "    bigram = Phrases(corpus, min_count=min_count, threshold=threshold)\n",
    "    bigram_model = Phraser(bigram)\n",
    "\n",
    "    trigram = Phrases(bigram[corpus], min_count=min_count, threshold=threshold)\n",
    "    trigram_model = Phraser(trigram)\n",
    "\n",
    "    return [trigram_model[bigram_model[text]] for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'] = make_trigrams(df_news['text'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de notícias com bi e trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Termos mais representativos após a identificação de bi e trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_terms = term_frequency(df_news['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Remoção de termos sem valor para o domínio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for term in df_terms['TERM'].to_list():\n",
    "    if len(term) > 15:\n",
    "        print(\"'\" + term + \"',\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### A partir da lista acima, são selecionados manualmente quais termos devem ser removidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unuseful_terms = [\n",
    "    'apologize_fail_tap_team',\n",
    "    'postmedia_network',\n",
    "    'network_latest_national_stories',\n",
    "    'soon_inbox_encounter_issue',\n",
    "    'click_unsubscribe_link_email',\n",
    "    'inc._365_bloor_street',\n",
    "    'ontario_m4w_3l4_416',\n",
    "    'thank_welcome_email_way',\n",
    "    'check_junk_folder_issue',\n",
    "    'story_midday_sun_newsroom',\n",
    "    'inbox_noon_late_headline',\n",
    "    'story_opinion_photo_toronto',\n",
    "    'sun_email_address_error',\n",
    "    'provide_valid_email_address',\n",
    "    'click_button_consent_receive',\n",
    "    'newsletter_postmedia_network_inc.',\n",
    "    'unsubscribe_time',\n",
    "    'original_archive'\n",
    "]\n",
    "\n",
    "def remove_unuseful_terms(text):\n",
    "    return [token for token in text if token.lower() not in unuseful_terms]\n",
    "\n",
    "df_news['text'] = df_news['text'].apply(remove_unuseful_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Identificação de tópicos com LDA\n",
    "\n",
    "Referência: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Criação do dicionário e corpus para o LDA usando o modelo BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Cria o dicionário a partir do corpus\n",
    "gs_dictionary = Dictionary(df_news['text'])\n",
    "\n",
    "# Remove os tokens muito raros (menos frequentes que `no_below`) ou muito comuns (mais frequentes que `no_above`%)\n",
    "gs_dictionary.filter_extremes(no_below=3, no_above=.20)\n",
    "\n",
    "# Cria o corpus usando o modelo de Bag of Words\n",
    "gs_corpus = [gs_dictionary.doc2bow(text) for text in df_news['text'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Dictionary size:', len(gs_dictionary), ', corpus size:', len(gs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Treinamento de modelos LDA \n",
    "\n",
    "* Para treinar os modelos LDA usamos diferentes valores para $k$ \n",
    "* Nesse tutorial, escolhemos $k=10$ até $k=30$, uma vez que temos aprox. 500 documentos no corpus\n",
    "* Para cada modelo geramos o score de coerência e perplexidade, que são analisados a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "\n",
    "def compute_lda_performance(dictionary, corpus, texts, start=1, limit=50, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    perplexity_values : Perplexity values corresponding to the LDA model with respective number of topics\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    num_topics_values = []\n",
    "    perplexity_values = []\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit+step, step):\n",
    "        num_topics_values.append(num_topics)\n",
    "\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, iterations=1000, id2word=dictionary, passes=10, random_state=100) \n",
    "\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence = coherencemodel.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "\n",
    "        perplexity = model.log_perplexity(corpus)\n",
    "        perplexity_values.append(perplexity)\n",
    "\n",
    "        print('Topics:', num_topics, '\\tPerplexity:', round(perplexity, 5), '\\tCoherence:', round(coherence, 5))\n",
    "\n",
    "    df_results = pd.DataFrame({'topics': num_topics_values, 'perplexity': perplexity_values, 'coherence': coherence_values})\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lda_models = compute_lda_performance(dictionary=gs_dictionary, corpus=gs_corpus, texts=df_news['text'], start=10, limit=30, step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Análise do score de coerência\n",
    "\n",
    "* Indica o quão \"interpretável\" são os tópicos para humanos\n",
    "* Indica o quanto as palavras mais representativas de cada tópico são similares entre si\n",
    "* Diferentes medidas de similaridade podem ser usadas, a padrão é o score c_v, que usa similaridade do cosseno\n",
    "* Heurística: quanto maior o score de coerência, melhor (mas nem sempre)\n",
    "    * Usar método do \"cotovelo\"\n",
    "    * Analisar o gráfico de tópicos (a seguir)\n",
    "    * Usar bom senso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scores de coerência de acordo com o número de tópicos\n",
    "ax = df_lda_models.plot.line(x='topics', y='coherence')\n",
    "ax.set_xlabel(\"Num Topics\")\n",
    "ax.set_ylabel(\"Coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top 5 modelos\n",
    "df_lda_models.sort_values('coherence', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Identifica o melhor número de tópicos de acordo com o score de coerência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_num_topics = df_lda_models.sort_values('coherence', ascending=False)['topics'].tolist()[0]\n",
    "\n",
    "best_num_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Treinamento do modelo com o melhor número de tópicos \n",
    "\n",
    "Aqui repetimos o treinamento do modelo com o melhor número de tópicos usando uma quantidade maior de iterações e passes no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=gs_corpus, num_topics=best_num_topics, \n",
    "                   iterations=10000, id2word=gs_dictionary, passes=100, \n",
    "                   random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Coerência e perplexidade do modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity (lower is better)\n",
    "print('\\nPerplexity: ', lda.log_perplexity(gs_corpus)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df_news['text'], \n",
    "                                     dictionary=gs_dictionary, coherence='c_v')\n",
    "print('\\nCoherence Score: ', coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Palavras mais representativas de cada tópico\n",
    "\n",
    "Alguns exemplos de tópicos:\n",
    "\n",
    "* **IMPOSTO DE CARBONO:** carbon_tax, emission, climate_change, tax, cbc, economy, cent, target, cost, program \n",
    "* **ABORTO:** abortion, debate, harper, comment, conservative_party, sex_marriage, law, view, conservative_leader, montreal \n",
    "* **PESQUISA ELEITORAL:** poll, mr._scheer, cent, mr._trudeau, survey, research, 30, age, positive, centre \n",
    "* **CORTE ORÇAMENTÁRIO:** billion, cut, platform, million, city, toronto, tax, bernier, budget, spending \n",
    "* **CASO DE RACISMO:** blackface, racist, apologize, black, apology, rcmp, woman, racism, global, makeup \n",
    "* **PETRÓLEO:** pipeline, oil, climate_change, climate, company, energy, encana, coal, spend, money \n",
    "* **IMIGRAÇÃO:** insurance, claim, refugee, comment, act, facebook, immigrant, immigration, insurance_broker, anti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, topic in enumerate(lda.top_topics(topn=5, texts=df_news['text'])):\n",
    "    terms = topic[0]\n",
    "    print('Topic', i, ', '.join([term[1] for term in terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Visualização dos tópicos\n",
    "\n",
    "* Cada bolha representa um tópico. Quanto maior a bolha, mais prevalente é esse tópico.\n",
    "* Um bom modelo de tópico terá bolhas grandes e não sobrepostas espalhadas por todo o gráfico.\n",
    "* Um modelo com muitos tópicos normalmente terá muitas sobreposições, bolhas de tamanho pequeno agrupadas em uma região do gráfico.\n",
    "\n",
    "Fonte: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def show_lda_vis(lda, gs_corpus, gs_dictionary):\n",
    "    # Workaround para evitar que o pyLDAvis esconda os botões do Jupyterlab\n",
    "    from IPython.display import HTML\n",
    "    css_str = '<style> \\\n",
    "    .jp-icon-warn0 path {fill: var(--jp-warn-color0);} \\\n",
    "    .bp3-button-text path { fill: var(--jp-inverse-layout-color3);} \\\n",
    "    .jp-icon-brand0 path { fill: var(--jp-brand-color0);} \\\n",
    "    text.terms { fill: #616161;} \\\n",
    "    </style>'\n",
    "    display(HTML(css_str))\n",
    "\n",
    "    # feed the LDA model into the pyLDAvis instance\n",
    "    warnings.filterwarnings('ignore')\n",
    "    return gensimvis.prepare(lda, gs_corpus, gs_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_lda_vis(lda, gs_corpus, gs_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Salva os 3 tópicos mais representativos de cada notícia no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_2_words = {}\n",
    "for topic in lda.show_topics(num_topics=100, num_words=10, formatted=False):\n",
    "    topic_id = topic[0]\n",
    "    topic_tokens = ', '.join([token[0] for token in topic[1]])\n",
    "    topic_2_words[topic_id] = topic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_topics_1 = []\n",
    "doc_topics_1_words = []\n",
    "doc_topics_1_percentages = []\n",
    "\n",
    "doc_topics_2 = []\n",
    "doc_topics_2_words = []\n",
    "doc_topics_2_percentages = []\n",
    "\n",
    "doc_topics_3 = []\n",
    "doc_topics_3_words = []\n",
    "doc_topics_3_percentages = []\n",
    "\n",
    "for i, doc in enumerate(df_news['text'].to_list()):\n",
    "    doc_bow = gs_dictionary.doc2bow(doc)\n",
    "    \n",
    "    # get document topics (each row contains a tuple with topic id and topic probability)\n",
    "    doc_topics = lda.get_document_topics(doc_bow)\n",
    "    \n",
    "    # sort topics by probability\n",
    "    doc_topics.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    # get them main topic and top 3 topics\n",
    "    topics = doc_topics[:3]\n",
    "    \n",
    "    if len(topics) > 0:\n",
    "        doc_topics_1_percentages.append(topics[0][1])\n",
    "        topic_id = topics[0][0]\n",
    "        doc_topics_1.append(topic_id)\n",
    "        doc_topics_1_words.append(topic_2_words[topic_id])\n",
    "    else:\n",
    "        doc_topics_1.append(None)\n",
    "        doc_topics_1_percentages.append(None)\n",
    "        doc_topics_1_words.append(None)\n",
    "        \n",
    "        \n",
    "    if len(topics) > 1:\n",
    "        doc_topics_2_percentages.append(topics[1][1])\n",
    "        topic_id = topics[1][0]\n",
    "        doc_topics_2.append(topic_id)\n",
    "        doc_topics_2_words.append(topic_2_words[topic_id])\n",
    "    else:\n",
    "        doc_topics_2.append(None)\n",
    "        doc_topics_2_percentages.append(None)\n",
    "        doc_topics_2_words.append(None)\n",
    "        \n",
    "        \n",
    "    if len(topics) > 2:\n",
    "        doc_topics_3_percentages.append(topics[2][1])\n",
    "        topic_id = topics[2][0]\n",
    "        doc_topics_3.append(topic_id)\n",
    "        doc_topics_3_words.append(topic_2_words[topic_id])\n",
    "    else:\n",
    "        doc_topics_3.append(None)\n",
    "        doc_topics_3_percentages.append(None)\n",
    "        doc_topics_3_words.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['topic'] = pd.Series(doc_topics_1)\n",
    "df_news['topic_words'] = pd.Series(doc_topics_1_words)\n",
    "df_news['topic_percentage'] = pd.Series(doc_topics_1_percentages)\n",
    "\n",
    "df_news['topic_1'] = pd.Series(doc_topics_1)\n",
    "df_news['topic_1_words'] = pd.Series(doc_topics_1_words)\n",
    "df_news['topic_1_percentage'] = pd.Series(doc_topics_1_percentages)\n",
    "\n",
    "df_news['topic_2'] = pd.Series(doc_topics_2)\n",
    "df_news['topic_2_words'] = pd.Series(doc_topics_2_words)\n",
    "df_news['topic_2_percentage'] = pd.Series(doc_topics_2_percentages)\n",
    "\n",
    "df_news['topic_3'] = pd.Series(doc_topics_3)\n",
    "df_news['topic_3_words'] = pd.Series(doc_topics_3_words)\n",
    "df_news['topic_3_percentage'] = pd.Series(doc_topics_3_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Distribuição dos tópicos mais representativos em cada notícia\n",
    "\n",
    "* O tópico mais representativo (em azul) tem alta dominância para a maioria das notícias\n",
    "* O segundo e terceiro tópicos mais representativos, em laranja e verde, tem uma representatividade baixa na maioria das notícias em comparação com o azul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax1 = pd.Series(doc_topics_1_percentages).plot.hist(bins=25)\n",
    "ax2 = pd.Series(doc_topics_2_percentages).plot.hist(bins=25)\n",
    "ax3 = pd.Series(doc_topics_3_percentages).plot.hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Distribuição dos tópicos mais dominantes entre as notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_news.groupby('topic_1').agg({\n",
    "    'article': 'count'\n",
    "}).reset_index().rename(columns={\n",
    "    'topic_1': 'dominant topic id',\n",
    "    'article': 'number of news',\n",
    "}).sort_values('number of news', ascending=False)\n",
    "\n",
    "df['dominant topic id'] = df['dominant topic id'].astype('int')\n",
    "\n",
    "ax = df.plot.bar(x='dominant topic id', y='number of news', figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['number of news'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de manchetes em 5 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    print('\\nTopic', i)\n",
    "    print(df_news[df_news['topic'] == i]['title'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_1(url):\n",
    "    df = df_news[df_news['url'] == url]\n",
    "    return df.iloc[0]['topic'] if df.shape[0] == 1 else None\n",
    "\n",
    "def get_topic_1_words(url):\n",
    "    df = df_news[df_news['url'] == url]\n",
    "    return df.iloc[0]['topic_words'] if df.shape[0] == 1 else None\n",
    "\n",
    "df_retweeted_urls['topic'] = df_retweeted_urls['retweeted_url'].apply(get_topic_1)\n",
    "df_retweeted_urls['topic_words'] = df_retweeted_urls['retweeted_url'].apply(get_topic_1_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Resultados\n",
    "\n",
    "Heatmap de tópicos vs retweets de usuários por faixa de polaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def relative_polarity_heatmap(df, column, oversample=True, title=None, x_label=None, y_label_left=None, y_label_right=None, \n",
    "                              cbar_label=None, top_n=20, vmax=1, numeric_index=False, only_dataframe=False):\n",
    "    \n",
    "    env_polarities = [value/10 for value in range(-10,11,1)]\n",
    "    \n",
    "    df_copy = df.copy()    \n",
    "    \n",
    "    \"\"\"\n",
    "    -------------------\n",
    "    RP(H) calculation\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a matrix with rows being 'column' parameter values and columns being polarities from -1 to +1\n",
    "    df = pd.crosstab(index=df[column], columns=df['user_P(H)_bin'], values=df[column], aggfunc='count')\n",
    "    df = df.fillna(0.0)\n",
    "    \n",
    "    # add faulting columns (for faulting polarities)\n",
    "    for polarity in env_polarities:\n",
    "        if not polarity in df.columns:\n",
    "            num_rows = df.shape[0]\n",
    "            df[polarity] = pd.Series([0.0] * num_rows)\n",
    "            \n",
    "    # reorder columns from -1.0 to +1.0\n",
    "    df = df[env_polarities]\n",
    "    \n",
    "    # scale by dividing the retweets count of each polarity for each domain by the max retweets count of each polarity from all domains\n",
    "    if oversample:\n",
    "        df_polarity_max_retweets = df.max(axis=0) # get polarity column max value\n",
    "        for polarity in env_polarities:\n",
    "            df[polarity] = df[polarity] / df_polarity_max_retweets[polarity]    \n",
    "    \n",
    "    # normalize values to 0-1 interval with min-max (by domain min-max from all polarities)\n",
    "    max_df = df.max(axis=1)\n",
    "    min_df = df.min(axis=1)\n",
    "    for polarity in env_polarities:\n",
    "        df[polarity] = (df[polarity] - min_df) / (max_df - min_df) \n",
    "       \n",
    "    # calculate polarity average without zeros and neutral users count\n",
    "    relative_polarities = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_sum = 0\n",
    "        count = 0\n",
    "        for polarity in env_polarities:\n",
    "            if polarity != 0.0 and row[polarity] > 0.0: # only count cells with non zero value and remove the neutral polarity\n",
    "                row_sum += row[polarity] * polarity\n",
    "                count += row[polarity]\n",
    "        if count > 0:\n",
    "            relative_polarities.append(row_sum / count)\n",
    "        else:\n",
    "            relative_polarities.append(None)\n",
    "        \n",
    "    df['relative_polarity'] = relative_polarities\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    -------------------\n",
    "    Data preparation\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    # count occurrences of 'column' values\n",
    "    df_rphs = df_copy.groupby(column).agg(\n",
    "        retweets_count=pd.NamedAgg(column=column, aggfunc='count')\n",
    "    ).sort_values(by=column, ascending=False)\n",
    "    df_rphs[column] = df.sort_index(ascending=False).index\n",
    "    df_rphs['RP(H)'] = df.sort_index(ascending=False)['relative_polarity']\n",
    "    \n",
    "    \"\"\"\n",
    "    -------------------\n",
    "    Data visualization\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    if not only_dataframe:\n",
    "    \n",
    "        # get only top N most retweeted 'column' values to include in the heatmap\n",
    "        if top_n:        \n",
    "            top_column_values = df_rphs.sort_values(by='retweets_count', ascending=False)[:top_n][column].unique()\n",
    "            df = df[df.index.isin(top_column_values)]\n",
    "\n",
    "        # sort heatmap rows by relative_polarity values\n",
    "        df = df.sort_values('relative_polarity')\n",
    "        relative_polarities = df['relative_polarity'].map('{:,.2f}'.format).astype('str').to_list()\n",
    "        \n",
    "        # drop relative_polarity to not include in the heatmap\n",
    "        df = df.drop(columns=['relative_polarity'])\n",
    "        df = df.fillna(0.0)        \n",
    "            \n",
    "        # get a print friendly datatable\n",
    "        row_indexes = list(range(1,len(df.index)+1))\n",
    "        row_values = df.index        \n",
    "        df_heatmap_table = pd.DataFrame({'ID': row_indexes, y_label_left: row_values, 'RP(H)': relative_polarities})\n",
    "        \n",
    "        # create a sequential numeric 'id' for heatmap rows\n",
    "        if numeric_index:            \n",
    "            df['id'] = row_indexes\n",
    "            df = df.set_index('id')\n",
    "\n",
    "        plt.subplots(figsize=(2,round(top_n/3.5)))\n",
    "        ax = sns.heatmap(df, annot=False, linewidths=.1, robust=True, cmap='YlOrBr', vmin=0, vmax=vmax, cbar=False, square=True)\n",
    "        ax.set_title(title or '')\n",
    "        ax.set_xlabel(x_label or 'User P(H)')\n",
    "        ax.set_ylabel(y_label_left or '')\n",
    "        #ax.collections[0].colorbar.set_label(cbar_label or 'Retweet density')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation = 0)\n",
    "\n",
    "        # maintain only 5 tick labels to simplify\n",
    "        for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if n not in [0, 5, 10, 15, 20]:\n",
    "                label.set_visible(False)\n",
    "\n",
    "        # add right y axis\n",
    "        ax2 = ax.twinx() # share x-axis\n",
    "        ax2.set_ylabel(y_label_right or '')\n",
    "        ax2.tick_params(right=True, pad=6)\n",
    "        ax2.set_aspect('auto', share=True, )\n",
    "        ax2.set_ylim((top_n, 0))\n",
    "        ax2.set_yticks(ax.get_yticks())\n",
    "        ax2.set_yticklabels(relative_polarities)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "        ax2.spines['bottom'].set_visible(False)\n",
    "        ax2.spines['left'].set_visible(False)\n",
    "\n",
    "        fig = ax.get_figure()\n",
    "        fig.set_size_inches(2, round(top_n/3.5))\n",
    "\n",
    "    return df_rphs, df_heatmap_table, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "df, df_heatmap, fig = relative_polarity_heatmap(\n",
    "    df=df_retweeted_urls, column='topic_words', y_label_left='Tópico', \n",
    "    y_label_right='Polaridade do tópico', x_label='Polaridade dos usuários',\n",
    "    top_n=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "0c9dfea1575e8b44ff0615653062e3f77db49c736187dedf2087539b29eac2fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
