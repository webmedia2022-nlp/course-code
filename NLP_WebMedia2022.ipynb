{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <center>Processamento de Linguagem Natural em Textos de Mídias Sociais: Fundamentos, Ferramentas e Aplicações</center>\n",
    "\n",
    "### <center>XXVIII Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia 2022)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Frances A. Santos (UNICAMP), Jordan Kobellarz (UTFPR), Fábio R. de Souza (USP), Leandro A. Villas (UNICAMP), Thiago H. Silva (UTFPR)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Curitiba, PR</center>\n",
    "<center>07 de Novembro de 2022</center>\n",
    "\n",
    "| <a href=\"https://colab.research.google.com/github/webmedia2022-nlp/course-code/blob/main/NLP_WebMedia2022.ipynb\" target=\"_parent\"><img style=\"float: center;\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir no Colab\"/></a> | <img style=\"float: center;\" src=\"https://img.shields.io/badge/python-v3.8.13-blue\" alt=\"Python Version\"/> |\n",
    "| --- | --- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm #Instalando dependências específicas do spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import getpass\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação do diretório \"data/\"\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from DataExtraction import DataExtraction\n",
    "from Preprocessing import Preprocessing\n",
    "from TextRepresentation import StatisticalModels, SentenceEmbeddings, WordEmbeddings\n",
    "from KnowledgeExtraction import Clustering, SemanticComprehension, SentimentAnalysis\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download de arquivos usados por bibliotecas\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Agenda\n",
    "\n",
    "<img src=\"figs/agenda.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>Introdução</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Por que mídias sociais?\n",
    "\n",
    "Mídias sociais são acessadas por aproximadamente 4,7 bilhões de usuários em todo o planeta (i.e., 59% da população) [Kemp 2022]\n",
    "\n",
    "<img src=\"figs/social-media.jpeg\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo (Twitter):\n",
    "\n",
    "* 200 Bilhões de postagens por ano\n",
    "* equivalente a 6 mil postagens por segundo\n",
    "\n",
    "Fonte: [Twitter Usage Statistics](https://www.internetlivestats.com/twitter-statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Possibilidades para acessar dados públicos em larga escala\n",
    "\n",
    "* **Twitter** - API (stream e histórico)\n",
    "* **Reddit** - API (stream e histórico)\n",
    "* **Meta (Instagram e Facebook)** - Plataforma CrowdTangle\n",
    "* **Swarm (Forsquare)** - API\n",
    "* **Flickr** - API\n",
    "* **Google Places** - API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Valiosa fonte de dados para diversas aplicações\n",
    "\n",
    "* **Na Academia**\n",
    "    * Análise de fenômenos sociais\n",
    "    * Sensoriamento social\n",
    "    * Detecção de notícias falsas\n",
    "    * Discurso de ódio\n",
    "    * Polarização política\n",
    "\n",
    "* **Na Indústria**\n",
    "    * Benchmarking (comparação com concorrentes)\n",
    "    * Forecasting (análise de tendências)\n",
    "    * Sistemas de recomendação\n",
    "    * Personalização / customização em larga escala\n",
    "    * Análise de risco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Por que textos?\n",
    "\n",
    "Textos escritos em linguagem natural, <br>estão presentes na maioria dos dados disponíveis de mídias sociais\n",
    "\n",
    "<img src=\"figs/social-media-content.png\" style=\"float: right; zoom:80%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>1. Textos de mídias sociais <br>Suas principais características e como coletá-los</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Onde encontrar dados textuais?\n",
    "\n",
    "* Postagens\n",
    "* Artigos\n",
    "* Mensagens / comentários\n",
    "* Metainformações de páginas, imagens, videos, perfis, postagens, mensagens, etc. \n",
    "* Extração de texto em imagem, áudio e video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mídias sociais consideradas\n",
    "\n",
    "\n",
    "| <img src=\"figs/twitter.png\" style=\"float: center; zoom:20%;\" /> | <img src=\"figs/reddit.png\" style=\"float: top center; zoom:50%;\" /> | <img src=\"figs/facebook.png\" style=\"float: center; zoom:40%;\" /> |\n",
    "| --- | --- | --- |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Twitter\n",
    "\n",
    "* Mídia social de *Microblogging*\n",
    "* Mensagens limitadas a 280 caracteres\n",
    "* Uma das primeiras redes a disponibilizar uma API para extração de dados públicos em larga escala\n",
    "* Possibilidade de coleta de dados históricos ou em tempo real (*streaming*)\n",
    "* Qualquer dado público pode ser acessado, exceto os de perfis privados (menos de 10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Característica proeminente\n",
    "\n",
    "Simplicidade nas interações e dicionário de dados:\n",
    "\n",
    "* tweets\n",
    "* hashtags #\n",
    "* menções @\n",
    "* retweets RT\n",
    "* respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "* **texto do tweet**\n",
    "* **timestamp**\n",
    "* **autor**\n",
    "    * nome\n",
    "    * localização\n",
    "    * se é verificado\n",
    "    * quantidade de seguidores, amigos, postagens\n",
    "    * data de criação da conta\n",
    "    * língua do perfil\n",
    "    * etc.\n",
    "* **geolocalização do tweet (GeoJson)**\n",
    "    * adicionada explícitamente\n",
    "    * ou capturada do dispositivo que gerou o tweet\n",
    "* **entidades**\n",
    "    * hashtags\n",
    "    * links\n",
    "    * menções\n",
    "    * mídias\n",
    "* **sinais sociais**\n",
    "    * quantidade de retweets\n",
    "    * quantidade de curtidas\n",
    "    * quantidade de respostas\n",
    "* etc. \n",
    "\n",
    "Conheça o [dicionário completo de dados de um tweet aqui](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de um tweet\n",
    "\n",
    "Alguns campos foram omitidos para facilitar a visualização. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"created_at\": \"Thu Apr 06 15:24:15 +0000 2017\",\n",
    "  \"id_str\": \"850006245121695744\",\n",
    "  \"text\": \"1\\/ Today we\\u2019re sharing our vision for the future of the Twitter API platform!\\nhttps:\\/\\/t.co\\/XweGngmxlP\",\n",
    "  \"user\": {\n",
    "    \"id\": 2244994945,\n",
    "    \"name\": \"Twitter Dev\",\n",
    "    \"screen_name\": \"TwitterDev\",\n",
    "    \"location\": \"Internet\",\n",
    "    \"url\": \"https:\\/\\/dev.twitter.com\\/\",\n",
    "    \"description\": \"Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\\/\\/twittercommunity.com\\/ \\u2328\\ufe0f #TapIntoTwitter\"\n",
    "  },\n",
    "  \"place\": {   \n",
    "  },\n",
    "  \"entities\": {\n",
    "    \"hashtags\": [      \n",
    "    ],\n",
    "    \"urls\": [\n",
    "    ],\n",
    "    \"user_mentions\": [     \n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Limite de 280 caracteres\n",
    "    * restringe capacidade argumentativa\n",
    "    * usuários contornam com uso de contrações de palavras, gírias da internet e emojis\n",
    "* Representatividade da população\n",
    "    * pode não representar bem o usuário médio de Internet\n",
    "    * [tendem a ser usadas por pessoas mais jovens, com maior renda e grau de escolaridade](https://blogs.oii.ox.ac.uk/policy/did-you-consider-twitters-lack-of-representativeness-before-doing-that-predictive-study/)\n",
    "* Representatividade do retorno da API\n",
    "    * a API de streaming se baseia na [**relevância** e não **completude** dos dados](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview)\n",
    "* Alta incidência de contas robô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Tweets\n",
    "\n",
    "- Utilizaremos a Twitter API v2 para coletar os tweets \n",
    "- Acesse a [página de desenvolverdores](https://developer.twitter.com/) e obtenha suas credenciais de acesso\n",
    "- Limitamos cada coleta a **10 tweets**, mas todo o conteúdo dos tweets é adicionado (*appending*) ao arquivo local data/tweets.json\n",
    "- Além dos campos *id* e *text* que estão presentes nos tweets por padrão, também solicitamos os campos *created_at, entities, geo, lang, public_metrics, source*. Para ver a lista completa de campos possíveis, acesse esta [página](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)\n",
    "- Filtramos para selecionar apenas os tweets escritos em Inglês ( *lang = \"en\"* ) e que contenham o termo \"nyc\", que referencia a cidade de Nova Iorque\n",
    "- Após coletar os tweets, extraímos os valores dos campos *text, timestamp_ms, ...* e retornamos no formato Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do Twitter\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "twitter_consumer_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'API KEY SECRET'\")\n",
    "twitter_consumer_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN KEY'\")\n",
    "twitter_access_token_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN SECRET'\")\n",
    "twitter_access_token_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'Bearer TOKEN'\")\n",
    "twitter_bearer_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Coleta de tweets\n",
    "try:\n",
    "    df_tweets = DataExtraction().twitter(\n",
    "        twitter_consumer_key, \n",
    "        twitter_consumer_secret, \n",
    "        twitter_access_token_key, \n",
    "        twitter_access_token_secret, \n",
    "        twitter_bearer_token\n",
    "    )\n",
    "except:\n",
    "    ## Caso aconteça algum erro durante a coleta, é carregado o arquivo existente\n",
    "    with open(\"data/tweets.json\", \"r\") as tweet_file:\n",
    "        tweets = json.load(tweet_file)\n",
    "        data = [{\n",
    "                \"created_at\": item[\"created_at\"],\n",
    "                \"url\": \"https://twitter.com/anyuser/status/\" + item[\"id\"],\n",
    "                \"score\": item[\"public_metrics\"][\"like_count\"],\n",
    "                \"text\": item[\"text\"],\n",
    "                \"length\":len(item[\"text\"]),\n",
    "                \"geo\": item[\"geo\"]\n",
    "                } for item in tweets]\n",
    "        df_tweets = pd.DataFrame(data)\n",
    "    \n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Reddit\n",
    "\n",
    "* Mídia social baseada em fóruns de discussão\n",
    "* Comunidades/fóruns → \\subreddits\n",
    "* Mais de 100K comunidades e 50 mi de usuários ativos diariamente em [2020](https://www.redditinc.com/advertising/audience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* Sistema de moderação autoorganizável\n",
    "    * \\subreddits possuem regras próprias criadas pelos moderadores e membros\n",
    "    * algumas comunidades possuem alto nível de comprometimento com as regras propostas\n",
    "    * mecanismos de recompensa para colaboradores ativos\n",
    "* Possibilidade de coletar dados em stream e histórico\n",
    "    * vantagem de permitir a recuperação do histórico completo\n",
    "* Permite acesso à qualquer informação disponível publicamente\n",
    "    * inclui postagens, comentários, perfis, comunidades e suas respectivas metainformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dados que podem ser obtidos via API\n",
    "\n",
    "**[submission (postagem)](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html)**\n",
    "* id\n",
    "* url\n",
    "* permalink\n",
    "* created_utc\n",
    "* title\n",
    "* selftext (conteúdo da postagem)\n",
    "* score (número de upvotes)\n",
    "* [author](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (Redditor)\n",
    "    * name\n",
    "    * created_utc\n",
    "    * comment_karma (pontuação do usuário)\n",
    "    * has_verified_email\n",
    "    * etc.\n",
    "* [comments](https://praw.readthedocs.io/en/stable/code_overview/models/comment.html) (árvore de comentários -- necessário percorrer com método específico para isso)\n",
    "    * author (Redditor)\n",
    "    * body\n",
    "    * distinguished\n",
    "    * etc.\n",
    "* distinguished (se a postagem foi destacada pelo moderador)\n",
    "* edited (se a postagem foi editada)\n",
    "* is_original_content (se foi marcada automaticamente como conteúdo original)\n",
    "* over_18 (se é conteúdo para maiores de 18 anos)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* a plataforma permite um alto grau de anonimidade\n",
    "    * é encorajado o uso de pseudônimo\n",
    "    * é possível fazer cadastro sem verificação\n",
    "    * abertura para comportamentos anti-éticos em comunidades não moderadas / permissivas\n",
    "* cada comunidade possui regras próprias\n",
    "    * práticas de moderação distintas\n",
    "    * dificultando a comparação \n",
    "* liberdade no formato\n",
    "    * campo aberto com possibiidade de uso de html e markdown\n",
    "* alta incidência de bots\n",
    "    * criam, fazem a curadoria e moderam conteúdos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts de \\subreddits\n",
    "\n",
    "- Utilizaremos a [API do Reddit](https://www.reddit.com/dev/api) para coletar posts\n",
    "- Você deverá criar uma conta para acessar a API em [reddit.com](https://reddit.com)\n",
    "- Depois de criar a conta, obtenha os Client ID e o Client Secret\n",
    "- No exemplo a seguir, coletamos os top 100 posts de 5 subreddits, contendo o texto, url, número de comentários, data de criação e score (número de upvotes do post)\n",
    "- Os dados são salvos no arquivo local data/reddit_posts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do Reddit\n",
    "\n",
    "print(\"Informe seu 'CLIENT ID'\")\n",
    "REDDIT_CLIENT_ID = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'CLIENT SECRET'\")\n",
    "REDDIT_CLIENT_SECRET = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Subreddits com discussões sérias sobre assuntos como política, história e ciência.\n",
    "subreddits = [\n",
    "    'politics',\n",
    "    'AskHistorians',\n",
    "    'changemyview',\n",
    "    'COVID19',\n",
    "    'EverythingScience',\n",
    "    'science'\n",
    "]\n",
    "\n",
    "# Coleta os top 100 posts de cada Subreddit\n",
    "df_reddit_posts = DataExtraction().reddit(\n",
    "    REDDIT_CLIENT_ID,\n",
    "    REDDIT_CLIENT_SECRET,\n",
    "    subreddits=subreddits,\n",
    "    top_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Apresenta alguns posts com texto\n",
    "df_reddit_posts[df_reddit_posts['length'] > 0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Facebook (Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Características proeminentes\n",
    "\n",
    "* Alto grau de controle de privacidade\n",
    "* O anonimato é desencorajado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Formas de obter dados\n",
    "\n",
    "* Via API nativa (limitada)\n",
    "* Via web scraping (desencorajado)\n",
    "* Via polls com usuários (não escalável)\n",
    "* Via programa [Social Science One](https://socialscience.one/grant-process) (acesso direto à base do Facebook | difícil acesso | apenas para uso acadêmico)\n",
    "* Via plataforma do [CrowdTangle](https://crowdtangle.com) (dados limitados a páginas e grupos famosos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### [CrowdTangle](https://crowdtangle.com)\n",
    "\n",
    "* Iniciativa da Meta criada para jornalistas, agências de checagem de fatos, profissionais de marketing e pesquisadores\n",
    "* Possibilidade de consultar e visualizar dados em tempo real pela interface (dashboards)\n",
    "* Mesmos dados apresentados na interface podem ser obtidos via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Informações que **podem** ser coletadas:\n",
    "     * quando algo foi postado\n",
    "     * tipo do post (video, imagem, texto)\n",
    "     * página, conta ou grupo onde o conteúdo foi postado\n",
    "     * quantidade de interações (likes, reações, comentários, compartilhamentos, visualizações de videos)\n",
    "     * páginas públicas ou contas que compartilharam o conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Informações que **não podem** ser coletadas:\n",
    "    * alcance ou impressões de um post\n",
    "    * conteúdos efêmeros, como stories, por exemplo\n",
    "    * informações demográficas de usuários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* A base de dados disponível se limita a:\n",
    "    * contas famosas (aprox. ~7 mi de páginas, grupos ou perfis verificados em 08/06/2021), incluindo:\n",
    "        * páginas públicas com mais de 50K curtidas\n",
    "        * grupos públicos com mais de 95K membros\n",
    "        * grupos públicos dos Estados Unidos com mais de 2K membros\n",
    "        * todos os perfis verificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplo de retorno da API do CrowdTangle\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"status\": 200,\n",
    "    \"result\": {\n",
    "        \"posts\": [\n",
    "            {\n",
    "                \"platformId\": \"47657117525_10154014482272526\",\n",
    "                \"platform\": \"Facebook\",\n",
    "                \"date\": \"2016-02-12 23:38:14\",\n",
    "                \"updated\": \"2020-08-23 05:48:22\",\n",
    "                \"type\": \"live_video_complete\",\n",
    "                \"message\": \"Draymond at Foot Locker for #NBAAllStarTO with a special shoutout to #DubNation.\",\n",
    "                \"expandedLinks\": [\n",
    "                    {\n",
    "                        \"original\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                        \"expanded\": \"https://www.facebook.com/warriors/videos/10154014482272526/\"\n",
    "                    }\n",
    "                ],\n",
    "                \"link\": \"https://www.facebook.com/warriors/videos/10154014482272526/\",\n",
    "                \"postUrl\": \"https://www.facebook.com/warriors/posts/10154014482272526\",\n",
    "                \"subscriberCount\": 6041837,\n",
    "                \"score\": 4.750579867017164,\n",
    "                \"media\": [\n",
    "                    {\n",
    "                        \"type\": \"video\",\n",
    "                        \"url\": \"https://video-sea1-1.xx.fbcdn.net/v/t42.1790-29/12718926_1213464465334694_1083747983_n.mp4?_nc_cat=109&_nc_sid=985c63&efg=eyJybHIiOjQ0MiwicmxhIjoxNDIwLCJ2ZW5jb2RlX3RhZyI6InYyXzQwMF9jcmZfMjdfbWFpbl8zLjBfc2QifQ%3D%3D&_nc_ohc=e7Ygz2qv-24AX-wSWX2&rl=442&vabr=246&_nc_ht=video-sea1-1.xx&oh=889e0d776d92a84bb57099cad3d28d55&oe=5F43C879\",\n",
    "                        \"height\": 0,\n",
    "                        \"width\": 0\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"photo\",\n",
    "                        \"url\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\",\n",
    "                        \"height\": 400,\n",
    "                        \"width\": 400,\n",
    "                        \"full\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t15.5256-10/12526285_831341603658336_1493677499_n.jpg?_nc_cat=101&_nc_sid=1055be&_nc_ohc=DH0vfblGwtIAX_x8SBs&_nc_ht=scontent-sea1-1.xx&oh=b09d6378fa261fd45345e79c50c254cb&oe=5F696BE1\"\n",
    "                    }\n",
    "                ],\n",
    "                \"statistics\": {\n",
    "                    \"actual\": {\n",
    "                        \"likeCount\": 24235,\n",
    "                        \"shareCount\": 753,\n",
    "                        \"commentCount\": 5675,\n",
    "                        \"loveCount\": 33,\n",
    "                        \"wowCount\": 18,\n",
    "                        \"hahaCount\": 3,\n",
    "                        \"sadCount\": 0,\n",
    "                        \"angryCount\": 5,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 0\n",
    "                    },\n",
    "                    \"expected\": {\n",
    "                        \"likeCount\": 3927,\n",
    "                        \"shareCount\": 279,\n",
    "                        \"commentCount\": 1041,\n",
    "                        \"loveCount\": 1046,\n",
    "                        \"wowCount\": 94,\n",
    "                        \"hahaCount\": 45,\n",
    "                        \"sadCount\": 14,\n",
    "                        \"angryCount\": 19,\n",
    "                        \"thankfulCount\": 0,\n",
    "                        \"careCount\": 2\n",
    "                    }\n",
    "                },\n",
    "                \"account\": {\n",
    "                    \"id\": 19889,\n",
    "                    \"name\": \"Golden State Warriors\",\n",
    "                    \"handle\": \"warriors\",\n",
    "                    \"profileImage\": \"https://scontent-sea1-1.xx.fbcdn.net/v/t1.0-1/p200x200/74788912_10158146665972526_3545220405897723904_n.jpg?_nc_cat=1&ccb=2&_nc_sid=dbb9e7&_nc_ohc=9snUpG_pdlQAX90IhWM&_nc_ht=scontent-sea1-1.xx&tp=6&oh=f8a3d3b62b507966ecc68de3b557fe84&oe=5FBF1185\",\n",
    "                    \"subscriberCount\": 11580228,\n",
    "                    \"url\": \"https://www.facebook.com/47657117525\",\n",
    "                    \"platform\": \"Facebook\",\n",
    "                    \"platformId\": \"47657117525\",\n",
    "                    \"accountType\": \"facebook_page\",\n",
    "                    \"pageAdminTopCountry\": \"US\",\n",
    "                    \"verified\": true\n",
    "                },\n",
    "                \"videoLengthMS\": 307968,\n",
    "                \"liveVideoStatus\": \"completed\",\n",
    "                \"Id\": \"19889|10154014482272526\",\n",
    "                \"legacyid\": 1686762829\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Limitações e desafios\n",
    "\n",
    "* Dados limitados a contas famosas\n",
    "    * contas menos famosas são subrepresentadas\n",
    "* Não é possível saber quem reagiu ou comentou em posts\n",
    "* Ferramenta muito nova / pouco explorada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coletando Posts no CrowdTangle\n",
    "\n",
    "* Utilizaremos a API do CrowdTangle para extrair posts do Facebook -- [documentação](https://github.com/CrowdTangle/API/wiki)\n",
    "* O primeiro passo é criar uma conta no CrowdTangle, depois criar um dashboard e obter o token da API para acessar os dados do dashboard\n",
    "* Para coletar posts via API, é necessário criar pelo menos uma lista em seu dashboard recém criado\n",
    "* Em nosso caso, criaremos uma lista para monitorar posts de páginas de mídias de notícias, incluindo CNN, NYT, BBC, NBC, NPR, Reuters, etc. \n",
    "* [Esse video explica como usar a interface do CrowdTangle para criar listas](https://vimeo.com/588999918). \n",
    "* [Esse video explica como acessar os dados via API](https://vimeo.com/453763307) explicando como executar todos os procedimentos acima\n",
    "\n",
    "> Observação: não é possível criar uma lista via API, somente pela interface do dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Credenciais da API do CrowdTangle\n",
    "\n",
    "print(\"Informe seu 'API_TOKEN'\")\n",
    "CROWDTANGLE_API_TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui coletamos os top 100 posts em cada mês, iniciando em start_date e terminando em end_date\n",
    "df_facebook_posts = DataExtraction().facebook(\n",
    "    CROWDTANGLE_API_TOKEN, \n",
    "    search_term='covid-19',\n",
    "    start_date = '2020-04-01',\n",
    "    end_date = '2021-04-01'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Amostra de posts do Facebook\n",
    "df_facebook_posts.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>2. Pré-processamento Textual</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, passamos pela série de tarefas necessárias para posterior aplicação de modelos de Aprendizado de Máquina.\n",
    "\n",
    "Mais especificamente, passaremos pelas seguintes etapas:\n",
    "\n",
    "- Normalização\n",
    "- Tokenização\n",
    "- Lemmatização\n",
    "- Stemmização\n",
    "- POS Tagging \n",
    "- Padrões Regex para Limpeza de Texto \n",
    "\n",
    "Para isso, apresentaremos duas versões: a biblioteca [NLTK](https://www.nltk.org/howto/portuguese_en.html), e o Spacy(https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização**\n",
    "\n",
    "Em primeiro lugar, realizamos a normalização de texto, para que consigamos computar a quantidade de palavras independente de terem sido escritas em letras maiúsculas ou minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Preprocessing()\n",
    "\n",
    "data = df_tweets[['text']].copy()\n",
    "data[\"normalized_text\"] = data['text'].apply(pipeline.normalization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex (Expressoes regulares)\n",
    "\n",
    "As Expressões Regulares (do inglês, Regular Expressions, cujos acrônimos são RE ou RegEx), são comandos que especificam padrões de busca em texto.\n",
    "\n",
    "São muito versáteis para limpeza de texto, uma vez que é permitido especificar através delas exatamente os padrões que são importantes para o nosso contexto.\n",
    "\n",
    "Alguns sites são muito úteis como referência para montar e testar padrões Regex, como o [Regex101](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "re_links = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_links, value='LINK')\n",
    "data['clean_text'] = data['normalized_text'].apply(apply_regex)\n",
    "\n",
    "re_mentions = r'@([A-Za-z0-9_]+)'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_mentions, value='USERNAME')\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "re_newline = '\\\\n'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_newline)\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "re_special_char = '\\W+'\n",
    "apply_regex = lambda x: pipeline.clean_regex(x, re_special_char)\n",
    "data['clean_text'] = data['clean_text'].apply(apply_regex)\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização\n",
    "\n",
    "A etapa de tokenização consiste em transformar um texto em uma lista de palavras e símbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['clean_text'].apply(pipeline.tokenization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging**\n",
    "\n",
    "A extração de POS Tags (Part of Speech, ou Partes-da-Fala), consiste em identificar o papel de cada termo dentro da estrutura sintática de uma frase.\n",
    "\n",
    "Alguns exemplos de POS Tag são: \n",
    "\n",
    "- ADJ: Adjetivo\n",
    "- ADP: Preposição (do inglês Adposition)\n",
    "- ADV: Advérbio\n",
    "- NOUN: Substantivo\n",
    "- VERB: Verbo\n",
    "- PROPN: Nomes próprios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos_tags'] = data['tokens'].apply(pipeline.pos_tagging)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemmização**\n",
    "\n",
    "A stemmização consiste em extrair o afixo de uma palavra. Isso permite que se reduza a variabilidade do corpus, ao agruparmos todas as referências a uma palavra única independente das variações de gênero, número e grau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stems'] = data['tokens'].apply(pipeline.stemming)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatização**\n",
    "\n",
    "A lemmatização consiste em uma tarefa cujo objetivo é o mesmo da Stemmização, com a diferença de que, ao invés de reduzir as palavras ao seu radical (se tornando, às vezes, ilegível), as palavras neste caso são reduzidas a sua inflexão mínima (*estamos, estaremos e estava*, para *está*, por exemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmas'] = data['tokens'].apply(pipeline.lemmatization)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outra Alternativa: Pipeline de NLP do Spacy\n",
    "\n",
    "O Spacy permite que todas as etapas realizadas de pré-processamento sejam realizadas em uma única chamada, num processo conhecido como *pipeline* de NLP. \n",
    "\n",
    "A diferença principal em relação ao NLTK está no fato de que, internamente, são utilizados modelos de Machine Learning, e modelos adicionais, como Entidades Nomeadas, são também oferecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo tudo de uma só vez com Spacy\n",
    "\n",
    "data['tokens'], data['pos_tags'], data['lemmas'] = zip(*data['clean_text'].apply(pipeline.nlp_pipeline))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>3. Representação de Textos <br> Utilizando Vetores Numéricos</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "- É a forma mais simples de representação de palavras para um algoritmo de aprendizado de máquina\n",
    "- Cada documento é representado por um vetor de tamanho N, onde N é a quantidade de tokens distintos no vocabulário\n",
    "- Cada token único é representado por uma posição no vetor, a ser preenchida com a quantidade de vezes que o token ocorre no documento\n",
    "- Caso não haja nenhuma ocorrência de um token no documento, sua respectiva posição recebe o valor 0\n",
    "\n",
    "<center> Tabela: Exemplo de representação utilizando BoW. Adaptade de Machine Learning Mastery<sup>1</sup> </center>\n",
    "\n",
    "| Documento | it | was | the | best | of| times | worst | age | wisdom | foolishness |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| it was the best of times | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "| it was the worst of times | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| it was the age of wisdom | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 |\n",
    "| it was the age of foolishness | 1 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">1</a>: Disponível em https://machinelearningmastery.com/gentle-introduction-bag-words-model/. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - facilidade de implementação\n",
    "    - ajuda a identificar as palavras significativas de um texto, baseado em sua frequência\n",
    "\n",
    "- **Desvantagens**\n",
    "    - grande consumo de memória para vocabulários muito extensos\n",
    "    - representações esparsas e apresentando enviesamento em relação a termos muito frequentes\n",
    "    - não considera a posição das palavras no texto:\n",
    "    \n",
    "    \n",
    "| Documento | I | went | to | the | cinema | and | liked | movie | but | not | popcorn\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| I went to the cinema and liked the movie but not the popcorn | 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "| I went to the cinema and liked the popcorn but not the movie| 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stats_models = StatisticalModels()\n",
    "stats_models.bow(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "- Dado um conjunto de documentos textuais, **palavras que ocorrem em muitos destes documentos provavelmente não serão relevantes para distinguir o conteúdo de cada um deles** \\[Robertson 2004\\]\n",
    "- Para superar essa limitação (que ocorre com o BoW), o TF-IDF primeiro calcula a frequência com que um determinado termo ocorra em um documento (i.e., TF)\n",
    "\n",
    "- E, então, é realizada a ponderação com a frequência com que o mesmo termo ocorra em um conjunto de documentos (i.e., IDF):\n",
    "\n",
    "<img src=\"figs/idf.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "onde $N$ corresponde ao número de documentos analisados e $df_i$ corresponde ao número de documentos em que ocorre o termo em questão.\n",
    "\n",
    "Assim, podemos representar cada elemento de uma matriz termo × documento por:\n",
    "\n",
    "<img src=\"figs/tfidf.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - o TF-IDF representou um passo importante para o desenvolvimento de técnicas relacionadas à extração de informação\n",
    "    - Tendo depois se expandido para outras técnicas de NLP, como extração de tópicos e classificação de texto\n",
    "\n",
    "- **Desvantagens**\n",
    "    - representação vetorial resultante ter o mesmo tamanho do vocabulário do texto (grande consumo de memória)\n",
    "    - não considera as proximidades semânticas em que os termos ocorrem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stats_models.tfidf(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**\n",
    "\n",
    "Como vimos, o TF-IDF, apesar de muito versátil, tem como principal limitação o tamanho dos vetores gerados, que correspondem na quantidade de documentos (ou sentenças) analisadas.\n",
    "\n",
    "Para resolver este problema, uma técnica que pode ser utilizada é o PCA (Principal Component Analysis), cuja função é reduzir a dimensionalidade dos vetores mantendo a informação codificada por ele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](http://www.nlpca.org/fig_pca_principal_component_analysis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = stats_models.bow(data[\"clean_text\"])\n",
    "\n",
    "pca_model, pca_transformation = stats_models.pca(tf_idf[1].toarray(), n_components=5)\n",
    "\n",
    "pca_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings\n",
    "\n",
    "- Com o Word2Vec, proposto por \\[Mikolov et al. 2013\\] inaugurou-se um novo paradigma de representação semântica vetorial: Word Embeddings\n",
    "- Principal característica é a atribuição de um vetor denso, de tamanho arbitrário, a cada palavra de um corpus, gerado a partir do treinamento de redes neurais\n",
    "- Esses vetores são gerados a partir da análise das **janelas semânticas** em que tais palavras venham a ocorrer\n",
    "- Resultou em uma série de inovações e vantagens:\n",
    "    - os vetores não precisam ser treinados apenas no corpus em que será feita a análise. Normalmente, são utilizados vetores pré-treinados sobre um corpus com vocabulário mais extenso, como a Wikipedia, e apenas são otimizados sobre o corpus avaliado\n",
    "    - É eliminado o problema da esparsidade de dados\n",
    "    - Resultados experimentais indicam que relações semânticas complexas podem ser capturadas, por exemplo: \n",
    "        - a relação entre os nomes de países e suas respectivas capitais\n",
    "        - sinônimos obtém representações mais próximas, enquanto antônimos se distanciam de forma equivalente no espaço vetorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "O Word2Vec é composto por dois modelos:\n",
    "- **Continuous Bag of Words (CBOW):** toma-se como entrada uma janela de palavras, e tenta-se prever qual palavra ocorreria naquele contexto\n",
    "- **Skip-Gram:** toma-se como entrada uma palavra, e a partir dela, tenta-se prever as palavras que venham a ocorrer em sua vizinhança\n",
    "\n",
    "Veja a arquitetura de ambos os modelos na ilustração a seguir:\n",
    "\n",
    "<img src=\"figs/word2vec.png\" style=\"float: center; zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Treinamento do modelo Word2Vec Skip-gram\n",
    "\n",
    " - toma como entrada um corpus de texto, com um tamanho $N$ de vocabulário\n",
    " - Inicialmente, são atribuídos valores aleatórios para cada um dos vetores das palavras do vocabulário\n",
    " - Os pesos são ajustados ao longo do treinamento, para que:\n",
    "     - palavras ocorrendo em contextos semelhantes obtenham representações vetoriais (embeddings) próximos\n",
    "     - e, palavras de significado distante, que não costumam ocorrer nos mesmos contextos, obtenham representações o mais distante possíveis entre si\n",
    " - Em cada palavra analisada por iteração, as instâncias positivas são palavras que realmente ocorrem em sua proximidade, e as negativas, uma seleção, de tamanho proporcional, de palavras que não ocorrem\n",
    " - O treinamento do algoritmo irá, então, minimizar a função de perda (Loss Function), cujo objetivo é maximizar o produto escalar entre uma palavra e um exemplo positivo de contexto, e minimizar em relação aos exemplos negativos\n",
    " \n",
    "- O tamanho do contexto observado é arbitrário, sendo definido como um parâmetro de treinamento\n",
    "\n",
    "- Ao final, duas representações são aprendidas:\n",
    "    - uma matriz $W$ contendo em cada vetor $w_i$ um word embedding para cada palavra do vocabulário\n",
    "    - e, uma matriz $C$ em que cada vetor $c_i$ é um embedding relativo ao contexto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemplo Word2Vec Skip-gram\n",
    "\n",
    "Adaptado de [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "- Arquitetura do modelo:\n",
    "\n",
    "<img src=\"figs/skipgramarch.png\" style=\"float: center; zoom:80%;\" />\n",
    "\n",
    "- Word embeddins de uma determinada palavra:\n",
    "\n",
    "<img src=\"figs/matrix_mult_w_one_hot.png\" style=\"float: center; zoom:80%;\" />\n",
    "\n",
    "- Cálculo do resultado na camada de saída:\n",
    "\n",
    "<img src=\"figs/output_weights_function.png\" style=\"float: center; zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Criando uma modelo de Word Embeddings com Word2Vec\n",
    "\n",
    "* Utilizaremos a biblioteca Gensim 4.2.0 -- [documentação](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* Vamos utilizar os tokens obtidos após a etapa de pré-processamento, uma vez que a entrada deve ser uma lista de tokens, como por exemplo:\n",
    "\n",
    "<center>$[[\"first\", \"sentence\"], [\"second\", \"sentence\"]]$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Cria o modelo e mostra o tamanho do vocabulário\n",
    "w2v_model = WordEmbeddings().word2vec(data[\"tokens\"])\n",
    "len(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Mostra o vocabulário\n",
    "\n",
    "vocab = list(w2v_model.wv.index_to_key)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Mostra as palavras mais similares com a palavra informada.\n",
    "## Caso a palavra não esteja no vocabulário, i.e., OOV, retorna uma mensagem de erro\n",
    "\n",
    "try:\n",
    "    sims = w2v_model.wv.most_similar(\"nyc\")\n",
    "    print(sims)\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vector = w2v_model.wv.get_vector(\"nyc\", norm=True)   # Retorna o numpy array da palavra\n",
    "print(vector.shape)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FastText\n",
    "\n",
    "No FastText, cada palavra é representada por uma sequência de N-gramas de caracteres. Por exemplo, dada uma palavra como “<farol>”, onde os caracteres especiais “<” e “>” indicam o início e o final da palavra, respectivamente, e uma janela (de tamanho arbitrário, escolhido pelo usuário) n = 3 , o FastText toma como entrada os 3-gramas:\n",
    "\n",
    "- *“<fa”, “far”, “aro”, “rol”, “ol>”* além da palavra completa *“farol”*.\n",
    "\n",
    "A cada um destes N-gramas, é associada uma representação vetorial única. Então, a representação vetorial\n",
    "final da palavra, consistirá na soma dos vetores de representação de todos os seus N-gramas\n",
    "[Joulin et al. 2016, Bojanowski et al. 2017].\n",
    "\n",
    "Isso ocorre para resolver uma limitação do Word2Vec original: o fato de que, nele, não há suporte para palavras OOV (out-of-vocabulary, ou fora do vocabulário.)\n",
    "\n",
    "Com o FastText, a cada uma destas palavras originalmente fora do vocabulário, se atribui uma representação próxima àquelas de grafia próxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cria o modelo e mostra o tamanho do vocabulário\n",
    "\n",
    "fast_text_model = WordEmbeddings().fasttext(data[\"tokens\"])\n",
    "len(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Mostra o vocabulário\n",
    "\n",
    "vocab = list(fast_text_model.wv.index_to_key)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Mostra as palavras mais similares com a palavra informada.\n",
    "## Caso a palavra não esteja no vocabulário, i.e., OOV, retorna uma mensagem de erro\n",
    "\n",
    "try:\n",
    "    sims = fast_text_model.wv.most_similar(\"nyc\")\n",
    "    print(sims)\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "vector = w2v_model.wv.get_vector(\"nyc\", norm=True)   # Retorna o numpy array da palavra\n",
    "print(vector.shape)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence Embeddings\n",
    "\n",
    "1. SkipThought\n",
    "2. InferSent\n",
    "3. **Universal Sentence Encoder (USE)**\n",
    "4. **SentenceBERT (SBERT)**\n",
    "5. Language-Agnostic SEntence Representations (LASER)\n",
    "6. Multilingual Universal Sentence Encoder (mUSE)\n",
    "7. **Language-agnostic BERT Sentence Embedding (LaBSE)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Smartphones\n",
    "    \"I like my phone\",\n",
    "    \"My phone is not good.\",\n",
    "    \"Your cellphone looks great.\",\n",
    "\n",
    "    # Weather\n",
    "    \"Will it snow tomorrow?\",\n",
    "    \"Recently a lot of hurricanes have hit the US\",\n",
    "    \"Global warming is real\",\n",
    "\n",
    "    # Food and health\n",
    "    \"An apple a day, keeps the doctors away\",\n",
    "    \"Eating strawberries is healthy\",\n",
    "    \"Is paleo better than keto?\",\n",
    "\n",
    "    # Asking about age\n",
    "    \"How old are you?\",\n",
    "    \"what is your age?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### SkipThought\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/skipThought.png\" style=\"float: center; zoom:100%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura do modelo SkipThought. Imagem de [Kiros et al. 2015].</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- Premissa: dada uma sentença de entrada, o modelo deve ser capaz de identificar regularidades na sentença que permitam inferir as senteças que vêm antes e depois dela. \n",
    "- Assim, a arquitetura do modelo é composta por três redes neurais, sendo:\n",
    "    - Uma para codificar uma sentença de entrada\n",
    "    - Outras duas para decodificar a sentença anterior e posterior à entrada\n",
    "- Treinamento via aprendizado não supervisionado\n",
    "- Durante o treinamento, o erro propagado pelas camadas de decodificação é utilizado para refinar o treinamento da camada de codificação\n",
    "- Ao final do treinamento, as camadas de decodificação são descartadas, restando apenas a de codificação que, por sua vez, é utilizada para gerar os embeddings de tamanho fixo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - Modelo não supervisionado, necessitando apenas do corpus de treinamento para criar o modelo de embeddings\n",
    "    - Essa arquitetura abriu várias possibilidades para novos modelos: baseados em redes convolucionais e até a expansão para trabalhar com parágrafos inteiros, em vez de sentenças\n",
    "\n",
    "- **Desvantagens**\n",
    "    - Necessita de contexto, i.e., a sentença anterior e posteiror à sentença de entrada. Por isso, torna-se difícil a utilização de textos de mídias sociais para o treinamento desse modelo\n",
    "    - Não encontramos a implementação estável do SkipThought disponível na linguagem Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### InferSent\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/infersent.png\" style=\"float: center; zoom:50%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura do modelo InferSent. Imagem de [Conneau et al. 2017].</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- Treinamento via aprendizado supervisionado\n",
    "- O InferSent é treinado com o corpus Stanford Natural Language Inference (SNLI), que é composto por triplas contendo:\n",
    "    - Uma premissa (uma sentença qualquer)\n",
    "    - Uma hipótese inferida a partir da premissa\n",
    "    - O julgamento de voluntários sobre a relação entre a premissa e a hipótese: ***vínculo, contradição, neutro***\n",
    "- Exemplo:\n",
    "    - Premissa: *todos os dias nessa localidade são ensolarados*\n",
    "    - Hipótese: *essa localidade está nublada*\n",
    "- A arquitetura do InferSent possui:\n",
    "    - Dois codificadores idênticos para as sentenças de entrada (premissa e hipótese), que geram os vetores $u$ e $v$\n",
    "    - Em seguida, uma camada para concatenar e extrair a relação entre os vetores $u$ e $v$, das seguintes formas:\n",
    "        - concatenando as duas representações $((u, v))$\n",
    "        - multiplicando os vetores $(u*v)$\n",
    "        - calculando o valor absoluto da diferença entre os vetores $(|u-v|)$\n",
    "    - Então, uma camada com um classificador com múltiplas camadas totalmente conectadas é alimentado pela representação resultante da camada anterior\n",
    "    - Por fim, a função de ativação Softmax, que possuí 3 saídas para representar cada uma das classes: vínculo, contradição e neutro\n",
    "- Diferentes arquiteturas de redes neurais foram avaliadas para os codificadores da arquitetura do InferSent, sendo redes neurais com células LSTM bidirecionais e camada de max pooling (referida no artigo como BiLSTM-max) o modelo com maior acurácia. Veja essa arquitetura na imagem abaixo:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/infersent-encoders.png\" style=\"float: center; zoom:50%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura BiLSTM-max. Imagem de [Conneau et al. 2017].</figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - O InferSent é capaz de generalizar melhor em várias tarefas de Semantic Textual Similarity (STS), que é um campo dentro de NLP relacionado a tarefas como tradução, sumarização e geração de textos, perguntas e respostas (QA), busca semântica e sistemas conversacionais\n",
    "    - O InferSent também é capaz de lidar com sentenças e textos com maior dimensão\n",
    "    - Não necessita que o corpus utilizado no treinamento esteja em uma sequência lógica, como é o caso do SkipThought\n",
    "\n",
    "- **Desvantagens**\n",
    "    - Treinamento supervisionado\n",
    "    - Disponível apenas para língua inglesa, limitando sua aplicação a esse idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Sentence Embeddings utilizando o InferSent\n",
    "\n",
    "- Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "- Certifique-se de ter pelo menos 9GB disponíveis em disco para isso\n",
    "- Devido ao download, a primeira execução é lenta\n",
    "- Para mais detalhes, acesse a página do [InferSent](https://github.com/facebookresearch/InferSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# InferSent\n",
    "infersent_embeddings = SentenceEmbeddings().infersent(sentences)\n",
    "infersent_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### USE\n",
    "\n",
    "\n",
    "- Oferece duas maneiras para gerar sentence embeddings de textos escritos em inglês:\n",
    "    - Utilizando o mecanismo de autoatenção (self-attention) da arquitetura Transformers \\[Vaswani et al. 2017\\]\n",
    "    <figure>\n",
    "        <img src=\"figs/self-attention.png\" style=\"float: center; zoom:80%;\" />\n",
    "        <center><figcaption>Ilustração do mecanismo self-attention da arquitetura Transformers, onde o Multi-Head Attention, consiste de várias camadas de atenção executando em paralelo. Imagem de [Vaswani et al. 2017].</figcaption></center>\n",
    "    </figure>\n",
    "    - Onde\n",
    "        - Q: a palavra de consulta (Query word)\n",
    "        - V: os vetores das palavras da sentença, i.e, word embeddings\n",
    "        - K: as chaves (Keys) são resultante entre o produto escalar entre Q e os vetores V\n",
    "    - Veja uma ilustração do processo de treinamento a seguir:\n",
    "    <figure>\n",
    "        <img src=\"figs/self-attention-example.png\" style=\"float: center; zoom:80%;\" />\n",
    "        <center><figcaption>Etapas para obter o valor de atenção. Imagem de [Arjun, Sarkar]<sup>2</sup>.</figcaption></center>\n",
    "    </figure>\n",
    "        \n",
    "    - Utilizando uma Deep Averaging Network (DAN) [Iyyer et al. 2015]\n",
    "    <figure>\n",
    "        <img src=\"figs/dan.png\" style=\"float: center; zoom:50%;\" />\n",
    "        <center><figcaption> Ilustração da arquitetura do modelo DAN. Imagem de [Iyyer et al. 2015].</figcaption></center>\n",
    "    </figure>\n",
    "    - A primeira camada computa a média dos word embeddings\n",
    "    - O vetor resultante é utilizado para alimentar uma ou mais camadas de uma rede neural profunda\n",
    "    - Por fim, é realizada a classificação (linear) na última camada (softmax)\n",
    "\n",
    "\n",
    "- Em comum, ambos os métodos recebem como entrada um a sequência de palavras, em minúsculo e tokenizada com Penn Treebank (PTB) tokenizer, e produzem um sentence embedding com 512 dimensões\n",
    "- Assim, é feito o treinamento multitarefa do USE, como mostra a figura a seguir, onde uma variedade de tarefas e estruturas são unidas por camadas/parâmetros de encoders compartilhados (caixas cinzas):\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/use-arch.png\" style=\"float: center; zoom:80%;\" />\n",
    "    <center><figcaption>Ilustração do treinamento multitarefa realizado pelo modelo USE. Imagem de Google AI Blog<sup>3</sup>.</figcaption></center>\n",
    "    </figure>\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">2</a>: Arjun, Sarkar. [\"All you need to know about 'Attention' and 'Transformers' - In-depth Understanding - Part 1.\"](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021) Towards Data Science. Último acesso em 31 de Outubro de 2022.\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">3</a>: Yang,Yinfei and Tar, Chris. [\"Advances in Semantic Textual Similarity.\"](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html) Google AI Blog. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - USE Transformers (USE_T):\n",
    "        - Considerando tanto a ordem como a identidade das palavras\n",
    "        - Alto desempenho em diferentes tarefas de NLP, mesmo quando os conjuntos de dados possuem poucas amostras para treinamento\n",
    "        \n",
    "    - USE DAN (USE_D):\n",
    "        - Complexidade de tempo linear, $O(n)$, sendo $n$ o tamanho da sentença de entrada\n",
    "        - Complexidade de espaço é constante no tamanho da entrada, $O(1)$\n",
    "\n",
    "- **Desvantagens**\n",
    "    - USE_T:\n",
    "        - Complexidade de tempo e de espaço é quadrática, $O(n^2)$\n",
    "    - USE_D:\n",
    "        - A informação sintática, i.e., a posição em que as palavras aparecem na frase, se perde e, por isso, a DAN é considerada uma função de composição não ordenada\n",
    "        - Precisa de conjunto de treinamento maiores para alcançar bons desempenhos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentence Embeddings utilizando o USE\n",
    "\n",
    "- Utilizaremos o modelo **DAN** (USE_D)\n",
    "- Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "- Certifique-se de ter pelo menos 1GB disponível em disco para isso\n",
    "- Devido ao download, a primeira execução é lenta\n",
    "- Para mais detalhes, acesse a página do [USE](https://tfhub.dev/google/universal-sentence-encoder/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# USE_D\n",
    "use_embeddings = SentenceEmbeddings().use(sentences)\n",
    "use_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SBERT\n",
    "\n",
    "- O SBERT é um modelo estado-da-arte baseado no Bidirectional Encoder Representations from Transformers (BERT)\n",
    "    - BERT foi criado para ser facilmente adaptado a diferentes tarefas de similaridade semântica sem necessitar grandes alterações em sua estrutura (agnóstico de tarefa)\n",
    "    - O BERT utiliza uma lógica similar à aplicada pelo SkipThought, onde o contexto anterior e posterior de uma palavra (ou sentença), é usado para identificar as características que melhor representam sua relação com seu entorno\n",
    "    - Arquitetura do BERT:\n",
    "    <figure>\n",
    "        <img src=\"figs/bert.png\" style=\"float: center; zoom:50%;\" />\n",
    "        <center><figcaption>Ilustração da arquitetura do modelo BERT. Imagem de Hugging Face<sup>4</sup>.</figcaption></center>\n",
    "    </figure>\n",
    "    - BERT utiliza duas estratégias de treinamento: Masked LM (MLM) e Next Sentence Prediction (NSP)\n",
    "    - Durante o treinamento, o BERT percorre o corpus tanto na ordem natural em que as sentenças ocorrem, quanto na ordem inversa, por isso o uso do termo \"bidirecional\"\n",
    "\n",
    "- O SBERT, semelhante ao InferSent, contem dois codificadores BERT e uma estrutura de redes siamesas, que compartilham os pesos entre si e necessita apenas de uma entrada, o que torna o SBERT muito mais rápido em comparação ao BERT\n",
    "- Veja a arquitetura do SBERT na figua abaixo:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/sbert.png\" style=\"float: center; zoom:100%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura do modelo SBERT. À esquerda, a arquitetura utilizada para tarefas de classificação, e à direita, para tarefas de regressão. Imagem de [Reimers and Gurevych 2019].</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- O SBERT utiliza a tarefa de Natural Language Inference (NLI) para treinamento (fine-tune)\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">4</a>: Muller, Britney. [\"BERT 101 🤗 State Of The Art NLP Model Explained.\"](https://huggingface.co/blog/bert-101) Hugging Face. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Vantagens**\n",
    "    - O SBERT obteve melhor desempenho em relação ao InferSent e USE\n",
    "    - Resolve os problemas de complexidade computacional apresentados pelo BERT\n",
    "    - Modelos SBERT monolíngues podem ser aumentados para tarefas de NLP multilíngues com boa capacidade de generalização\n",
    "    - SBERT está disponível por meio de uma biblioteca de código aberto feita em Python, chamada [Sentence-Transformes](https://www.sbert.net), que disponibiliza vários modelos pré-treinados para diferentes tarefas, incluindo modelos multilíngues\n",
    "    - Documentação bem estruturada e expõe alguns métodos fáceis de usar para a geração de embeddings\n",
    "    - A transferência de aprendizado também pode ser facilmente feita com essa biblioteca, permitindo adaptar qualquer modelo disponibilizado a diferentes tarefas de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentence Embeddings utilizando o SBERT\n",
    "\n",
    "- Utilizaremos o modelo **all-MiniLM-L6-v2**, que é 5x mais rápido que sua versão base (**all-mpnet-base-v2**) e significativamente menor (de 420MB para 80MB), mas ainda mantém um bom desempenho\n",
    "- O termo **all-** indica que o modelo foi treinado com todos os dados disponíveis (mais de 1 bilhão de pares de treinamento) e são projetados como modelos de propósito geral\n",
    "- Para mais detalhes, acesse a página do [SBERT](https://www.sbert.net/docs/pretrained_models.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SBERT\n",
    "sbert_embeddings = SentenceEmbeddings().sbert(sentences)\n",
    "sbert_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelos Multilíngues\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/lang-agnostic.png\" style=\"float: center; zoom:80%;\" />\n",
    "    <center><figcaption>Ilustração dos primeiros dois componentes principais para frases idênticas em Inglês e Russo. Imagem de [Yang,Yinfei and Ahmad, Amin. 2020]<sup>5</sup>.</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">5</a>: Yang,Yinfei and Ahmad, Amin. \"Making monolingual sentence embeddings multilingual using knowledge distillation.\" arXiv preprint arXiv:2004.09813 (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### LASER\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/laser.png\" style=\"float: center; zoom:80%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura do modelo LASER. Imagem de [Iyyer et al. 2015].</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- Primeiro modelo a explorar a representação de sentenças multilíngues para propósito geral, ou seja, sem ter uma tarefa de NLP específica\n",
    "- LASER utiliza uma arquitetura sequence-to-sequence encoder-decoder\n",
    "    - Encoder: é composto por uma rede LSTM bidirecional (ou simplesmente, BiLSTM)\n",
    "    - Decoder: é composto por apenas uma rede LSTM\n",
    "- Cada palavra é primeiro codificada por um vocabulário de [Byte-Pair encoding (BPE)](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0), onde é feita a concatenação de todos os corpora de treinamento dos 93 idiomas suportados pelo modelo, antes de alimentar a rede BiLSTM\n",
    "- Assim, não é necessário informar explicitamente o idioma de entrada para o encoder, o que auxília o modelo a aprender as representações independente do idioma\n",
    "- O sentence embedding é obtido no encoder ao aplicar a operação max pooling sobre a BiLSTM\n",
    "- O decoder recebe como entrada o sentence embedding gerado pelo encoder, concatenado em cada etapa do tempo com a codificação BPE da sentença alvo (i.e., as palavras $<s>, y_1, ..., y_n$) e a codificação que específica qual idioma deve ser gerado (representado pela variável $L_id$ )\n",
    "- o LASER foi treinado de maneira end-to-end para tarefa de tradução, considerando dados anotados em apenas dois idiomas alvos, onde foram utilizados Inglês e Espanhol\n",
    "- Após o treinamento, o encoder torna-se capaz de gerar sentence embeddings em qualquer um dos 93 idiomas do conjunto de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Sentence Embeddings utilizando o LASER\n",
    "\n",
    "- Antes de utilizar o LASER, você deve fazer o download do modelo.\n",
    "- Para isso, você deve executar o comando '!python -m laserembeddings download-models \"data\"', que está comentado no próximo slide\n",
    "- Você pode informar o código de idioma (ISO 639-1), para cada sentença da lista\n",
    "- Por padrão, consideramos que todas as sentenças estão escritas em inglês (\"en\")\n",
    "- Para mais detalhes, acesse a página do [LASER](https://github.com/facebookresearch/LASER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# LASER\n",
    "# Para fazer o download, descomente a linha abaixo:\n",
    "!python -m laserembeddings download-models \"data\"\n",
    "laser_embeddings = SentenceEmbeddings().laser(sentences)\n",
    "laser_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### mUSE\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/muse.png\" style=\"float: center; zoom:100%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura do modelo mUSE. Imagem de Google AI Blog<sup>6</sup>.</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- O mUSE, como o próprio nome sugere, é uma extensão do modelo USE\n",
    "- Como podemos ver em sua arquitetura, o mUSE também utiliza o multi-task dual-encoder framework\n",
    "- Além disso, ele utiliza uma tarefa bridging de translation ranking, para ser capaz de aprender representações semelhantes para frases com significados idênticos, pórem, escritas em idiomas distintos\n",
    "- A tarefa de classificação de tradução (translation ranking), tem o objetivo de encontrar a tradução verdadeira sobre uma coleção de frases no idioma alvo (target)\n",
    "- Alem disso, também foram adicionados dois modelos multilíngues multitarefas, sendo um baseado em Rede Neural Convolucional (Convolutional Neural Network, CNN) e outro na arquitetura Transformers\n",
    "- Também foi adicionado um modelo Transformers multilíngue para uso em recuperação de perguntas e respostas (Retrieval Question Answering, ReQA).\n",
    "- Ao todo, 16 idiomas distintos são suportados pelo mUSE\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">6</a>: Yang,Yinfei and Ahmad, Amin. [\"Multilingual Universal Sentence Encoder for Semantic Retrieval.\"](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html) Google AI Blog. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Sentence Embeddings utilizando o mUSE\n",
    "\n",
    "- Utilizaremos o modelo **Convolutional Neural Network** (CNN)\n",
    "- Na primeira execução, é feito o download de arquivos de modelos e embeddings\n",
    "- Certifique-se de ter pelo menos 1GB disponível em disco para isso\n",
    "- Devido ao download, a primeira execução é lenta\n",
    "- Para mais detalhes, acesse a página do [mUSE](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# mUSE\n",
    "muse_embeddings = SentenceEmbeddings().muse(sentences)\n",
    "muse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LaBSE\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/labse.png\" style=\"float: center; zoom:60%;\" />\n",
    "    <center><figcaption>Ilustração da arquitetura do modelo LaBSE. Imagem de [Feng et al. 2020].</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- LaBSE também utiliza a abordagem dual-encoders, semelhante ao mUSE, para aprender representações multilíngues\n",
    "    - Dual-encoders: pares de sentenças de origem e alvo são codificadas separadamente, onde os embeddings de cada uma são obtidos por encoders distintos, mas que compartilham seus parâmetros\n",
    "- O principal diferencial do LaBSE é a combinação de um encoder pré-treinado baseado no modelo de linguagem BERT com os dual-encoders\n",
    "- LaBSE é treinado (fine-tune) considerando a tarefa de tradução\n",
    "- Com isso, o LaBSE consegue reduzir drasticamente a quantidade de treinamento necessário e, ainda assim, alcançar um desempenho superior\n",
    "- Ao todo, 109 idiomas distintos são suportados pelo LaBSE\n",
    "- o LaBSE também foi capaz de produzir bons resultados para mais de 30 idiomas além dos que já são suportados, mesmo não havendo quaisquer dados de treinamento de tais idiomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentence Embeddings utilizando o LaBSE\n",
    "\n",
    "- A implementação do modelo LaBSE encontra-se disponível na biblioteca SentenceTransformers\n",
    "- Na primeira execução, é feito o download de arquivos do modelo:\n",
    "    \n",
    "    <center>model = SentenceTransformer('sentence-transformers/LaBSE')</center>\n",
    "\n",
    "- Certifique-se de ter pelo menos 2GB disponível em disco para isso\n",
    "- Para mais detalhes, acesse a página do [LaBSE](https://huggingface.co/sentence-transformers/LaBSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# LaBSE\n",
    "labse_embeddings = SentenceEmbeddings().labse(sentences)\n",
    "labse_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similaridade entre sentenças\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src=\"figs/sts-example.png\" style=\"float: center; zoom:130%;\" />\n",
    "    <center><figcaption>Ilustração da tarefa STS. Imagem de Google AI Blog<sup>7</sup>.</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- A tarefa de similaridade textual semântica (ou, Semantic Textual Similarity, STS) é responsável por avaliar quão similar são dois textos, em relação ao seu significado\n",
    "- STS é um problema difícil devido às nuances da linguagem natural, sendo que dois textos semelhantes podem não ter uma única palavra em comum\n",
    "- Com o advento dos modelos de sentences embeddings, resultados animadores têm sido publicados, como pode ser visto em [Semantic Textual Similarity on STS Benchmark](https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark)\n",
    "- A seguir, apresentaremos um exemplo simples para calcular a similaridade entre sentenças utilizando sentence embeddings\n",
    "\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">7</a>: Yang,Yinfei and Tar, Chris. [\"Advances in Semantic Textual Similarity.\"](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html) Google AI Blog. Último acesso em 31 de Outubro de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo baseado em:\n",
    "# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "sent_emb = SentenceEmbeddings().sbert(sentences) #escolha o modelo de sentence embeddings de sua preferência\n",
    "plot_similarity(sentences, sent_emb, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelagem e Extração de Conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agrupamento de Textos (clusterização)\n",
    "\n",
    "* **Objetivo:** agrupar objetos similares (documentos, sentenças ou termos)\n",
    "* Métodos mais conhecidos:\n",
    "    - k-means\n",
    "    - Agrupamento Hierárquico\n",
    "    - Detecção de Comunidades em Grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Dataset de exemplo\n",
    "\n",
    "* Iremos apresentar os resultados de cada método usando um mesmo conjuntos de dados\n",
    "* Dataset: \"20 newsgroups\" do Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Carregamos apenas algumas categorias presentes no dataset 20 newsgroups\n",
    "categories = [\n",
    "    \"alt.atheism\",  # Ateísmo\n",
    "    \"talk.religion.misc\",  # Religião\n",
    "    \"comp.graphics\",  # Computação gráfica\n",
    "    \"sci.space\",  # Ciência - espaço\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    shuffle=True,\n",
    "    random_state=100,\n",
    "    categories=categories\n",
    ")\n",
    "\n",
    "labels = dataset.target\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset.data)} Documentos - {true_k} Categorias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformação do dataset em um vetor TF-IDF\n",
    "\n",
    "tfidf_model, tfidf_data = stats_models.tfidf(dataset.data)\n",
    "\n",
    "print(f\"Documentos: {tfidf_data.shape[0]}, Features: {tfidf_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### k-means\n",
    "\n",
    "* Simples e intuitivo, por isso é o mais famoso\n",
    "\n",
    "<img src=\"figs/kmeans.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "Imagem: Exemplo de clusters no espaço bidimensional usando PCA. \n",
    "\n",
    "Fonte: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Intuição do k-means:\n",
    "\n",
    "1. Escolher o número de agrupamentos $k$ (ou clusters)\n",
    "2. Definir os centróides de cada cluster no espaço N-dimensional (definidos aleatóriamente ou por heurística); \n",
    "3. Alocar cada texto ao cluster mais próximo (e.g. usando similaridade do cosseno, distância euclidiana, etc.); \n",
    "4. Recalcular os centróides dos clusters de acordo com a média dos textos contidos neles;\n",
    "    * por isso o nome k-means\n",
    "6. Repetir o processo até atingir critério de parada:\n",
    "    * número de iterações max. ou \n",
    "    * quando não houver alterações significativas nos centróides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina modelos com k=2 até k=30 para testar usando o método do cotovelo\n",
    "results = []\n",
    "for k in range(2, 30):\n",
    "    kmeans = Clustering().kmeans(tfidf_data, k=k)\n",
    "    kmeans_model = kmeans.fit(tfidf_data)\n",
    "\n",
    "    sse = kmeans_model.inertia_\n",
    "    results.append([sse, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneebow.rotor import Rotor\n",
    "\n",
    "rotor = Rotor()\n",
    "rotor.fit_rotate(results)\n",
    "\n",
    "# Plota a curva de k versus SSE (soma dos erros quadráticos)\n",
    "rotor.plot_elbow()\n",
    "\n",
    "# Obtém o valor ótimo de clusters\n",
    "optimal_k = rotor.get_knee_index()\n",
    "print('Número óotimo de clusters:', optimal_k)\n",
    "\n",
    "#pd.DataFrame(results, columns=['SSE', 'k']).plot.scatter('k', 'SSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina o modelo k-means com o número ótimo de clusters\n",
    "kmeans = Clustering().kmeans(tfidf_data, k=optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevê o cluster de cada texto no conjunto de dados\n",
    "clusters = kmeans.fit_predict(tfidf_data)\n",
    "\n",
    "# Aqui fazemos a redução dimensional com PCA e plotamos os textos coloridos de acordo com seu cluster\n",
    "Clustering().kmeans_pca_plot(tfidf_data, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Agrupamento Hierárquico\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD:código do agrupamento hierárquico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Detecção de Comunidades em Grafos\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelagem de Tópicos\n",
    "\n",
    "* **Objetivo:** identificar estruturas semânticas em um corpus\n",
    "* Processo não supervisionado\n",
    "* A ordem das palavras não importa, portanto podem ser usadas representações como o Bag of Words (BoW) ou Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos em um corpus de notícias\n",
    "\n",
    "* delimitar diferentes eventos (e.g., confronto entre Rússia e Ucrânia, Copa do Mundo, etc.)\n",
    "* identificar grandes temas (e.g., economia, política, educação, etc.)\n",
    "* pautas de discussão (e.g., aborto, questões climáticas, combate ao crime, etc.)\n",
    "* !!! Ou todos os casos acima misturados -- o que é comum de acontecer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Funcionamento de um modelo de tópicos\n",
    "\n",
    "* cada documento pode ser representado por um histograma com a contagem de cada termo contido nele\n",
    "* a forma desse histograma é proveniente de uma distribuição entre $k$ tópicos \n",
    "* os $k$ tópicos são distribuídos entre os termos no vocabulário do corpus\n",
    "\n",
    "O objetivo da modelagem de tópicos, então, é aprender essas distribuições.\n",
    "\n",
    "<img src=\"figs/topic-model.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "Fonte: https://pyro.ai/examples/prodlda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Técnicas para modelagem de tópicos\n",
    "\n",
    "* Latent Dirichlet Allocation (LDA)\n",
    "* Latent Semantic Analysis (LSA) ou LSI (redução dimencional)\n",
    "* Probabilistic Semantic Analysis (pLSA) (versão probabilística do LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Pré-processamento para modelagem de tópicos\n",
    "\n",
    "* tentar preservar palavras que podem ser representativas para o domínio\n",
    "* não é necessário preservar a sequência dos termos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Desafios\n",
    "\n",
    "Obs: os exemplos a seguir são de tópicos identificados em um corpus de páginas web sobre as Eleições de 2018 no Brasil e Eleições de 2019 no Canadá.\n",
    "\n",
    "* Muitas vezes os tópicos são difíceis de serem caracterizados, necessitando conhecimento específico de domínio\n",
    "    * e.g. \"battisti, pf, italiano, italia, grafico, recurso, utc, extradicao, moro, deus\"\n",
    "    * e.g. \"pipeline, climate, oil, energy, fossil_fuel, carbon, france, kinsella, people_party, industry\"\n",
    "* As top N palavras de um tópico nem sempre delimitam claramente o assunto\n",
    "    * e.g. \"ex_presidente, ciro_gomes, ciro, artista, cunha, universidade, dilma, eduardo, classificar, reeleicao\"\n",
    "* O valor $k$ é escolhido manualmente\n",
    "    * mesmo usando métricas como score de coerência e distribuição dos tópicos ao longo do corpus, ainda é necessário o julgamento humano da adequabilidade do valor $k$\n",
    "* Tópico evoluem ao longo do tempo (para isso existem modelos temporais de tópicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplo prático\n",
    "\n",
    "Ao final deste notebook, colocamos um exemplo prático usando o LDA em um dataset de notícias sobre as eleições canadenses de 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compreensão Semântica e Emocional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Detecção de Intenção\n",
    "\n",
    "<img src=\"figs/ir-example.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Detecção de intenção\n",
    "- Uma intenção fornece uma interpretação geral do significado de uma expressão\n",
    "- Normalmente, é uma tarefa abstraída em um processo de classificação\n",
    "- Para isso, o primeiro passo é a obtenção de um conjunto de expressões rotuladas (*corpus*) para treinamento do modelo de classificação\n",
    "- Como veremos a seguir, existem muitos *corpus* disponíveis publicamente para essa tarefa.\n",
    "- Mas, caso deseje preparar o seu próprio *corpus* com dados de mídias sociais, você pode utilizar a modelagem de tópicos para determinar quais assuntos estão presentes no *corpus* e realizar a separação dos dados e, então, revisar e rotular manualmente cada um deles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus\n",
    "- Iremos considerar o conjunto de dados disponível em [Wang, Jinpeng, et al.]<sup>[1](#footnoteIntentCorpus)</sup>\n",
    "- Categorias de intenção nos *tweets*: **Food & Drink, Travel, Career & Education, Goods & Services, Event & Activities, Trifle, Non-intent**\n",
    "- Veja na Tabela a seguir mais detalhes sobre os dados:\n",
    "\n",
    "<center> Tabela: Intenções e exemplos do conjunto de dados [Wang, Jinpeng, et al.]<sup>1</sup> </center>\n",
    "\n",
    "| Categoria | # (%) | Exemplo |\n",
    "| --- | --- | --- |\n",
    "| Food & Drink | 245 (11,5%) | hungry...i need a salad......four more days to the BEYONCE CONCERT... |\n",
    "| Travel | 187 (8,78%) | I need a vacation really bad. I need a trip to Disneyland! |\n",
    "| Career & Education | 159 (7,46%) | this makes me want to be a lawyer RT @someuser new favorite line from an ... |\n",
    "| Goods & Services | 251 (11,78%) | mhmmm, i wannna a new phoneeee. ... i have to go to the hospital. ... |\n",
    "| Event & Activities | 312 (15.07%) | on my way to go swimming with the twoon @someuser; i love her so muchhhhh! |\n",
    "| Trifle | 436 (20,47%) | I'm so happy that I get to take a shower with myself. :D |\n",
    "| Non-intent | 531 (24,92%) | So sad that Ronaldo will be leaving these shores...http://URL |\n",
    "\n",
    "\n",
    "<a name=\"footnoteIntentCorpus\">1</a>: Wang, Jinpeng, et al. \"Mining user intents in twitter: A semi-supervised approach to inferring intent categories for tweets.\" Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelo de Classificação de Intenções\n",
    "<br>\n",
    "\n",
    "<center>Redes Neurais Recorrentes (RNN) X Sentence Embeddings (imagem de [Feng et al. 2020])</center>\n",
    "\n",
    "<img src=\"figs/bilstm-ir.png\" style=\"float: left; zoom:60%;\" />\n",
    "<img src=\"figs/labse.png\" style=\"float: right; zoom:60%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#temporario\n",
    "intents = [\n",
    "    \"Smartphones\",\n",
    "    \"Smartphones\",\n",
    "    \"Smartphones\",\n",
    "\n",
    "    \"Weather\",\n",
    "    \"Weather\",\n",
    "    \"Weather\",\n",
    "\n",
    "    \n",
    "    \"Food and health\",\n",
    "    \"Food and health\",\n",
    "    \"Food and health\",\n",
    "    \n",
    "    \"Asking about age\",\n",
    "    \"Asking about age\"\n",
    "]\n",
    "\n",
    "intent_labse_model, X_test_labse, y_test_labse, classes = SemanticComprehension().training_intents(\"labse\", sentences, intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs = 20\n",
    "# Batch size = 32\n",
    "# Hidden layers = 300\n",
    "# Max sequence length = 280 --> tweet size\n",
    "intent_bilstm_model, X_test_rnn, y_test_rnn, classes = SemanticComprehension().training_intents(\"bilstm\", sentences, intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaBSE's performance\n",
    "y_hat = intent_labse_model.predict(X_test_labse)\n",
    "SemanticComprehension().plot_confusion_matrix(y_test_labse, y_hat, classes, \"figs/ir-labse-cm.png\")\n",
    "\n",
    "# RNN's performance\n",
    "y_hat = intent_bilstm_model.predict(X_test_rnn)        \n",
    "SemanticComprehension().plot_confusion_matrix(np.argmax(y_test_rnn,axis=1), np.argmax(y_hat, axis=1), classes, \"figs/ir-rnn-cm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix de Confusão\n",
    "<br>\n",
    "\n",
    "<center>Redes Neurais Recorrentes (RNN) X Sentence Embeddings</center>\n",
    "\n",
    "<img src=\"figs/ir-rnn-cm.png\" style=\"float: left; zoom:22%;\" />\n",
    "<img src=\"figs/ir-labse-cm.png\" style=\"float: right; zoom:22%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Predição de intents para os tweets coletados\n",
    "intents = CompreensaoSemantica().predicao_intencoes(intent_labse_model, sentences) ## configurado apenas para o modelo LaBSE\n",
    "intents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconhecimento de Entidades Nomeadas\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corpus\n",
    "- Iremos considerar o conjunto de dados disponível em  https://www.kaggle.com/code/amoghjrules/twitter-entity-recognition-using-bilstms\n",
    "- abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "for sentence in tqdm(sentences):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    entity = {}\n",
    "    for i, ent in enumerate(doc.ents):\n",
    "        entity[i] = {\n",
    "                        \"value\":ent.text,\n",
    "                        \"entity\":ent.label_,\n",
    "                        \"start\":ent.start_char,\n",
    "                        \"end\":ent.end_char\n",
    "                    }\n",
    "    entities.append(entity)\n",
    "\n",
    "print(entities[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Análise de Sentimentos (AS)\n",
    "\n",
    "* Também conhecida como **Mineração de Opiniões**\n",
    "* AS é o \"estudo computacional das opiniões, atitudes e emoções de pessoas em relação a uma entidade\" [Medhat et al. 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tarefas de AS\n",
    "\n",
    "* **Detecção de sentimento**\n",
    "    - e.g. positivo, negativo ou neutro\n",
    "* **Identificação de emoções**\n",
    "    - e.g. sentimentos como raiva, antecipação, nojo, medo, alegria, tristeza, surpresa, confiança, etc. \n",
    "* **Detecção de toxicidade** \n",
    "    - e.g. categorias como insulto, profanidade, conteúdo sexualmente explícito, etc. [Jigsaw 2022]\n",
    "* **Análise multilíngue de sentimentos**\n",
    "* **Detecção de sarcasmo**\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exemplos de aplicações de AS\n",
    "\n",
    "* Identificação de comentários agressivos em notícias [Jigsaw 2022]\n",
    "* Extração da opinião pública sobre um candidato ou partido político [Pang et al. 2008]\n",
    "* Priorização de respostas a avaliações negativas de produtos [Bougie et al. 2003] \n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vantages da AS\n",
    "\n",
    "* Permite análises em larga escala\n",
    "* Reduz a subjetividade provocada por avaliadores humanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Níveis da AS\n",
    "\n",
    "1. **Nível de documento** - premissa: documento expressa opinião sobre apenas uma entidade; \n",
    "2. **Nível de frase** - premissa: frase expressa opinião sobre apenas uma entidade;  \n",
    "3. **Nível de aspecto** - múltiplas opiniões sobre múltiplos aspectos (ou alvos)\n",
    "    - e.g. “A <span style=\"color:#f00\">qualidade de voz</span> deste telefone <span style=\"color:#f00\">não é boa</span>, mas a <span style=\"color:#00f\">vida útil da bateria</span> é <span style=\"color:#00f\">longa</span>”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Abordagens para criação de modelos de AS\n",
    "\n",
    "1. **Usando Léxicos** \n",
    "    * <span style=\"color: #088B00\">VANTAGEM</span>: independência de domínio\n",
    "    * <span style=\"color: #f00\">DESVANTAGEM</span>: menor precisão e baixa escalabilidade\n",
    "2. **Aprendizado de máquina (ML)**\n",
    "    * <span style=\"color: #088B00\">VANTAGEM</span>: maior precisão\n",
    "    * <span style=\"color: #f00\">DESVANTAGEM</span>: maior dependência de domínio\n",
    "3. **Híbrido (léxico + ML)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### A seguir iremos apresentar\n",
    "\n",
    "* EmoLex\n",
    "* LIWC\n",
    "* Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frases que vamos usar como exemplo para os modelos de AS\n",
    "\n",
    "emotional_sentences = [\n",
    "\n",
    "    # exemplo de frase positiva\n",
    "    \"How good it is to live in Curitiba!\",\n",
    "\n",
    "    # exemplo de frase neutra\n",
    "    \"This car is grey.\",\n",
    "\n",
    "    # exemplo de frase negativa\n",
    "    \"Shut up, you're an idiot!\",\n",
    "\n",
    "    # exemplo de frase negativa, mas com palavras que podem confundir o modelo de AS como \"friend\"\n",
    "    \"It must be so sad to have you as a friend\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### EmoLex\n",
    "\n",
    "* Criado em 2013, é um dos maiores léxicos disponíveis em língua Inglesa\n",
    "* Baseado em unigramas e bigramas dos léxicos\n",
    "    * General Inquirer Lexicon\n",
    "    * WordNet Affect Lexicon\n",
    "* Anotações feitas via crowdsourcing pelo Mechanical Turk\n",
    "* Associa palavras com as oito emoções da teoria de Plutchik [Plutchik 1980]\n",
    "    * raiva, medo, antecipação, confiança, surpresa, tristeza, alegria e desgosto\n",
    "* Também inclui as catergorias de sentimento negativo e positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de sentenças processadas com o Emolex (frequência de emoções nas sentenças)\n",
    "SentimentAnalysis.emolex(emotional_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LIWC\n",
    "\n",
    "* Ferramenta para identificar características linguísticas, psicológicas e sociais em textos [Pennebaker et al. 2001]\n",
    "* AS baseada em léxico (um dos maiores e mais completos na quantidade de termos e categorias cobertas)\n",
    "* Léxicos separados para línguas diferentes, sendo o inglês a língua padrão\n",
    "* Disponível apenas para Windows via interface gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Categorias de conteúdo disponíveis no LIWC\n",
    "\n",
    "* **emoções positivas** (e.g. amor, legal, doce, etc.)\n",
    "* **emoções negativas** (e.g. ferido, feio, desagradável, etc.)\n",
    "* **processos sociais** (e.g. filha, marido, vizinho, adulto, bebê, etc.)\n",
    "* **processos cognitivos** (e.g. pensar, conhecer, causa, etc.)\n",
    "* **processos perceptivos** (e.g. observar, escutar, sentir, etc.)\n",
    "* **processos biológicos** (e.g. comer, sangue, dor, etc.)\n",
    "* **relatividade** (e.g. chega, vai, embaixo, ontem, até, fim, etc.)\n",
    "* **preocupações sociais** (e.g. auditar, igreja, cozinhar, trabalhar, mestrado, etc.)\n",
    "* **consentimento** (e.g. concordar, ok, etc.)\n",
    "* **não-fluências e palavras de preenchimento** (e.g. hm, er, umm, certo, etc.)\n",
    "* entre outras [Tausczik and Pennebaker 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Categorias de função disponíveis no LIWC\n",
    "\n",
    "* **pronomes** \n",
    "* **preposições** \n",
    "* **artigos** \n",
    "* **conjunções** \n",
    "* **verbos auxiliares** \n",
    "* entre outras [Tausczik and Pennebaker 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Interface do LIWC\n",
    "\n",
    "<img src=\"figs/liwc.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Exemplo de processamento com o LIWC\n",
    "\n",
    "Para executar o processamento das sentenças de exemplo, será necessário [adquirir uma licença](https://www.liwc.app/buy), instalar o LIWC em um computador com Windows e processar o arquivo .csv gerado na célula a seguir. \n",
    "\n",
    "Você também poderá usar a versão de [teste online](https://www.liwc.app/demo), inserindo manualmente cada uma das sentenças geradas no .csv a seguir (essa versão é limitada a algumas poucas categorias, mas estão disponíveis as categorias de \"Negative tone\" e \"Positive tone\", bastante úteis em tarefas de AS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = 'data/emotional_sentences_LIWC.csv'\n",
    "\n",
    "if not os.path.exists(file):\n",
    "    # cria um .csv com exemplos para ser processado pelo LIWC\n",
    "    df_LIWC = pd.DataFrame({'text': emotional_sentences})\n",
    "    df_LIWC.to_csv(file, index=False)\n",
    "else:\n",
    "    # carrega o arquivo com exemplos processado pelo LIWC\n",
    "    df_LIWC = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apresenta apenas algumas colunas mais interessantes para AS\n",
    "df_LIWC[['text', 'affect', 'posemo', 'negemo', 'anx', 'anger', 'sad']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Perspective API\n",
    "\n",
    "* Usado para identificação de toxicidade em comentários online\n",
    "* Modelo multilíngue\n",
    "* Disponibilizado gratuitamente por meio de uma iniciativa da Jigsaw, uma empresa da Google [Jigsaw 2022]\n",
    "* Acessado via API pública\n",
    "* Adotada pelos principais veículos jornalísticos internacionais para moderar comentários em seus portais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Desafios para identificar toxicidade em comentários\n",
    "\n",
    "* Geralmente são textos curtos - ex.: “sei… 😏”, “¬¬”, “no way!”\n",
    "* Uso de emojis ambíguos - ex.: 😏,🌚,😋 \n",
    "* Erros de ortografia propositais (ou não) - ex.: “çocorro”;\n",
    "* Gírias da internet - ex.: “vc”, “pq”, “lol”, “iti malia”\n",
    "* Sutilezas inerentes à língua ou localidade - ex.: “oxi”, “p**ra!”;\n",
    "* Especificidades de contexto - ex.: “ex-presidiário”, “bozo”;\n",
    "* Uso de figuras de linguagem - ex.: metáfora, ironia, etc.;\n",
    "* Susceptíveis a ataques adversários - ex.: “st.Up1d”\n",
    "* Podem ocorrer de forma esparsa no conjunto de dados;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Definição de comentário tóxico\n",
    "\n",
    "<center>\"Um comentário rude, desrespeitoso ou irracional que provavelmente fará você sair de uma discussão.\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Funcionamento do Perspective API\n",
    "\n",
    "* Atribui uma pontuação contínua entre 0 e 1 para diferentes categorias de toxicidade de acordo com o % no texto\n",
    "* Uma pontuação mais alta para uma determinada categoria (ou atributo), indica uma maior probabilidade de um leitor perceber que o comentário possui este atributo\n",
    "* e.g. “Você é um idiota” pode receber uma pontuação de $0.8$ para o atributo TOXICIDADE, indicando que 8 entre 10 pessoas perceberiam esse comentário como tóxico\n",
    "* Portanto um comentário com pontuação de TOXICIDADE de $0.9$ não necessariamente é mais tóxico  que uma com $0.7$\n",
    "\n",
    " [Jigsaw 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"figs/perspective.png\" style=\"float: center; zoom:100%;\" />\n",
    "\n",
    "Fonte: https://perspectiveapi.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Atributos de produção (multilíngue)\n",
    "\n",
    "- **TOXICITY** - “Um comentário rude, desrespeitoso ou irracional que provavelmente fará com que as pessoas deixem uma discussão”; \n",
    "- **SEVERE_TOXICITY** - “Um comentário que é muito odioso, agressivo, desrespeitoso, ou muito provável de fazer um usuário sair de uma discussão, ou desistir de compartilhar sua perspectiva. Este atributo é muito menos sensível a formas mais leves de toxicidade, como comentários que incluem usos positivos de palavrões”; \n",
    "- **IDENTITY_ATTACK** - “Comentários negativos ou de ódio direcionados a alguém por causa de sua identidade”; \n",
    "- **INSULT** - “Comentário ofensivo, inflamatório ou negativo para uma pessoa ou grupo de pessoas”; \n",
    "- **PROFANITY** - “Xingamentos, palavrões ou outras linguagens obscenas, ou profanas”; \n",
    "- **THREAT** - “Descreve a intenção de infligir dor, lesão ou violência contra um indivíduo, ou grupo”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Acessando o Perspective API\n",
    "\n",
    "* Para executar os exemplos a seguir, você deverá solicitar acesso ao Perspective API no Google Cloud seguindo [esse passo a passo](https://developers.perspectiveapi.com/s/docs-get-started)\n",
    "* Obtenha a chave da API para usar nos próximos passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credenciais do Perspective API\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "PERSPECTIVE_API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemplo de sentenças processadas com o Perspective API\n",
    "SentimentAnalysis.perspective(emotional_sentences, PERSPECTIVE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>6. Aplicações</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 6.1 Polarização Política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Contextualização\n",
    "\n",
    "* Artigo [Reaching the bubble may not be enough: news media role in online political polarization](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00357-3)\n",
    "* Autores\n",
    "    * Jordan K. Kobellarz - UTFPR\n",
    "    * Miloš Broćić - University of Toronto\n",
    "    * Alexandre R. Graeml - UTFPR\n",
    "    * Daniel Silver - University of Toronto\n",
    "    * Thiago H. Silva- UTFPR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Objetivo\n",
    "\n",
    "Estudar o papel de intermediadores em cenários polarizados (Bubble Reachers)\n",
    "    \n",
    "<img src=\"figs/bubble-reachers.png\" style=\"float: center; zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Situações estudadas\n",
    "\n",
    "* Eleição presidencial brasileira de 2018\n",
    "* Eleição federal canadense de 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Dados / Metodologia\n",
    "\n",
    "1. Dados coletados usando a API de Streaming do Twitter\n",
    "2. Identificação de polaridade dos usuários por meio de hashtags\n",
    "3. Identificação dos top 100 Bubble Reachers por meio de uma métrica de centralidade chamada Intergroup Reaching\n",
    "4. Extração de artigos em sites de notícias apontadas em tweets\n",
    "5. Extração de entidades: (1) domínios | (2) conteúdos | (3) tópicos\n",
    "6. Identificação da polaridade de cada entidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Resultados\n",
    "\n",
    "* Os Bubble Reachers mais representativos foram, em geral, contas de mídias de notícias neutras\n",
    "* Mídias de notícias neutras são eficientes em contornar os filtros-bolha e distribuir conteúdos a grupos distintos na rede\n",
    "* Mesmo sendo expostos a realidades distintas, usuários compartilham apenas conteúdos que reforçam seu viés\n",
    "* O comportamento de compartilhamento depende mais do viés do conteúdo do que do tópico do conteúdo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### A seguir\n",
    "\n",
    "* Será apresentado um exemplo de extração de tópicos no dataset de notícias apontadas em tweets\n",
    "* Iremos apresentar somente o caso Canadense por brevidade\n",
    "* No final, será analisada a relação entre a polaridade dos tópicos e dos usuários que o retuitaram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Análise e pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Carregando os conjuntos de dados de notícias\n",
    "\n",
    "* Notícias sobre as eleições canadenses de 2019\n",
    "* Dataset em inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataframe_from_drive_csv(url):\n",
    "    path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# news dataset\n",
    "url = 'https://drive.google.com/file/d/1AQjdqe9QRFK7ydNteZPM_IJk1eH-H3n_/view?usp=share_link'\n",
    "df_news = load_dataframe_from_drive_csv(url)\n",
    "\n",
    "# retweeted links dataset\n",
    "url = 'https://drive.google.com/file/d/1Nn0I_tZnBWgUTNDeeE4bGzVTBhrTOJo1/view?usp=share_link'\n",
    "df_retweeted_urls = load_dataframe_from_drive_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Quantidade de notícias no conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_news[['retweets_count', 'title', 'description', 'article']].sort_values('retweets_count', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Remoção de notícias duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news = df_news.drop_duplicates(subset=['title', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Combinação do título + descrição + corpo do artigo em uma única string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'] = df_news['title'].astype(str) + ' ' + df_news['description'].astype(str) + ' ' + df_news['article'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Estatísticas sobre o tamanho dos textos\n",
    "\n",
    "* Textos longos (> 4500 caracteres em média)\n",
    "* O menor texto tem 310 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text_length'] = df_news['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de notícias\n",
    "\n",
    "* Escrita formal\n",
    "* Sem erros de ortografia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for content in df_news.sample(3)['text'].to_list():\n",
    "    print(content[:500], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Pré-processamento dos textos\n",
    "\n",
    "* Remoção de \n",
    "    * links \n",
    "    * caracteres isolados \n",
    "    * stop-words\n",
    "* Lematização usando Spacy\n",
    "* Transoformação em lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "\n",
    "# Instalação das dependências em inglês para a biblioteca Spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Stopwords em inglês contidas na biblioteca Spacy\n",
    "stop_words = list(spacy_nlp.Defaults.stop_words)\n",
    "\n",
    "# Stopwords em inglês contidas na biblioteca do NLTK\n",
    "stop_words += nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Algumas outras palavras para serem removidas além das stop-words\n",
    "stop_words += [\n",
    "    '-pron-', 'video', 'try', 'refresh', 'continue', 'article', 'load', 'browser', 'say', 'will', \n",
    "    'would', 'content', 'news', 'sign', 'register', 'home', 'page', 'advertisement'\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Remoção de links \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Transforma o texto em um documento Spacy\n",
    "    spacy_doc = spacy_nlp(text)\n",
    "\n",
    "    # Usa o Spacy para lematizar o texto e remover stop words\n",
    "    tokens = [token.lemma_.lower() for token in spacy_doc if token.lemma_.lower() not in stop_words]\n",
    "\n",
    "    # Remove caracteres isolados\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'] = df_news['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de notícias pré-processadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Frequência de termos mais representativos após o pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seta os padrões default para o matplot\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "def get_all_terms(corpus):\n",
    "    terms = []\n",
    "    for text in corpus:\n",
    "        terms = terms + text\n",
    "    return terms\n",
    "\n",
    "\n",
    "def term_frequency(corpus, num_plot=50, num_show=1000):\n",
    "    plt.figure(figsize=(15, 5)) \n",
    "\n",
    "    terms = get_all_terms(corpus)\n",
    "    fdist = nltk.FreqDist(terms)\n",
    "\n",
    "    if num_plot > 0:\n",
    "        fdist.plot(num_plot)\n",
    "\n",
    "    # return a dataframe with terms and frequencies\n",
    "    data = [[term, frequency] for term, frequency in fdist.most_common(num_show)]\n",
    "    return pd.DataFrame(data, columns=['TERM', 'FREQUENCY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_terms = term_frequency(df_news['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Geração de bi e trigramas\n",
    "\n",
    "Usando o Gemsim para formar bi e trigramas caso os termos coocorram pelo menos 10 vezes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "def make_trigrams(corpus, min_count=5, threshold=10):\n",
    "\n",
    "    # Criar os modelos de bi e trigramas\n",
    "\n",
    "    # Obs.: quanto maior o threshold, menos N-gramas são formados\n",
    "    bigram = Phrases(corpus, min_count=min_count, threshold=threshold)\n",
    "    bigram_model = Phraser(bigram)\n",
    "\n",
    "    trigram = Phrases(bigram[corpus], min_count=min_count, threshold=threshold)\n",
    "    trigram_model = Phraser(trigram)\n",
    "\n",
    "    return [trigram_model[bigram_model[text]] for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text'] = make_trigrams(df_news['text'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de notícias com bi e trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Termos mais representativos após a identificação de bi e trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_terms = term_frequency(df_news['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Remoção de termos sem valor para o domínio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for term in df_terms['TERM'].to_list():\n",
    "    if len(term) > 15:\n",
    "        print(\"'\" + term + \"',\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### A partir da lista acima, são selecionados manualmente quais termos devem ser removidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unuseful_terms = [\n",
    "    'apologize_fail_tap_team',\n",
    "    'postmedia_network',\n",
    "    'network_latest_national_stories',\n",
    "    'soon_inbox_encounter_issue',\n",
    "    'click_unsubscribe_link_email',\n",
    "    'inc._365_bloor_street',\n",
    "    'ontario_m4w_3l4_416',\n",
    "    'thank_welcome_email_way',\n",
    "    'check_junk_folder_issue',\n",
    "    'story_midday_sun_newsroom',\n",
    "    'inbox_noon_late_headline',\n",
    "    'story_opinion_photo_toronto',\n",
    "    'sun_email_address_error',\n",
    "    'provide_valid_email_address',\n",
    "    'click_button_consent_receive',\n",
    "    'newsletter_postmedia_network_inc.',\n",
    "    'unsubscribe_time',\n",
    "    'original_archive'\n",
    "]\n",
    "\n",
    "def remove_unuseful_terms(text):\n",
    "    return [token for token in text if token.lower() not in unuseful_terms]\n",
    "\n",
    "df_news['text'] = df_news['text'].apply(remove_unuseful_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Identificação de tópicos com LDA\n",
    "\n",
    "Referência: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Criação do dicionário e corpus para o LDA usando o modelo BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Cria o dicionário a partir do corpus\n",
    "gs_dictionary = Dictionary(df_news['text'])\n",
    "\n",
    "# Remove os tokens muito raros (menos frequentes que `no_below`) ou muito comuns (mais frequentes que `no_above`%)\n",
    "gs_dictionary.filter_extremes(no_below=3, no_above=.20)\n",
    "\n",
    "# Cria o corpus usando o modelo de Bag of Words\n",
    "gs_corpus = [gs_dictionary.doc2bow(text) for text in df_news['text'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Dictionary size:', len(gs_dictionary), ', corpus size:', len(gs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Treinamento de modelos LDA \n",
    "\n",
    "* Para treinar os modelos LDA usamos diferentes valores para $k$ \n",
    "* Nesse tutorial, escolhemos $k=10$ até $k=30$, uma vez que temos aprox. 500 documentos no corpus\n",
    "* Para cada modelo geramos o score de coerência e perplexidade, que são analisados a seguir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def compute_lda_performance(dictionary, corpus, texts, start=1, limit=50, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    perplexity_values : Perplexity values corresponding to the LDA model with respective number of topics\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    num_topics_values = []\n",
    "    perplexity_values = []\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit+step, step):\n",
    "        num_topics_values.append(num_topics)\n",
    "\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, iterations=1000, id2word=dictionary, passes=10, random_state=100) \n",
    "\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence = coherencemodel.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "\n",
    "        perplexity = model.log_perplexity(corpus)\n",
    "        perplexity_values.append(perplexity)\n",
    "\n",
    "        print('Topics:', num_topics, '\\tPerplexity:', round(perplexity, 5), '\\tCoherence:', round(coherence, 5))\n",
    "\n",
    "    df_results = pd.DataFrame({'topics': num_topics_values, 'perplexity': perplexity_values, 'coherence': coherence_values})\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_lda_models = compute_lda_performance(dictionary=gs_dictionary, corpus=gs_corpus, texts=df_news['text'], start=10, limit=30, step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Análise do score de coerência\n",
    "\n",
    "* Indica o quão \"interpretável\" são os tópicos para humanos\n",
    "* Indica o quanto as palavras mais representativas de cada tópico são similares entre si\n",
    "* Diferentes medidas de similaridade podem ser usadas, a padrão é o score c_v, que usa similaridade do cosseno\n",
    "* Heurística: quanto maior o score de coerência, melhor (mas nem sempre)\n",
    "    * Usar método do \"cotovelo\"\n",
    "    * Analisar o gráfico de tópicos (a seguir)\n",
    "    * Usar bom senso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scores de coerência de acordo com o número de tópicos\n",
    "ax = df_lda_models.plot.line(x='topics', y='coherence')\n",
    "ax.set_xlabel(\"Num Topics\")\n",
    "ax.set_ylabel(\"Coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top 5 modelos\n",
    "df_lda_models.sort_values('coherence', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Identifica o melhor número de tópicos de acordo com o score de coerência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_num_topics = df_lda_models.sort_values('coherence', ascending=False)['topics'].tolist()[0]\n",
    "\n",
    "best_num_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Treinamento do modelo com o melhor número de tópicos \n",
    "\n",
    "Aqui repetimos o treinamento do modelo com o melhor número de tópicos usando uma quantidade maior de iterações e passes no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=gs_corpus, num_topics=best_num_topics, \n",
    "                   iterations=10000, id2word=gs_dictionary, passes=100, \n",
    "                   random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Coerência e perplexidade do modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity (lower is better)\n",
    "print('\\nPerplexity: ', lda.log_perplexity(gs_corpus)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df_news['text'], \n",
    "                                     dictionary=gs_dictionary, coherence='c_v')\n",
    "print('\\nCoherence Score: ', coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Palavras mais representativas de cada tópico\n",
    "\n",
    "Alguns exemplos de tópicos:\n",
    "\n",
    "* **IMPOSTO DE CARBONO:** carbon_tax, emission, climate_change, tax, cbc, economy, cent, target, cost, program \n",
    "* **ABORTO:** abortion, debate, harper, comment, conservative_party, sex_marriage, law, view, conservative_leader, montreal \n",
    "* **PESQUISA ELEITORAL:** poll, mr._scheer, cent, mr._trudeau, survey, research, 30, age, positive, centre \n",
    "* **CORTE ORÇAMENTÁRIO:** billion, cut, platform, million, city, toronto, tax, bernier, budget, spending \n",
    "* **CASO DE RACISMO:** blackface, racist, apologize, black, apology, rcmp, woman, racism, global, makeup \n",
    "* **PETRÓLEO:** pipeline, oil, climate_change, climate, company, energy, encana, coal, spend, money \n",
    "* **IMIGRAÇÃO:** insurance, claim, refugee, comment, act, facebook, immigrant, immigration, insurance_broker, anti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, topic in enumerate(lda.top_topics(topn=5, texts=df_news['text'])):\n",
    "    terms = topic[0]\n",
    "    print('Topic', i, ', '.join([term[1] for term in terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Visualização dos tópicos\n",
    "\n",
    "* Cada bolha representa um tópico. Quanto maior a bolha, mais prevalente é esse tópico.\n",
    "* Um bom modelo de tópico terá bolhas grandes e não sobrepostas espalhadas por todo o gráfico.\n",
    "* Um modelo com muitos tópicos normalmente terá muitas sobreposições, bolhas de tamanho pequeno agrupadas em uma região do gráfico.\n",
    "\n",
    "Fonte: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def show_lda_vis(lda, gs_corpus, gs_dictionary):\n",
    "    # Workaround para evitar que o pyLDAvis esconda os botões do Jupyterlab\n",
    "    from IPython.display import HTML\n",
    "    css_str = '<style> \\\n",
    "    .jp-icon-warn0 path {fill: var(--jp-warn-color0);} \\\n",
    "    .bp3-button-text path { fill: var(--jp-inverse-layout-color3);} \\\n",
    "    .jp-icon-brand0 path { fill: var(--jp-brand-color0);} \\\n",
    "    text.terms { fill: #616161;} \\\n",
    "    </style>'\n",
    "    display(HTML(css_str))\n",
    "\n",
    "    # feed the LDA model into the pyLDAvis instance\n",
    "    warnings.filterwarnings('ignore')\n",
    "    return gensimvis.prepare(lda, gs_corpus, gs_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_lda_vis(lda, gs_corpus, gs_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Salva os 3 tópicos mais representativos de cada notícia no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_2_words = {}\n",
    "for topic in lda.show_topics(num_topics=100, num_words=10, formatted=False):\n",
    "    topic_id = topic[0]\n",
    "    topic_tokens = ', '.join([token[0] for token in topic[1]])\n",
    "    topic_2_words[topic_id] = topic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_topics_1 = []\n",
    "doc_topics_1_words = []\n",
    "doc_topics_1_percentages = []\n",
    "\n",
    "doc_topics_2 = []\n",
    "doc_topics_2_words = []\n",
    "doc_topics_2_percentages = []\n",
    "\n",
    "doc_topics_3 = []\n",
    "doc_topics_3_words = []\n",
    "doc_topics_3_percentages = []\n",
    "\n",
    "for i, doc in enumerate(df_news['text'].to_list()):\n",
    "    doc_bow = gs_dictionary.doc2bow(doc)\n",
    "    \n",
    "    # get document topics (each row contains a tuple with topic id and topic probability)\n",
    "    doc_topics = lda.get_document_topics(doc_bow)\n",
    "    \n",
    "    # sort topics by probability\n",
    "    doc_topics.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    # get them main topic and top 3 topics\n",
    "    topics = doc_topics[:3]\n",
    "    \n",
    "    if len(topics) > 0:\n",
    "        doc_topics_1_percentages.append(topics[0][1])\n",
    "        topic_id = topics[0][0]\n",
    "        doc_topics_1.append(topic_id)\n",
    "        doc_topics_1_words.append(topic_2_words[topic_id])\n",
    "    else:\n",
    "        doc_topics_1.append(None)\n",
    "        doc_topics_1_percentages.append(None)\n",
    "        doc_topics_1_words.append(None)\n",
    "        \n",
    "        \n",
    "    if len(topics) > 1:\n",
    "        doc_topics_2_percentages.append(topics[1][1])\n",
    "        topic_id = topics[1][0]\n",
    "        doc_topics_2.append(topic_id)\n",
    "        doc_topics_2_words.append(topic_2_words[topic_id])\n",
    "    else:\n",
    "        doc_topics_2.append(None)\n",
    "        doc_topics_2_percentages.append(None)\n",
    "        doc_topics_2_words.append(None)\n",
    "        \n",
    "        \n",
    "    if len(topics) > 2:\n",
    "        doc_topics_3_percentages.append(topics[2][1])\n",
    "        topic_id = topics[2][0]\n",
    "        doc_topics_3.append(topic_id)\n",
    "        doc_topics_3_words.append(topic_2_words[topic_id])\n",
    "    else:\n",
    "        doc_topics_3.append(None)\n",
    "        doc_topics_3_percentages.append(None)\n",
    "        doc_topics_3_words.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news['topic'] = pd.Series(doc_topics_1)\n",
    "df_news['topic_words'] = pd.Series(doc_topics_1_words)\n",
    "df_news['topic_percentage'] = pd.Series(doc_topics_1_percentages)\n",
    "\n",
    "df_news['topic_1'] = pd.Series(doc_topics_1)\n",
    "df_news['topic_1_words'] = pd.Series(doc_topics_1_words)\n",
    "df_news['topic_1_percentage'] = pd.Series(doc_topics_1_percentages)\n",
    "\n",
    "df_news['topic_2'] = pd.Series(doc_topics_2)\n",
    "df_news['topic_2_words'] = pd.Series(doc_topics_2_words)\n",
    "df_news['topic_2_percentage'] = pd.Series(doc_topics_2_percentages)\n",
    "\n",
    "df_news['topic_3'] = pd.Series(doc_topics_3)\n",
    "df_news['topic_3_words'] = pd.Series(doc_topics_3_words)\n",
    "df_news['topic_3_percentage'] = pd.Series(doc_topics_3_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Distribuição dos tópicos mais representativos em cada notícia\n",
    "\n",
    "* O tópico mais representativo (em azul) tem alta dominância para a maioria das notícias\n",
    "* O segundo e terceiro tópicos mais representativos, em laranja e verde, tem uma representatividade baixa na maioria das notícias em comparação com o azul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax1 = pd.Series(doc_topics_1_percentages).plot.hist(bins=25)\n",
    "ax2 = pd.Series(doc_topics_2_percentages).plot.hist(bins=25)\n",
    "ax3 = pd.Series(doc_topics_3_percentages).plot.hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Distribuição dos tópicos mais dominantes entre as notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_news.groupby('topic_1').agg({\n",
    "    'article': 'count'\n",
    "}).reset_index().rename(columns={\n",
    "    'topic_1': 'dominant topic id',\n",
    "    'article': 'number of news',\n",
    "}).sort_values('number of news', ascending=False)\n",
    "\n",
    "df['dominant topic id'] = df['dominant topic id'].astype('int')\n",
    "\n",
    "ax = df.plot.bar(x='dominant topic id', y='number of news', figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['number of news'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Exemplos de manchetes em 5 tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    print('\\nTopic', i)\n",
    "    print(df_news[df_news['topic'] == i]['title'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_1(url):\n",
    "    df = df_news[df_news['url'] == url]\n",
    "    return df.iloc[0]['topic'] if df.shape[0] == 1 else None\n",
    "\n",
    "def get_topic_1_words(url):\n",
    "    df = df_news[df_news['url'] == url]\n",
    "    return df.iloc[0]['topic_words'] if df.shape[0] == 1 else None\n",
    "\n",
    "df_retweeted_urls['topic'] = df_retweeted_urls['retweeted_url'].apply(get_topic_1)\n",
    "df_retweeted_urls['topic_words'] = df_retweeted_urls['retweeted_url'].apply(get_topic_1_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Resultados\n",
    "\n",
    "Heatmap de tópicos vs retweets de usuários por faixa de polaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def relative_polarity_heatmap(df, column, oversample=True, title=None, x_label=None, y_label_left=None, y_label_right=None, \n",
    "                              cbar_label=None, top_n=20, vmax=1, numeric_index=False, only_dataframe=False):\n",
    "    \n",
    "    env_polarities = [value/10 for value in range(-10,11,1)]\n",
    "    \n",
    "    df_copy = df.copy()    \n",
    "    \n",
    "    \"\"\"\n",
    "    -------------------\n",
    "    RP(H) calculation\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a matrix with rows being 'column' parameter values and columns being polarities from -1 to +1\n",
    "    df = pd.crosstab(index=df[column], columns=df['user_P(H)_bin'], values=df[column], aggfunc='count')\n",
    "    df = df.fillna(0.0)\n",
    "    \n",
    "    # add faulting columns (for faulting polarities)\n",
    "    for polarity in env_polarities:\n",
    "        if not polarity in df.columns:\n",
    "            num_rows = df.shape[0]\n",
    "            df[polarity] = pd.Series([0.0] * num_rows)\n",
    "            \n",
    "    # reorder columns from -1.0 to +1.0\n",
    "    df = df[env_polarities]\n",
    "    \n",
    "    # scale by dividing the retweets count of each polarity for each domain by the max retweets count of each polarity from all domains\n",
    "    if oversample:\n",
    "        df_polarity_max_retweets = df.max(axis=0) # get polarity column max value\n",
    "        for polarity in env_polarities:\n",
    "            df[polarity] = df[polarity] / df_polarity_max_retweets[polarity]    \n",
    "    \n",
    "    # normalize values to 0-1 interval with min-max (by domain min-max from all polarities)\n",
    "    max_df = df.max(axis=1)\n",
    "    min_df = df.min(axis=1)\n",
    "    for polarity in env_polarities:\n",
    "        df[polarity] = (df[polarity] - min_df) / (max_df - min_df) \n",
    "       \n",
    "    # calculate polarity average without zeros and neutral users count\n",
    "    relative_polarities = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_sum = 0\n",
    "        count = 0\n",
    "        for polarity in env_polarities:\n",
    "            if polarity != 0.0 and row[polarity] > 0.0: # only count cells with non zero value and remove the neutral polarity\n",
    "                row_sum += row[polarity] * polarity\n",
    "                count += row[polarity]\n",
    "        if count > 0:\n",
    "            relative_polarities.append(row_sum / count)\n",
    "        else:\n",
    "            relative_polarities.append(None)\n",
    "        \n",
    "    df['relative_polarity'] = relative_polarities\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    -------------------\n",
    "    Data preparation\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    # count occurrences of 'column' values\n",
    "    df_rphs = df_copy.groupby(column).agg(\n",
    "        retweets_count=pd.NamedAgg(column=column, aggfunc='count')\n",
    "    ).sort_values(by=column, ascending=False)\n",
    "    df_rphs[column] = df.sort_index(ascending=False).index\n",
    "    df_rphs['RP(H)'] = df.sort_index(ascending=False)['relative_polarity']\n",
    "    \n",
    "    \"\"\"\n",
    "    -------------------\n",
    "    Data visualization\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    if not only_dataframe:\n",
    "    \n",
    "        # get only top N most retweeted 'column' values to include in the heatmap\n",
    "        if top_n:        \n",
    "            top_column_values = df_rphs.sort_values(by='retweets_count', ascending=False)[:top_n][column].unique()\n",
    "            df = df[df.index.isin(top_column_values)]\n",
    "\n",
    "        # sort heatmap rows by relative_polarity values\n",
    "        df = df.sort_values('relative_polarity')\n",
    "        relative_polarities = df['relative_polarity'].map('{:,.2f}'.format).astype('str').to_list()\n",
    "        \n",
    "        # drop relative_polarity to not include in the heatmap\n",
    "        df = df.drop(columns=['relative_polarity'])\n",
    "        df = df.fillna(0.0)        \n",
    "            \n",
    "        # get a print friendly datatable\n",
    "        row_indexes = list(range(1,len(df.index)+1))\n",
    "        row_values = df.index        \n",
    "        df_heatmap_table = pd.DataFrame({'ID': row_indexes, y_label_left: row_values, 'RP(H)': relative_polarities})\n",
    "        \n",
    "        # create a sequential numeric 'id' for heatmap rows\n",
    "        if numeric_index:            \n",
    "            df['id'] = row_indexes\n",
    "            df = df.set_index('id')\n",
    "\n",
    "        plt.subplots(figsize=(2,round(top_n/3.5)))\n",
    "        ax = sns.heatmap(df, annot=False, linewidths=.1, robust=True, cmap='YlOrBr', vmin=0, vmax=vmax, cbar=False, square=True)\n",
    "        ax.set_title(title or '')\n",
    "        ax.set_xlabel(x_label or 'User P(H)')\n",
    "        ax.set_ylabel(y_label_left or '')\n",
    "        #ax.collections[0].colorbar.set_label(cbar_label or 'Retweet density')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation = 0)\n",
    "\n",
    "        # maintain only 5 tick labels to simplify\n",
    "        for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "            if n not in [0, 5, 10, 15, 20]:\n",
    "                label.set_visible(False)\n",
    "\n",
    "        # add right y axis\n",
    "        ax2 = ax.twinx() # share x-axis\n",
    "        ax2.set_ylabel(y_label_right or '')\n",
    "        ax2.tick_params(right=True, pad=6)\n",
    "        ax2.set_aspect('auto', share=True, )\n",
    "        ax2.set_ylim((top_n, 0))\n",
    "        ax2.set_yticks(ax.get_yticks())\n",
    "        ax2.set_yticklabels(relative_polarities)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "        ax2.spines['bottom'].set_visible(False)\n",
    "        ax2.spines['left'].set_visible(False)\n",
    "\n",
    "        fig = ax.get_figure()\n",
    "        fig.set_size_inches(2, round(top_n/3.5))\n",
    "\n",
    "    return df_rphs, df_heatmap_table, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "df, df_heatmap, fig = relative_polarity_heatmap(\n",
    "    df=df_retweeted_urls, column='topic_words', y_label_left='Tópico', \n",
    "    y_label_right='Polaridade do tópico', x_label='Polaridade dos usuários',\n",
    "    top_n=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "0c9dfea1575e8b44ff0615653062e3f77db49c736187dedf2087539b29eac2fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
