{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Processamento de Linguagem Natural em Textos de Mídias Sociais: Fundamentos, Ferramentas e Aplicações</center>\n",
    "\n",
    "### <center>XXVIII Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia 2022)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Frances A. Santos (UNICAMP), Jordan Kobellarz (UTFPR), Fábio R. de Souza (USP), Leandro A. Villas (UNICAMP), Thiago H. Silva (UTFPR)</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center>Curitiba, PR</center>\n",
    "<center>07 de Novembro de 2022</center>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/webmedia2022-nlp/course-code/blob/main/NLP_WebMedia2022.ipynb\" target=\"_parent\"><img style=\"float: right;\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir no Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Introdução \n",
    "- Texto de mídias sociais: suas principais características e como coletá-los\n",
    "- Pré-processamento textual\n",
    "- Representação de textos utilizando vetores numéricos\n",
    "- Modelagem e extração de conhecimento\n",
    "- Compreensão semântica e emocional\n",
    "- Possíveis aplicações\n",
    "- Perguntas & Respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introdução\n",
    "\n",
    "<img src=\"figs/social-media.jpeg\" style=\"float: center; zoom:50%;\" />\n",
    "\n",
    "- Mídias sociais ..\n",
    "- To Do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.3 in /home/jordan/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.21.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in /home/jordan/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.3.4)\n",
      "Requirement already satisfied: tweepy==4.10.1 in /home/jordan/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (4.10.1)\n",
      "Requirement already satisfied: praw==7.6.0 in /home/jordan/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (7.6.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jordan/anaconda3/lib/python3.7/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jordan/anaconda3/lib/python3.7/site-packages (from pandas==1.3.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/jordan/anaconda3/lib/python3.7/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/jordan/anaconda3/lib/python3.7/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/jordan/anaconda3/lib/python3.7/site-packages (from tweepy==4.10.1->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /home/jordan/anaconda3/lib/python3.7/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/jordan/anaconda3/lib/python3.7/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/jordan/anaconda3/lib/python3.7/site-packages (from praw==7.6.0->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jordan/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jordan/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jordan/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jordan/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jordan/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy==4.10.1->-r requirements.txt (line 3)) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import warnings\n",
    "import pathlib\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "from ExtracaoDados import ExtracaoDados\n",
    "from PreProcessamento import PreProcessamento\n",
    "from ModelosRepresentacao import ModelosEstatisticos, SentenceEmbeddings, WordEmbeddings\n",
    "from ExtracaoConhecimento import Agrupamento, CompreensaoSemantica, CompreensaoEmocional\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Textos de mídias sociais\n",
    "<br></br>\n",
    "To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Twitter\n",
    "\n",
    "<br></br>\n",
    "To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coletando Tweets\n",
    "\n",
    "- Utilizaremos a Twitter API v2 para coletar os tweets \n",
    "- Acesse a [página de desenvolverdores](https://developer.twitter.com/) e obtenha suas credenciais de acesso\n",
    "- Limitamos cada coleta a 10 tweets, mas todo o conteúdo dos tweets é adicionado (*appending*) ao arquivo local data/tweets.json\n",
    "- Além dos campos *id* e *text* que estão presentes nos tweets por padrão, também solicitamos os campos *created_at, entities, geo, lang, public_metrics, source*. Para ver a lista completa de campos possíveis, acesse esta [página](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet)\n",
    "- Filtramos para selecionar apenas os tweets escritos em Inglês ( *lang = \"en\"* ) e que contenham o termo \"nyc\", que referencia a cidade de Nova Iorque\n",
    "- Após coletar os tweets, extraímos os valores dos campos *text, timestamp_ms, ...* e retornamos no formato Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe seu 'API KEY'\n",
      "········\n",
      "Informe seu 'API KEY SECRET'\n",
      "········\n",
      "Informe seu 'ACCESS TOKEN KEY'\n",
      "········\n",
      "Informe seu 'ACCESS TOKEN SECRET'\n",
      "········\n",
      "Informe seu 'Bearer TOKEN'\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "## Get Twitter API credentials\n",
    "\n",
    "print(\"Informe seu 'API KEY'\")\n",
    "twitter_consumer_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'API KEY SECRET'\")\n",
    "twitter_consumer_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN KEY'\")\n",
    "twitter_access_token_key = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'ACCESS TOKEN SECRET'\")\n",
    "twitter_access_token_secret = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'Bearer TOKEN'\")\n",
    "twitter_bearer_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "Unauthorized",
     "evalue": "401 Unauthorized\nUnauthorized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_431773/2724402620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtwitter_access_token_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtwitter_access_token_secret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtwitter_bearer_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projetos/py/WebMedia2022/course-code/ExtracaoDados/extracaoDados.py\u001b[0m in \u001b[0;36mtwitter\u001b[0;34m(self, CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN_KEY, ACCESS_TOKEN_SECRET, BEARER_TOKEN)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbearer_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBEARER_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nyc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"geo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"public_metrics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36madd_rules\u001b[0;34m(self, add, **params)\u001b[0m\n\u001b[1;32m    666\u001b[0m         return self._make_request(\n\u001b[1;32m    667\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"/2/tweets/search/stream/rules\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0mendpoint_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dry_run\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStreamRule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         )\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         response = self.request(method, route, params=request_params,\n\u001b[0;32m--> 127\u001b[0;31m                                 json=json, user_auth=user_auth)\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnauthorized\u001b[0m: 401 Unauthorized\nUnauthorized"
     ]
    }
   ],
   "source": [
    "# collect tweets\n",
    "df_tweets = ExtracaoDados().twitter(\n",
    "    twitter_consumer_key, \n",
    "    twitter_consumer_secret, \n",
    "    twitter_access_token_key, \n",
    "    twitter_access_token_secret, \n",
    "    twitter_bearer_token\n",
    ")\n",
    "\n",
    "df_tweets.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reddit\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coletando Posts de \\subreddits\n",
    "\n",
    "- Utilizaremos a [API do Reddit](https://www.reddit.com/dev/api) para coletar posts\n",
    "- Você deverá criar uma conta para acessar a API em [reddit.com](https://reddit.com)\n",
    "- Depois de criar a conta, obtenha os Client ID e o Client Secret\n",
    "- No exemplo a seguir, coletamos os top 100 posts de 5 subreddits, contendo o texto, url, número de comentários, data de criação e score (número de upvotes do post)\n",
    "- Os dados são salvos no arquivo local data/reddit_posts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informe seu 'CLIENT ID'\n",
      "········\n",
      "Informe seu 'CLIENT SECRET'\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "## Get Reddit API credentials\n",
    "\n",
    "print(\"Informe seu 'CLIENT ID'\")\n",
    "REDDIT_CLIENT_ID = getpass.getpass()\n",
    "\n",
    "print(\"Informe seu 'CLIENT SECRET'\")\n",
    "REDDIT_CLIENT_SECRET = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 6/6 [01:10<00:00, 11.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# Subreddits with serious/relevant discussions\n",
    "subreddits = [\n",
    "    'politics',\n",
    "    'AskHistorians',\n",
    "    'changemyview',\n",
    "    'COVID19',\n",
    "    'EverythingScience',\n",
    "    'science'\n",
    "]\n",
    "\n",
    "# collect top 100 posts from each subreddit\n",
    "df_reddit_posts = ExtracaoDados().reddit(\n",
    "    REDDIT_CLIENT_ID, \n",
    "    REDDIT_CLIENT_SECRET,\n",
    "    subreddits=subreddits,\n",
    "    top_n=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1.583273e+09</td>\n",
       "      <td>https://www.reddit.com/r/COVID19/comments/fd29...</td>\n",
       "      <td>Please consider downloading BOINC or folding@h...</td>\n",
       "      <td>3124</td>\n",
       "      <td>1203</td>\n",
       "      <td>Hello all.\\n\\nI believe this has been posted b...</td>\n",
       "      <td>3181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1.521000e+09</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/84ai...</td>\n",
       "      <td>Physicist Stephen Hawking dies aged 76</td>\n",
       "      <td>199300</td>\n",
       "      <td>3851</td>\n",
       "      <td>We regret to hear that [Stephen Hawking died t...</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1.511109e+09</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/7e1j...</td>\n",
       "      <td>Raising the taxes of graduate students by as m...</td>\n",
       "      <td>124379</td>\n",
       "      <td>10935</td>\n",
       "      <td>Science and technology development has been th...</td>\n",
       "      <td>2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1.602797e+09</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/jbwl...</td>\n",
       "      <td>[Megathread] World's most prestigious scientif...</td>\n",
       "      <td>80112</td>\n",
       "      <td>4480</td>\n",
       "      <td>We have received numerous submissions concerni...</td>\n",
       "      <td>2288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1.437997e+09</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/3ere...</td>\n",
       "      <td>Science Ama Series: I am Stephen Hawking, theo...</td>\n",
       "      <td>79242</td>\n",
       "      <td>8604</td>\n",
       "      <td>I signed an open letter earlier this year impl...</td>\n",
       "      <td>2211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          created                                                url  \\\n",
       "315  1.583273e+09  https://www.reddit.com/r/COVID19/comments/fd29...   \n",
       "500  1.521000e+09  https://www.reddit.com/r/science/comments/84ai...   \n",
       "503  1.511109e+09  https://www.reddit.com/r/science/comments/7e1j...   \n",
       "557  1.602797e+09  https://www.reddit.com/r/science/comments/jbwl...   \n",
       "562  1.437997e+09  https://www.reddit.com/r/science/comments/3ere...   \n",
       "\n",
       "                                                 title   score  num_comments  \\\n",
       "315  Please consider downloading BOINC or folding@h...    3124          1203   \n",
       "500             Physicist Stephen Hawking dies aged 76  199300          3851   \n",
       "503  Raising the taxes of graduate students by as m...  124379         10935   \n",
       "557  [Megathread] World's most prestigious scientif...   80112          4480   \n",
       "562  Science Ama Series: I am Stephen Hawking, theo...   79242          8604   \n",
       "\n",
       "                                                  text  length  \n",
       "315  Hello all.\\n\\nI believe this has been posted b...    3181  \n",
       "500  We regret to hear that [Stephen Hawking died t...     998  \n",
       "503  Science and technology development has been th...    2592  \n",
       "557  We have received numerous submissions concerni...    2288  \n",
       "562  I signed an open letter earlier this year impl...    2211  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some posts with text\n",
    "df_reddit_posts[df_reddit_posts['length'] > 0].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré - Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é um método para..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PreProcessamento(dados)\n",
    "\n",
    "tokens = pipeline.tokenizacao()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stemmização é um método para..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem =  pipeline.stemmizacao()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Representação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos Estatísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelosEstatisticos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de Conhecimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicações"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4727e67ed413a359b1b437df07d70871e30a250310fa3c2ba8c310119f033ef3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
